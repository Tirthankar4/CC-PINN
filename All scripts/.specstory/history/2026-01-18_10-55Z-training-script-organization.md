<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 0760b9a6-4492-4670-8cd6-02255b061939 (2026-01-18 10:55Z) -->

# Training script organization (2026-01-18 10:55Z)

_**User (2026-01-18 09:51Z)**_

@utilities/training_diagnostics.py in utilities is critically needed during training I think among all scripts in that folder. Shift this file to training folder and check if any other script is needed in the training and visualisation process (or config.py has any option related to any script other than training_diagnostics)

---

_**Agent (model claude-4.5-haiku, mode Agent)**_

I'll help you move `training_diagnostics.py` to the training folder and check for dependencies. Let me start by examining the relevant files.

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\config.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\train.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\trainer.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\physics.py</summary>

</details>
</tool-use>

---

Now let me check the utilities folder to see what other files are there:

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** ‚Ä¢ Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** ‚Ä¢ Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** ‚Ä¢ Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

Now let me search for all imports of training_diagnostics to understand the dependencies:

---

<tool-use data-tool-type="search" data-tool-name="grep">
<details>
<summary>Tool use: **grep** ‚Ä¢ Grep for "training_diagnostics|TrainingDiagnostics" ‚Ä¢ 7 matches</summary>

Output mode: files_with_matches


| File |
|------|
| `.\train.py` |
| `.\training\trainer.py` |
| `.\utilities\DIAGNOSTICS_GUIDE.md` |
| `.\utilities\training_diagnostics.py` |
| `.\package_for_kaggle.py` |
| `.\training\physics.py` |
| `.\utilities\__init__.py` |


</details>
</tool-use>

---

Perfect! Now let me check how training_diagnostics is imported in these files:

---

<tool-use data-tool-type="search" data-tool-name="grep">
<details>
<summary>Tool use: **grep** ‚Ä¢ Grep for "from.*training_diagnostics|import.*training_diagnostics|from.*TrainingDiagnostics" ‚Ä¢ 2 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `.\training\trainer.py` | `from utilities.training_diagnostics import TrainingDiagnostics` | L9 |
| `.\training\physics.py` | `from utilities.training_diagnostics import TrainingDiagnostics` | L8 |


</details>
</tool-use>

---

Now let me check train.py to see how it uses training_diagnostics:

---

<tool-use data-tool-type="search" data-tool-name="grep">
<details>
<summary>Tool use: **grep** ‚Ä¢ Grep for "ENABLE_TRAINING_DIAGNOSTICS|training_diagnostics" in "C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\train.py" ‚Ä¢ 15 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `.\train.py` | `training_diagnostics = train(` | L332 |
| `.\train.py` | `if ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:` | L361 |
| `.\train.py` | `training_diagnostics.run_comprehensive_diagnostics(` | L372 |
| `.\train.py` | `print("  1. training_diagnostics.png - Training convergence")` | L381 |
| `.\training\trainer.py` | `from utilities.training_diagnostics import TrainingDiagnostics` | L9 |
| `.\training\trainer.py` | `from config import BATCH_SIZE, NUM_BATCHES, ENABLE_TRAINING_DIAGNOSTICS` | L10 |
| `.\training\trainer.py` | `diagnostics = TrainingDiagnostics(save_dir='./diagnostics/') if ENABLE_TRAINING_DIAGNOSTICS else None` | L43 |
| `.\utilities\training_diagnostics.py` | `plt.savefig(f'\{self.save_dir\}/training_diagnostics\{suffix\}.png', dpi=150, bbox_inches='tight')` | L104 |
| `.\utilities\training_diagnostics.py` | `print("  1. training_diagnostics.png - Training convergence analysis")` | L632 |
| `.\utilities\DIAGNOSTICS_GUIDE.md` | `## Plot 1: Training Diagnostics (`training_diagnostics.png`)` | L17 |
| `.\package_for_kaggle.py` | `("utilities/training_diagnostics.py", "utilities.training_diagnostics"),` | L35 |
| `.\config.py` | `ENABLE_TRAINING_DIAGNOSTICS = True  # Enable automatic training diagnostics plots and logging` | L58 |
| `.\training\physics.py` | `from utilities.training_diagnostics import TrainingDiagnostics` | L8 |
| `.\training\physics.py` | `from config import IC_WEIGHT, ENABLE_TRAINING_DIAGNOSTICS` | L14 |
| `.\utilities\__init__.py` | `# - training_diagnostics: Training metrics and plots` | L3 |


</details>
</tool-use>

---

Perfect! Now let me create a comprehensive summary and then move the file. Let me also check the visualization and config files more carefully:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\utilities\__init__.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\package_for_kaggle.py</summary>

</details>
</tool-use>

---

Now I have a clear picture. Let me create a comprehensive analysis and move the file:

---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** ‚Ä¢ Todo List</summary>

- [-] Move training_diagnostics.py from utilities to training folder
- [ ] Update import in training/trainer.py from utilities to local
- [ ] Update import in training/physics.py from utilities to local
- [ ] Update module path in package_for_kaggle.py to reflect new location
- [ ] Update utilities/__init__.py to remove training_diagnostics reference

</details>
</tool-use>

---

Now let me move the file and update all imports:

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** ‚Ä¢ Read file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\utilities\training_diagnostics.py</summary>

</details>
</tool-use>

---

Now I'll move the file and update all imports:

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** ‚Ä¢ Edit file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\training_diagnostics.py</summary>

**Chunk 1**
Lines added: 638, lines removed: 1

```diff
@@ -1,1 +1,638 @@
- 
+ import os
+ import numpy as np
+ import torch
+ import matplotlib.pyplot as plt
+ from matplotlib.gridspec import GridSpec
+ 
+ 
+ class TrainingDiagnostics:
+     """
+     Optimized diagnostics for PINN performance analysis.
+     Focused on diagnosing high-tmax failures with 5 critical plots.
+     """
+ 
+     def __init__(self, save_dir='./diagnostics/'):
+         self.save_dir = save_dir
+         os.makedirs(save_dir, exist_ok=True)
+ 
+         # Storage for tracking metrics over iterations
+         self.history = {
+             'iteration': [],
+             'total_loss': [],
+             'pde_loss': [],
+             'ic_loss': [],
+             'mean_rho': [],
+             'max_rho': [],
+         }
+ 
+     def _prepare_inputs(self, geomtime_col):
+         """Normalize collocation inputs to the model's expected format."""
+         if isinstance(geomtime_col, (list, tuple)):
+             return geomtime_col
+         # Single tensor [N, D] -&gt; list of [N,1]
+         return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]
+ 
+     def log_iteration(self, iteration, model, loss_dict, geomtime_col):
+         """Call this every N iterations during training."""
+         self.history['iteration'].append(iteration)
+         self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))
+         self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))
+         self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))
+ 
+         # Compute diagnostics from current model state
+         with torch.no_grad():
+             inputs = self._prepare_inputs(geomtime_col)
+             pred = model(inputs)
+             # Use first channel as density/log-density proxy
+             rho = pred[:, 0].detach().cpu().numpy()
+ 
+             # Density statistics
+             self.history['mean_rho'].append(np.nanmean(rho))
+             self.history['max_rho'].append(np.nanmax(rho))
+ 
+     def plot_diagnostics(self, iteration=None):
+         """
+         Generate streamlined training diagnostic plot.
+         Combined loss components and balance in one figure.
+         """
+         if len(self.history['iteration']) == 0:
+             print("No diagnostic data to plot.")
+             return
+         
+         iters = self.history['iteration']
+         suffix = f"_iter_{iteration}" if iteration is not None else "_final"
+ 
+         # Combined training diagnostics plot
+         fig = plt.figure(figsize=(14, 5))
+         gs = GridSpec(1, 3, figure=fig)
+         
+         # Subplot 1: Loss evolution
+         ax1 = fig.add_subplot(gs[0, 0])
+         ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')
+         ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')
+         ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')
+         ax1.set_xlabel('Iteration', fontsize=11)
+         ax1.set_ylabel('Loss (log scale)', fontsize=11)
+         ax1.set_title('Loss Components', fontsize=12, fontweight='bold')
+         ax1.legend(fontsize=10)
+         ax1.grid(True, alpha=0.3)
+         
+         # Subplot 2: Loss balance ratio
+         ax2 = fig.add_subplot(gs[0, 1])
+         ic_arr = np.array(self.history['ic_loss'])
+         pde_arr = np.array(self.history['pde_loss'])
+         ratio = pde_arr / (ic_arr + 1e-10)
+         ax2.plot(iters, ratio, 'purple', linewidth=2)
+         ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')
+         ax2.set_xlabel('Iteration', fontsize=11)
+         ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)
+         ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')
+         ax2.legend(fontsize=10)
+         ax2.grid(True, alpha=0.3)
+         
+         # Subplot 3: Density evolution
+         ax3 = fig.add_subplot(gs[0, 2])
+         ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean œÅ')
+         ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max œÅ')
+         ax3.set_xlabel('Iteration', fontsize=11)
+         ax3.set_ylabel('Density', fontsize=11)
+         ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')
+         ax3.legend(fontsize=10)
+         ax3.grid(True, alpha=0.3)
+         
+         plt.tight_layout()
+         plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')
+         plt.close()
+         
+         print(f"Training diagnostic plot saved to {self.save_dir}")
+ 
+     # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================
+     
+     def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=80, n_temporal=60):
+         """
+         Compute PDE residuals on a regular grid over space-time.
+         Critical for identifying WHERE and WHEN physics breaks down.
+         """
+         from core.losses import pde_residue
+         
+         device = next(model.parameters()).device
+         
+         print(f"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...")
+         
+         if dimension == 2:
+             x = torch.linspace(0, 1, n_spatial, device=device)
+             y = torch.linspace(0, 1, n_spatial, device=device)
+             t = torch.linspace(0, tmax, n_temporal, device=device)
+             
+             # Create meshgrid
+             X, Y, T = torch.meshgrid(x, y, t, indexing='ij')
+             colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
+             
+             with torch.no_grad():
+                 rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)
+             
+             # Reshape back to grid
+             rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
+             vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
+             vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
+             phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
+             
+             return {
+                 'x': x.cpu().numpy(),
+                 'y': y.cpu().numpy(),
+                 't': t.cpu().numpy(),
+                 'rho_residual': rho_r,
+                 'vx_residual': vx_r,
+                 'vy_residual': vy_r,
+                 'phi_residual': phi_r
+             }
+         
+         elif dimension == 3:
+             # For 3D, use coarser grid to save memory
+             n_spatial_3d = max(32, n_spatial // 2)
+             x = torch.linspace(0, 1, n_spatial_3d, device=device)
+             y = torch.linspace(0, 1, n_spatial_3d, device=device)
+             z = torch.linspace(0, 1, n_spatial_3d, device=device)
+             t = torch.linspace(0, tmax, n_temporal, device=device)
+             
+             # For 3D, we'll slice at z=0.5 to create 2D heatmaps
+             z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)
+             X, Y, T = torch.meshgrid(x, y, t, indexing='ij')
+             colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), z_slice.reshape(-1, 1), T.reshape(-1, 1)]
+             
+             with torch.no_grad():
+                 rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)
+             
+             rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
+             vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
+             vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
+             phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
+             
+             return {
+                 'x': x.cpu().numpy(),
+                 'y': y.cpu().numpy(),
+                 't': t.cpu().numpy(),
+                 'rho_residual': rho_r,
+                 'vx_residual': vx_r,
+                 'vy_residual': vy_r,
+                 'phi_residual': phi_r
+             }
+     
+     def plot_residual_heatmaps(self, residual_data, slice_idx=None):
+         """
+         Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.
+         Shows WHEN and WHERE each equation fails.
+         """
+         if slice_idx is None:
+             slice_idx = residual_data['rho_residual'].shape[1] // 2
+         
+         fig = plt.figure(figsize=(16, 10))
+         gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
+         
+         residuals = [
+             ('Continuity Equation', residual_data['rho_residual']),
+             ('Momentum X Equation', residual_data['vx_residual']),
+             ('Momentum Y Equation', residual_data['vy_residual']),
+             ('Poisson Equation', residual_data['phi_residual'])
+         ]
+         
+         for idx, (name, resid) in enumerate(residuals):
+             ax = fig.add_subplot(gs[idx // 2, idx % 2])
+             
+             # Slice at fixed y
+             resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow
+             
+             # Use log scale for better visualization
+             resid_log = np.log10(np.abs(resid_slice) + 1e-12)
+             
+             im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',
+                           extent=[residual_data['x'][0], residual_data['x'][-1],
+                                  residual_data['t'][0], residual_data['t'][-1]],
+                           vmin=-10, vmax=0)
+             ax.set_xlabel('x', fontsize=11)
+             ax.set_ylabel('Time', fontsize=11)
+             ax.set_title(f'{name} Residual (log‚ÇÅ‚ÇÄ)', fontsize=12, fontweight='bold')
+             cbar = plt.colorbar(im, ax=ax)
+             cbar.set_label('log‚ÇÅ‚ÇÄ|residual|', fontsize=10)
+         
+         plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data["y"][slice_idx]:.2f})', 
+                     fontsize=14, fontweight='bold', y=0.995)
+         plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')
+         plt.close()
+     
+     def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=100):
+         """
+         Check if mass and momentum are conserved over time.
+         Critical for physical consistency - should be flat lines!
+         """
+         device = next(model.parameters()).device
+         
+         print(f"  Checking conservation laws at {n_times} time points...")
+         
+         conservation_data = {
+             'times': [],
+             'total_mass': [],
+             'total_momentum_x': [],
+             'total_momentum_y': []
+         }
+         
+         time_points = np.linspace(0, tmax, n_times)
+         
+         for t_val in time_points:
+             if dimension == 2:
+                 x = torch.linspace(0, 1, n_grid, device=device)
+                 y = torch.linspace(0, 1, n_grid, device=device)
+                 X, Y = torch.meshgrid(x, y, indexing='ij')
+                 T = torch.full_like(X, t_val)
+                 
+                 colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
+                 
+                 with torch.no_grad():
+                     pred = model(colloc)
+                     rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()
+                     vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()
+                     vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()
+                 
+                 # Integrate using trapezoidal rule
+                 dx = 1.0 / (n_grid - 1)
+                 total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)
+                 total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)
+                 total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)
+                 
+                 conservation_data['times'].append(t_val)
+                 conservation_data['total_mass'].append(total_mass)
+                 conservation_data['total_momentum_x'].append(total_px)
+                 conservation_data['total_momentum_y'].append(total_py)
+             
+             elif dimension == 3:
+                 # For 3D, use coarser grid
+                 n_grid_3d = max(40, n_grid // 2)
+                 x = torch.linspace(0, 1, n_grid_3d, device=device)
+                 y = torch.linspace(0, 1, n_grid_3d, device=device)
+                 z = torch.linspace(0, 1, n_grid_3d, device=device)
+                 X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')
+                 T = torch.full_like(X, t_val)
+                 
+                 colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]
+                 
+                 with torch.no_grad():
+                     pred = model(colloc)
+                     rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()
+                     vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()
+                     vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()
+                 
+                 dx = 1.0 / (n_grid_3d - 1)
+                 total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)
+                 total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)
+                 total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)
+                 
+                 conservation_data['times'].append(t_val)
+                 conservation_data['total_mass'].append(total_mass)
+                 conservation_data['total_momentum_x'].append(total_px)
+                 conservation_data['total_momentum_y'].append(total_py)
+         
+         return conservation_data
+     
+     def plot_conservation_laws(self, conservation_data):
+         """Plot conserved quantities over time - should be flat lines!"""
+         fig = plt.figure(figsize=(16, 5))
+         gs = GridSpec(1, 3, figure=fig)
+         
+         times = conservation_data['times']
+         
+         # Mass conservation
+         ax1 = fig.add_subplot(gs[0, 0])
+         mass = np.array(conservation_data['total_mass'])
+         mass_initial = mass[0]
+         mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift
+         ax1.plot(times, mass_drift, 'b-', linewidth=2.5)
+         ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')
+         ax1.set_xlabel('Time', fontsize=11)
+         ax1.set_ylabel('Mass drift (%)', fontsize=11)
+         ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')
+         ax1.legend(fontsize=10)
+         ax1.grid(True, alpha=0.3)
+         
+         # Momentum X conservation
+         ax2 = fig.add_subplot(gs[0, 1])
+         px = np.array(conservation_data['total_momentum_x'])
+         px_initial = px[0] if abs(px[0]) &gt; 1e-10 else 1.0
+         px_drift = ((px - px_initial) / abs(px_initial)) * 100
+         ax2.plot(times, px_drift, 'g-', linewidth=2.5)
+         ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')
+         ax2.set_xlabel('Time', fontsize=11)
+         ax2.set_ylabel('Momentum X drift (%)', fontsize=11)
+         ax2.set_title('Momentum X Conservation', fontsize=12, fontweight='bold')
+         ax2.legend(fontsize=10)
+         ax2.grid(True, alpha=0.3)
+         
+         # Momentum Y conservation
+         ax3 = fig.add_subplot(gs[0, 2])
+         py = np.array(conservation_data['total_momentum_y'])
+         py_initial = py[0] if abs(py[0]) &gt; 1e-10 else 1.0
+         py_drift = ((py - py_initial) / abs(py_initial)) * 100
+         ax3.plot(times, py_drift, 'orange', linewidth=2.5)
+         ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')
+         ax3.set_xlabel('Time', fontsize=11)
+         ax3.set_ylabel('Momentum Y drift (%)', fontsize=11)
+         ax3.set_title('Momentum Y Conservation', fontsize=12, fontweight='bold')
+         ax3.legend(fontsize=10)
+         ax3.grid(True, alpha=0.3)
+         
+         plt.tight_layout()
+         plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')
+         plt.close()
+     
+     def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):
+         """
+         Compute FFT of density field at different times.
+         Reveals spectral bias and high-frequency mode damping.
+         """
+         device = next(model.parameters()).device
+         
+         print(f"  Computing spectral content at {n_times} time points...")
+         
+         time_points = np.linspace(0, tmax, n_times)
+         spectra = []
+         
+         for t_val in time_points:
+             if dimension == 2:
+                 x = torch.linspace(0, 1, n_grid, device=device)
+                 y = torch.linspace(0, 1, n_grid, device=device)
+                 X, Y = torch.meshgrid(x, y, indexing='ij')
+                 T = torch.full_like(X, t_val)
+                 
+                 colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
+                 
+                 with torch.no_grad():
+                     pred = model(colloc)
+                     rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()
+                 
+                 # 2D FFT
+                 fft = np.fft.fft2(rho)
+                 power = np.abs(np.fft.fftshift(fft))**2
+                 
+                 # Create wavenumber grid
+                 kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)
+                 ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)
+                 kx_shift = np.fft.fftshift(kx)
+                 ky_shift = np.fft.fftshift(ky)
+                 KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')
+                 K = np.sqrt(KX**2 + KY**2)
+                 
+                 spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})
+             
+             elif dimension == 3:
+                 # For 3D, take a 2D slice at z=0.5
+                 n_grid_3d = max(64, n_grid // 2)
+                 x = torch.linspace(0, 1, n_grid_3d, device=device)
+                 y = torch.linspace(0, 1, n_grid_3d, device=device)
+                 X, Y = torch.meshgrid(x, y, indexing='ij')
+                 Z = torch.full_like(X, 0.5)
+                 T = torch.full_like(X, t_val)
+                 
+                 colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]
+                 
+                 with torch.no_grad():
+                     pred = model(colloc)
+                     rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()
+                 
+                 fft = np.fft.fft2(rho)
+                 power = np.abs(np.fft.fftshift(fft))**2
+                 
+                 kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)
+                 ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)
+                 kx_shift = np.fft.fftshift(kx)
+                 ky_shift = np.fft.fftshift(ky)
+                 KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')
+                 K = np.sqrt(KX**2 + KY**2)
+                 
+                 spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})
+         
+         return spectra
+     
+     def plot_spectral_evolution(self, spectra_data):
+         """
+         Plot power spectrum evolution over time.
+         Shows if high-frequency modes are being damped (spectral bias).
+         """
+         fig = plt.figure(figsize=(16, 5))
+         gs = GridSpec(1, 2, figure=fig)
+         
+         # Left: Radially averaged power spectra
+         ax1 = fig.add_subplot(gs[0, 0])
+         
+         colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))
+         
+         for idx, spec in enumerate(spectra_data):
+             # Radial binning
+             k_max = np.max(spec['k'])
+             k_bins = np.linspace(0, k_max, 25)
+             power_avg = []
+             k_centers = []
+             
+             for i in range(len(k_bins)-1):
+                 mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])
+                 if mask.any():
+                     power_avg.append(np.mean(spec['power'][mask]))
+                     k_centers.append((k_bins[i] + k_bins[i+1]) / 2)
+             
+             if len(k_centers) &gt; 0:
+                 ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], 
+                           linewidth=2, markersize=4, label=f"t={spec['time']:.2f}")
+         
+         ax1.set_xlabel('Wavenumber k', fontsize=11)
+         ax1.set_ylabel('Power', fontsize=11)
+         ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')
+         ax1.legend(fontsize=9, loc='best')
+         ax1.grid(True, alpha=0.3, which='both')
+         
+         # Right: 2D power spectrum at final time
+         ax2 = fig.add_subplot(gs[0, 1])
+         final_spec = spectra_data[-1]
+         power_log = np.log10(final_spec['power'] + 1e-12)
+         
+         im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',
+                        extent=[-np.max(final_spec['k']), np.max(final_spec['k']),
+                               -np.max(final_spec['k']), np.max(final_spec['k'])])
+         ax2.set_xlabel('k_x', fontsize=11)
+         ax2.set_ylabel('k_y', fontsize=11)
+         ax2.set_title(f'2D Power Spectrum at t={final_spec["time"]:.2f}', 
+                      fontsize=12, fontweight='bold')
+         cbar = plt.colorbar(im, ax=ax2)
+         cbar.set_label('log‚ÇÅ‚ÇÄ(Power)', fontsize=10)
+         
+         plt.tight_layout()
+         plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')
+         plt.close()
+     
+     def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):
+         """
+         Compute field statistics over time to track error accumulation.
+         """
+         device = next(model.parameters()).device
+         
+         print(f"  Computing temporal statistics at {n_times} time points...")
+         
+         time_points = np.linspace(0, tmax, n_times)
+         stats = {
+             'times': [],
+             'rho_mean': [],
+             'rho_std': [],
+             'rho_max': [],
+             'grad_rho_mean': [],
+             'grad_rho_max': []
+         }
+         
+         for t_val in time_points:
+             if dimension == 2:
+                 x = torch.linspace(0, 1, n_spatial, device=device)
+                 y = torch.linspace(0, 1, n_spatial, device=device)
+                 X, Y = torch.meshgrid(x, y, indexing='ij')
+                 T = torch.full_like(X, t_val)
+                 
+                 X_flat = X.reshape(-1, 1)
+                 Y_flat = Y.reshape(-1, 1)
+                 T_flat = T.reshape(-1, 1)
+                 
+                 X_flat.requires_grad_(True)
+                 Y_flat.requires_grad_(True)
+                 
+                 colloc = [X_flat, Y_flat, T_flat]
+                 
+                 pred = model(colloc)
+                 rho = pred[:, 0:1]
+                 
+                 # Compute gradients
+                 grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False)[0]
+                 grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]
+                 grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)
+                 
+                 rho_np = rho.detach().cpu().numpy().flatten()
+                 grad_np = grad_mag.detach().cpu().numpy().flatten()
+                 
+                 stats['times'].append(t_val)
+                 stats['rho_mean'].append(np.mean(rho_np))
+                 stats['rho_std'].append(np.std(rho_np))
+                 stats['rho_max'].append(np.max(rho_np))
+                 stats['grad_rho_mean'].append(np.mean(grad_np))
+                 stats['grad_rho_max'].append(np.max(grad_np))
+         
+         return stats
+     
+     def plot_temporal_statistics(self, stats_data):
+         """
+         Plot field statistics over time.
+         Shows if solution becomes unphysical or gradients explode.
+         """
+         fig = plt.figure(figsize=(16, 5))
+         gs = GridSpec(1, 3, figure=fig)
+         
+         times = stats_data['times']
+         
+         # Density statistics
+         ax1 = fig.add_subplot(gs[0, 0])
+         ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')
+         ax1.fill_between(times,
+                          np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),
+                          np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),
+                          alpha=0.3, color='b', label='¬±1 std')
+         ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')
+         ax1.set_xlabel('Time', fontsize=11)
+         ax1.set_ylabel('Density', fontsize=11)
+         ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')
+         ax1.legend(fontsize=10)
+         ax1.grid(True, alpha=0.3)
+         
+         # Gradient statistics
+         ax2 = fig.add_subplot(gs[0, 1])
+         ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')
+         ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')
+         ax2.set_xlabel('Time', fontsize=11)
+         ax2.set_ylabel('|‚àáœÅ| (log scale)', fontsize=11)
+         ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')
+         ax2.legend(fontsize=10)
+         ax2.grid(True, alpha=0.3, which='both')
+         
+         # Growth rate analysis
+         ax3 = fig.add_subplot(gs[0, 2])
+         rho_max = np.array(stats_data['rho_max'])
+         if len(rho_max) &gt; 5:
+             # Fit exponential growth
+             log_rho = np.log(rho_max + 1e-10)
+             valid_idx = np.isfinite(log_rho)
+             if valid_idx.sum() &gt; 5:
+                 times_valid = np.array(times)[valid_idx]
+                 log_rho_valid = log_rho[valid_idx]
+                 coeffs = np.polyfit(times_valid, log_rho_valid, 1)
+                 growth_rate = coeffs[0]
+                 fit = np.exp(np.poly1d(coeffs)(times))
+                 
+                 ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max œÅ')
+                 ax3.semilogy(times, fit, 'r--', linewidth=2, 
+                            label=f'Exp fit (Œ≥={growth_rate:.4f})')
+                 ax3.set_xlabel('Time', fontsize=11)
+                 ax3.set_ylabel('Max Density (log scale)', fontsize=11)
+                 ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')
+                 ax3.legend(fontsize=10)
+                 ax3.grid(True, alpha=0.3, which='both')
+         
+         plt.tight_layout()
+         plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')
+         plt.close()
+     
+     def run_comprehensive_diagnostics(self, model, dimension, tmax):
+         """
+         Run all post-training diagnostics for high-tmax failure analysis.
+         
+         Generates 5 critical diagnostic plots:
+         1. Training diagnostics (loss, balance, density) - already generated
+         2. PDE residual heatmaps - WHERE/WHEN physics breaks
+         3. Conservation laws - mass/momentum drift
+         4. Spectral evolution - frequency content and spectral bias
+         5. Temporal statistics - field evolution and growth rates
+         
+         Args:
+             model: Trained PINN model
+             dimension: Spatial dimension (2 or 3)
+             tmax: Maximum time to analyze
+         """
+         print("\n" + "="*70)
+         print("  Running Comprehensive High-Tmax Diagnostics")
+         print("="*70)
+         
+         # Plot 2: PDE Residual Heatmaps
+         print("\n[1/4] Computing PDE residual heatmaps...")
+         residuals = self.compute_residual_heatmap(model, dimension, tmax)
+         self.plot_residual_heatmaps(residuals)
+         print("      ‚úì Residual heatmaps saved")
+         
+         # Plot 3: Conservation Laws
+         print("\n[2/4] Checking conservation laws...")
+         conservation = self.check_conservation_laws(model, dimension, tmax)
+         self.plot_conservation_laws(conservation)
+         print("      ‚úì Conservation laws plot saved")
+         
+         # Plot 4: Spectral Evolution
+         print("\n[3/4] Computing spectral evolution...")
+         spectra = self.compute_spectral_content(model, dimension, tmax)
+         self.plot_spectral_evolution(spectra)
+         print("      ‚úì Spectral evolution plot saved")
+         
+         # Plot 5: Temporal Statistics
+         print("\n[4/4] Computing temporal statistics...")
+         stats = self.compute_temporal_statistics(model, dimension, tmax)
+         self.plot_temporal_statistics(stats)
+         print("      ‚úì Temporal statistics plot saved")
+         
+         print("\n" + "="*70)
+         print(f"  All diagnostics saved to: {self.save_dir}")
+         print("="*70)
+         print("\nDiagnostic Summary:")
+         print("  1. training_diagnostics.png - Training convergence analysis")
+         print("  2. residual_heatmaps.png - Spatiotemporal PDE violations")
+         print("  3. conservation_laws.png - Mass/momentum conservation drift")
+         print("  4. spectral_evolution.png - Frequency content and spectral bias")
+         print("  5. temporal_statistics.png - Field evolution and growth rates")
+         print("="*70 + "\n")
+ 
```

</details>

</tool-use>

---

Now let me update the imports in trainer.py:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** ‚Ä¢ Edit file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\trainer.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -7,5 +7,5 @@
  import numpy as np
  import torch
- from utilities.training_diagnostics import TrainingDiagnostics
+ from training.training_diagnostics import TrainingDiagnostics
  from config import BATCH_SIZE, NUM_BATCHES, ENABLE_TRAINING_DIAGNOSTICS
  from training.physics import closure_batched
```

</details>

</tool-use>

---

Now update physics.py:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** ‚Ä¢ Edit file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\physics.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -6,5 +6,5 @@
  from torch.autograd import Variable
  from core.losses import ASTPN, pde_residue
- from utilities.training_diagnostics import TrainingDiagnostics
+ from training.training_diagnostics import TrainingDiagnostics
  from core.data_generator import diff
  from core.model_architecture import PINN
```

</details>

</tool-use>

---

Now update package_for_kaggle.py:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** ‚Ä¢ Edit file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\package_for_kaggle.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -33,5 +33,5 @@
      ("core/losses.py", "core.losses"),
      ("core/model_architecture.py", "core.model_architecture"),
-     ("utilities/training_diagnostics.py", "utilities.training_diagnostics"),
+     ("training/training_diagnostics.py", "training.training_diagnostics"),
      ("training/physics.py", "training.physics"),
      ("training/trainer.py", "training.trainer"),
```

</details>

</tool-use>

---

Now update utilities/__init__.py:

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** ‚Ä¢ Edit file: C:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\utilities\__init__.py</summary>

**Chunk 1**
Lines added: undefined, lines removed: 1

```diff
@@ -1,5 +1,4 @@
  # Utility scripts for analysis and diagnostics
  # - analyze_numerical: Numerical solution analysis
- # - training_diagnostics: Training metrics and plots
  # - compare_model: Model comparison tools
  # - find_good_seeds: Seed search utilities
```

</details>

</tool-use>

---

Now delete the old file from utilities:

---

<tool-use data-tool-type="write" data-tool-name="delete_file">
<details><summary>Tool use: **delete_file**</summary>

Explanation: Delete the original training_diagnostics.py file from utilities folder since it has been moved to training folder


</details>
</tool-use>

---

Perfect! Now let me update the todos and create a comprehensive summary:

---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** ‚Ä¢ Todo List</summary>

- [x] Move training_diagnostics.py from utilities to training folder
- [x] Update import in training/trainer.py from utilities to local
- [x] Update import in training/physics.py from utilities to local
- [x] Update module path in package_for_kaggle.py to reflect new location
- [x] Update utilities/__init__.py to remove training_diagnostics reference

</details>
</tool-use>

---

Perfect! Now let me create a comprehensive summary document:

## Summary: Moving `training_diagnostics.py` to Training Folder

I've successfully moved `training_diagnostics.py` from `utilities/` to `training/` and updated all dependencies. Here's what was done:

### **Files Changed:**

1. **Created**: `training/training_diagnostics.py` ‚úì
   - Moved the entire `TrainingDiagnostics` class with all diagnostics functionality

2. **Updated Imports**:
   - `training/trainer.py` (line 9): Changed `from utilities.training_diagnostics` ‚Üí `from training.training_diagnostics`
   - `training/physics.py` (line 8): Changed `from utilities.training_diagnostics` ‚Üí `from training.training_diagnostics`
   - `package_for_kaggle.py` (line 35): Updated module path from `"utilities/training_diagnostics.py"` ‚Üí `"training/training_diagnostics.py"`
   - `utilities/__init__.py`: Removed the reference to `training_diagnostics` from comments

3. **Deleted**: `utilities/training_diagnostics.py` ‚úì

### **Why This Move Makes Sense:**

`training_diagnostics.py` is **critically needed during training** and belongs in the `training/` folder because:

1. **Direct Usage in Training Pipeline**: 
   - `trainer.py` instantiates `TrainingDiagnostics` every time training runs (when `ENABLE_TRAINING_DIAGNOSTICS=True`)
   - Logs metrics every 50 iterations during both Adam and L-BFGS optimization phases

2. **Tight Integration**:
   - Both `trainer.py` and `physics.py` import it
   - Works closely with the training loop, not as a standalone utility

3. **Functional Purpose**: 
   - It provides real-time training monitoring and post-training analysis
   - Essential for diagnosing high-tmax failures (5 diagnostic plots)

### **Other Files in `utilities/` (Diagnostic/Analysis Tools):**

These remain in `utilities/` because they are **optional analysis scripts** for post-hoc investigation:

- `analyze_numerical.py` - Analysis of numerical solutions
- `compare_initial_phi.py` - Comparison utilities
- `compare_laplacian_operators_gpu.py` - GPU operator comparison
- `compare_model.py` - Model comparison tools
- `compare_models_with_fd.py` - FD comparison
- `diagnose_comparison.py` - Diagnostic comparisons
- `fd_diagnostics.py` - FD diagnostics
- `find_good_seeds.py` - Seed search utilities
- `test_*.py` files - Testing/validation scripts

### **Configuration Parameter:**

`config.py` has one relevant parameter (line 58):

```python
ENABLE_TRAINING_DIAGNOSTICS = True  # Enable automatic training diagnostics plots and logging
```

This is the **only config option** related to the training diagnostics system.

### **Files Using `training_diagnostics`:**

1. `train.py` - Main script (receives diagnostics object from `train()` function)
2. `training/trainer.py` - Creates and manages diagnostics during training
3. `training/physics.py` - Imports but doesn't directly use it (keeps import for consistency)
4. `package_for_kaggle.py` - Updated module path for Kaggle packaging

All imports and references have been updated. The training diagnostics is now logically located in the `training/` folder where it belongs! üéØ

---

