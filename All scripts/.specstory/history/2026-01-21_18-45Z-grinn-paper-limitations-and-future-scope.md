<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 67c252ec-3c53-468a-a266-e56c479bfe28 (2026-01-21 18:45Z) -->

# GRINN paper limitations and future scope (2026-01-21 18:45Z)

_**User (2026-01-21 18:45Z)**_

The attached img is a section from the GRINN paper, which was the first paper showing the ability of PINNs to learn molecular cloud collapse. That paper focused on the 2D and 3D sinusoidal case in the current codebase. In the context of the image and the current codebase, what do these limitations map to? like how would one go on trying to tackle these issues or explore what the paper couldnt. No need of any codes just a general idea is sufficient

---

_**Agent (model claude-4.5-sonnet-thinking, mode Ask)**_

<think><details><summary>Thought Process</summary>
The user has shared an image showing section 5.4 "Limitations and future scope" from the GRINN paper. They want to understand how these limitations map to the current codebase and how one might tackle these issues.

Let me first read the current config file to understand the codebase better, then I'll analyze the limitations mentioned in the paper excerpt.

From the image, the key limitations mentioned are:
1. Dependence on hyperparameter choice (hidden layers, neurons, activation functions) - lack of scaling laws
2. GRINN struggles to converge when space and time extent increase (specifically for t > 7)
3. Domain decomposition and evolutionary deep neural networks could help
4. Extension to surrogate modeling - training on a range of parameters/ICs to predict behavior without solving PDEs each time

Let me read the config file and explore the codebase to understand the current setup.</details></think>

---

I'll help you understand how the GRINN paper's limitations map to your current codebase. Let me first examine the key configuration and architecture files to provide context.

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: config.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: core/model_architecture.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: train.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Good, I now have a clear picture of the codebase. Let me also check what the current data generation and initial conditions look like to fully understand the scope.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: core/initial_conditions.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: training/trainer.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Perfect! Now I have a comprehensive understanding of the codebase. Let me map the limitations from the GRINN paper to the current codebase and provide actionable insights on how to tackle these issues.

The key limitations from the paper are:
1. **Hyperparameter dependence** - lack of scaling laws for PINNs
2. **Convergence issues for large spatial/temporal domains** (especially t > 7)
3. **Potential solutions**: domain decomposition, evolutionary neural networks
4. **Extension to surrogate modeling** - training on a range of parameters/ICs

Now let me provide a comprehensive answer about how these map to the current codebase.</details></think>

---

Based on the GRINN paper excerpt and your current codebase, here's how the limitations map to your implementation and potential approaches to tackle them:

## 1. **Hyperparameter Dependence (Network Architecture)**

**Current State in Your Codebase:**
- Fixed architecture: `num_neurons = 64`, `num_layers = 5`, `harmonics = 3`
- Single activation function: `DEFAULT_ACTIVATION = 'sin'`
- Manual trial-and-error required for different problem scales

**What This Limitation Means:**
The paper mentions there are no established scaling laws for PINNs - meaning you don't know if you need 3 layers or 10 layers, 32 neurons or 128 neurons for a given problem. Your current setup works for the cases tested, but might fail for:
- Larger domains (`num_of_waves > 2`)
- Longer time horizons (`tmax > 3`)
- More complex initial conditions (higher `POWER_EXPONENT` values)

**Potential Approaches:**
- **Systematic hyperparameter search**: Create a grid search script that varies `num_neurons`, `num_layers`, and `harmonics` for different `tmax` values and tracks convergence metrics
- **Adaptive architecture**: Start with a smaller network and dynamically add layers/neurons during training if residuals remain high
- **Neural Architecture Search (NAS)**: Use evolutionary algorithms or reinforcement learning to automatically discover optimal architectures for different problem scales
- **Activation function studies**: Your codebase already supports multiple activations (`sin`, `tanh`, `relu`, `elu`) - systematically test which works best for different time scales

## 2. **Temporal/Spatial Domain Scaling (t > 7 Failure)**

**Current State in Your Codebase:**
- You have `t_upto_3/` and `t_upto_6/` diagnostic folders, suggesting you've already encountered issues at larger `tmax`
- Current training uses: `tmax = 2.0`, which is conservative
- The paper mentions GRINN struggles for `t > 7`

**What This Limitation Means:**
As you increase the simulation time or spatial extent, the network needs to learn increasingly complex spatiotemporal patterns. The fixed collocation point strategy (`N_0 = 40000`, `N_r = 40000`) doesn't adapt to increased domain complexity.

**Potential Approaches:**

### a) **Domain Decomposition (XPINN-style)**
The paper explicitly mentions this. Your codebase has a `methods/` folder (currently empty) - this is where you'd implement:
- **Temporal decomposition**: Break `[0, tmax]` into overlapping windows `[0, 3]`, `[2.5, 5.5]`, `[5, 8]`, etc.
- **Spatial decomposition**: For `num_of_waves > 1`, split domain into subregions with interface matching conditions
- Train separate networks for each subdomain and enforce continuity at boundaries
- This allows specialization - each network only needs to learn local behavior

### b) **Progressive Time Training**
Instead of training on `[0, tmax]` all at once:
```
Stage 1: Train on t ∈ [0, 2] until convergence
Stage 2: Freeze lower layers, extend to t ∈ [0, 4]
Stage 3: Continue extending...
```
This prevents the network from "forgetting" early-time behavior when learning late-time dynamics.

### c) **Adaptive Collocation Point Sampling**
Your current `ASTPN` uses uniform sampling. For large `tmax`:
- Sample more densely in regions of high PDE residuals (use your `residual_heatmaps.png` diagnostics to identify these)
- Implement importance sampling based on solution gradients
- Increase `N_r` dynamically as training progresses

### d) **Causality-Aware Parameterization**
You already have `STARTUP_DT = 0.01` and `USE_PARAMETERIZATION = "exponential"` - these are excellent! For larger `tmax`:
- Use time-dependent weights: penalize late-time residuals more heavily
- Implement multi-scale time encodings: add features like `sin(ωt)` for multiple frequencies `ω` to capture both fast and slow dynamics

## 3. **Extension to Surrogate Modeling**

**Current State:**
- Your code is currently a "forward solver" - given one set of ICs, solve the PDEs
- Each new IC requires full retraining

**What Surrogate Modeling Would Add:**
Instead of training for a single `(a, rho_1, POWER_EXPONENT)`, train on a distribution:
- Input to network: `[x, y, z, t, a, rho_1, power_exp, ...]` (parameters become inputs)
- Output: `[ρ, vx, vy, vz, φ]` as before
- Train on multiple scenarios simultaneously

**Potential Approaches:**

### a) **Parameter-Conditioned PINN**
Modify your `PINN` architecture:
```python
# Current: forward(self, X) where X = [x, y, z, t]
# Surrogate: forward(self, X, params) where params = [a, rho_1, power_exp, ...]
```

Train on a dataset of initial conditions:
- Sample 50-100 different ICs from a distribution
- For each IC, generate collocation points
- Network learns the mapping: (space, time, IC_params) → (fields)

### b) **Transfer Learning**
Your codebase already saves models (`model.pth`). For surrogate-like behavior without full retraining:
- Train a "base" model on `a=0.05`, `tmax=2`
- For new parameter `a=0.1`, initialize from base model and fine-tune
- This is much faster than training from scratch

### c) **Reduced-Order Modeling Integration**
Instead of pure PINN surrogate:
- Use PINNs to learn a correction to a fast approximate solver
- Run your LAX solver (`LAX_torch.py`) to get a coarse solution
- Train PINN to learn the residual: `solution_true = solution_LAX + correction_PINN`
- This hybrid approach is faster and more accurate

## 4. **Evolutionary Deep Neural Networks**

**What This Means:**
Network architecture evolves during training - not just weights, but topology changes.

**Potential Approaches:**
- **Layer pruning**: Start with `num_layers=10`, remove layers that don't contribute to loss reduction
- **Neuron growth**: Monitor gradient magnitudes; add neurons to layers with saturated activations
- **Network morphism**: Dynamically widen or deepen network while preserving function

## **Concrete Next Steps for Your Codebase:**

Given your existing infrastructure, here's a prioritized roadmap:

### **Short-term (Immediate Exploration):**
1. **Systematic tmax scaling study**: Run training for `tmax = [2, 3, 4, 5, 6, 7, 8]` with current architecture and document where/how it fails (use your diagnostic plots)
2. **Hyperparameter sensitivity**: Test `num_layers = [3, 5, 7, 10]` and `num_neurons = [32, 64, 128, 256]` at `tmax=4`
3. **Analyze failure modes**: Use your `residual_heatmaps.png` to identify if failures are temporal (late-time error growth) or spatial (specific regions)

### **Medium-term (Research Extensions):**
1. **Implement temporal domain decomposition**: Create a `methods/temporal_decomposition.py` that trains 3 networks on overlapping time windows
2. **Adaptive collocation**: Modify `ASTPN` to resample based on residual magnitudes every N iterations
3. **Progressive training**: Add curriculum learning to `trainer.py` - gradually extend `tmax` during training

### **Long-term (Paper-worthy Contributions):**
1. **Surrogate model**: Extend to parameter-conditioned PINN for astrophysics parameter studies
2. **Hybrid PINN-FD**: Combine your LAX solver with PINN corrections
3. **Scaling laws discovery**: Use your systematic studies to derive empirical scaling laws (e.g., "for `tmax=T`, need `num_neurons ~ 64*sqrt(T)`")

The beauty of your current codebase is that it's already well-structured for these extensions - your modular design with separate `core/`, `training/`, `numerical_solvers/` makes it easy to implement these ideas without major refactoring.

---

