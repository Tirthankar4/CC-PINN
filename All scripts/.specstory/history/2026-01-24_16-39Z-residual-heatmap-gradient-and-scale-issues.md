<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 8bb6dcb1-4fec-409b-80d7-519f0e4d1f43 (2026-01-24 16:39Z) -->

# Residual heatmap gradient and scale issues (2026-01-24 16:39Z)

_**User (2026-01-24 16:39Z)**_

Based on the script, here are the **specific edits** needed to fix the residual heatmap black/yellow issue:

## **Problem 1: `compute_residual_heatmap` function (around line 100-150)**

### Current (BROKEN):
```python
def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):
    ...
    if dimension == 2:
        ...
        colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
        
        with torch.no_grad():  # ❌ THIS BREAKS DERIVATIVES
            rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)
```

### Fixed:
```python
def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):
    ...
    if dimension == 2:
        ...
        # ✅ ADD requires_grad=True to enable derivative computation
        colloc = [
            X.reshape(-1, 1).requires_grad_(True),
            Y.reshape(-1, 1).requires_grad_(True),
            T.reshape(-1, 1).requires_grad_(True)
        ]
        
        # ✅ REMOVE with torch.no_grad() or use normal scope
        rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)
```

**Apply same fix for dimension == 3 case** (add `.requires_grad_(True)` and remove `with torch.no_grad()`)

***

## **Problem 2: `plot_residual_heatmaps` function (around line 200-250)**

### Current (MISLEADING):
```python
def plot_residual_heatmaps(self, residual_data, slice_idx=None):
    ...
    for idx, (name, resid) in enumerate(residuals):
        ...
        resid_log = np.log10(np.abs(resid_slice) + 1e-12)
        
        im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',
                      extent=[...],
                      vmin=-10, vmax=0)  # ❌ FIXED SCALE HIDES STRUCTURE
```

### Fixed (Option A - Dynamic scale):
```python
def plot_residual_heatmaps(self, residual_data, slice_idx=None):
    ...
    for idx, (name, resid) in enumerate(residuals):
        ...
        resid_log = np.log10(np.abs(resid_slice) + 1e-16)
        
        # ✅ DYNAMIC VMIN/VMAX
        data_min = np.min(resid_log)
        data_max = np.max(resid_log)
        
        im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',
                      extent=[...],
                      vmin=data_min, vmax=data_max)
        
        ax.set_title(f'{name} Residual\nMin: 10^{data_min:.1f}, Max: 10^{data_max:.1f}',
                     fontsize=12, fontweight='bold')
```

### Fixed (Option B - Adaptive with floor):
```python
        # ✅ ADAPTIVE with sensible floor (shows relative structure)
        data_max = np.max(resid_log)
        data_min = max(data_max - 4, -12)  # Show 4 orders of magnitude or floor at 1e-12
        
        im = ax.imshow(resid_log, ..., vmin=data_min, vmax=data_max)
```

***

## **Summary of Changes:**

### File: `training_diagnostics.py`

1. **Line ~120-145** (in `compute_residual_heatmap` for 2D):
   - Change: `colloc = [X.reshape(-1, 1), ...]`
   - To: `colloc = [X.reshape(-1, 1).requires_grad_(True), ...]`
   - **Remove** `with torch.no_grad():`

2. **Line ~150-170** (in `compute_residual_heatmap` for 3D):
   - Same fix: add `.requires_grad_(True)` to all collocation tensors
   - **Remove** `with torch.no_grad():`

3. **Line ~220-240** (in `plot_residual_heatmaps` inside the loop):
   - Add before `ax.imshow`:
     ```python
     data_min = np.min(resid_log)
     data_max = np.max(resid_log)
     ```
   - Change: `vmin=-10, vmax=0`
   - To: `vmin=data_min, vmax=data_max`
   - Update title to show actual ranges:
     ```python
     ax.set_title(f'{name} Residual\nMin: 10^{data_min:.1f}, Max: 10^{data_max:.1f}', ...)
     ```

These three changes will fix both the "black plots" (caused by disabled gradients returning zeros) and the "misleading yellow" (caused by fixed color scale hiding the actual residual structure).

Do these fixes to the @training/training_diagnostics.py 

---

_**Agent (model gpt-5.2-codex, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Applying targeted code edits**

I'm preparing to update the file by replacing torch.no_grad blocks, ensuring requires_grad is set for 3D cases, adjusting log epsilon to 1e-16, and updating the title before running read_lints.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "training/training_diagnostics.py"
}
```
Additional data:

```json
{
  "codeblockId": "5bbacaf4-662c-451e-9c50-129fef9e538e",
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/training/training_diagnostics.py",
      "query": "",
      "fragment": "",
      "_formatted": "file:///c%3A/Users/tirth/Documents/Projects/GRINN/Power%20spectrum%20perturbations/training/training_diagnostics.py",
      "_fsPath": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\training_diagnostics.py"
    },
    "codeblockId": "5bbacaf4-662c-451e-9c50-129fef9e538e",
    "codeBlockIdx": 0,
    "content": "import os\r\nimport numpy as np\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\n\r\nclass TrainingDiagnostics:\r\n    \"\"\"\r\n    Comprehensive diagnostics for PINN performance analysis.\r\n    \r\n    TWO DIAGNOSTIC MODES:\r\n    \r\n    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t&gt;0)\r\n       Focus: Why does PINN fail at long times?\r\n       Generates 6 diagnostic plots:\r\n         - Training diagnostics: Loss convergence and balance\r\n         - PDE residual evolution: When/where/which equations fail\r\n         - Conservation violations: Mass and momentum drift\r\n         - Temporal error accumulation: How errors compound over time\r\n         - Spectral bias analysis: Frequency damping and mode evolution\r\n         - Solution stability: Detecting unphysical behavior\r\n    \r\n    2. IC FITTING MODE (3D power spectrum only)\r\n       Focus: Why can't PINN fit complex initial conditions?\r\n       Generates 5 diagnostic plots:\r\n         - Training diagnostics: Loss convergence with IC component breakdown\r\n         - IC spatial comparison: Predicted vs true IC fields\r\n         - IC power spectrum: Scale-by-scale fit quality\r\n         - IC component convergence: Which component fails to converge\r\n         - IC metrics summary: Correlation and RMS error statistics\r\n    \r\n    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True\r\n    \"\"\"\r\n\r\n    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):\r\n        self.save_dir = save_dir\r\n        self.dimension = dimension\r\n        self.perturbation_type = str(perturbation_type).lower()\r\n        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')\r\n        \r\n        os.makedirs(save_dir, exist_ok=True)\r\n\r\n        # Storage for tracking metrics over iterations\r\n        self.history = {\r\n            'iteration': [],\r\n            'total_loss': [],\r\n            'pde_loss': [],\r\n            'ic_loss': [],\r\n            'mean_rho': [],\r\n            'max_rho': [],\r\n        }\r\n        \r\n        # Additional tracking for 3D power spectrum IC diagnostics\r\n        if self.is_3d_power_spectrum:\r\n            self.history.update({\r\n                'ic_rho': [],\r\n                'ic_vx': [],\r\n                'ic_vy': [],\r\n                'ic_vz': [],\r\n                'ic_phi': [],\r\n            })\r\n\r\n    def _prepare_inputs(self, geomtime_col):\r\n        \"\"\"Normalize collocation inputs to the model's expected format.\"\"\"\r\n        if isinstance(geomtime_col, (list, tuple)):\r\n            return geomtime_col\r\n        # Single tensor [N, D] -&gt; list of [N,1]\r\n        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]\r\n\r\n    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):\r\n        \"\"\"\r\n        Call this every N iterations during training.\r\n        \r\n        Args:\r\n            iteration: Current iteration number\r\n            model: PINN model\r\n            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses\r\n            geomtime_col: Collocation points\r\n            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum\r\n        \"\"\"\r\n        self.history['iteration'].append(iteration)\r\n        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))\r\n        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))\r\n        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))\r\n\r\n        # Compute diagnostics from current model state\r\n        with torch.no_grad():\r\n            inputs = self._prepare_inputs(geomtime_col)\r\n            pred = model(inputs)\r\n            # Use first channel as density/log-density proxy\r\n            rho = pred[:, 0].detach().cpu().numpy()\r\n\r\n            # Density statistics\r\n            self.history['mean_rho'].append(np.nanmean(rho))\r\n            self.history['max_rho'].append(np.nanmax(rho))\r\n        \r\n        # Track component IC losses for 3D power spectrum\r\n        if self.is_3d_power_spectrum and ic_component_losses is not None:\r\n            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))\r\n            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))\r\n            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))\r\n            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))\r\n            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))\r\n\r\n    def plot_diagnostics(self, iteration=None):\r\n        \"\"\"\r\n        Generate streamlined training diagnostic plot.\r\n        Combined loss components and balance in one figure.\r\n        \"\"\"\r\n        if len(self.history['iteration']) == 0:\r\n            print(\"No diagnostic data to plot.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        suffix = f\"_iter_{iteration}\" if iteration is not None else \"_final\"\r\n\r\n        # Combined training diagnostics plot\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        # Subplot 1: Loss evolution\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')\r\n        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')\r\n        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')\r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 2: Loss balance ratio\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ic_arr = np.array(self.history['ic_loss'])\r\n        pde_arr = np.array(self.history['pde_loss'])\r\n        ratio = pde_arr / (ic_arr + 1e-10)\r\n        ax2.plot(iters, ratio, 'purple', linewidth=2)\r\n        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')\r\n        ax2.set_xlabel('Iteration', fontsize=11)\r\n        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)\r\n        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 3: Density evolution\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')\r\n        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')\r\n        ax3.set_xlabel('Iteration', fontsize=11)\r\n        ax3.set_ylabel('Density', fontsize=11)\r\n        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n        \r\n        print(f\"Training diagnostic plot saved to {self.save_dir}\")\r\n\r\n    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================\r\n    \r\n    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n        \"\"\"\r\n        Compute PDE residuals on a regular grid over space-time.\r\n        Critical for identifying WHERE and WHEN physics breaks down.\r\n        \r\n        Note: For power spectrum cases, uses higher resolution to match IC generation grid.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum to match IC generation\r\n        if n_spatial is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_spatial = 80\r\n        \r\n        print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...\")\r\n        \r\n        if dimension == 2:\r\n            x = torch.linspace(0, 1, n_spatial, device=device)\r\n            y = torch.linspace(0, 1, n_spatial, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # Create meshgrid\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n            \r\n            # Reshape back to grid\r\n            rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n        \r\n        elif dimension == 3:\r\n            # For 3D, use coarser grid to save memory\r\n            n_spatial_3d = max(32, n_spatial // 2)\r\n            x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            z = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # For 3D, we'll slice at z=0.5 to create 2D heatmaps\r\n            z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                z_slice.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n            \r\n            rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n    \r\n    def plot_residual_heatmaps(self, residual_data, slice_idx=None):\r\n        \"\"\"\r\n        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.\r\n        Shows WHEN and WHERE each equation fails.\r\n        \"\"\"\r\n        if slice_idx is None:\r\n            slice_idx = residual_data['rho_residual'].shape[1] // 2\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        residuals = [\r\n            ('Continuity Equation', residual_data['rho_residual']),\r\n            ('Momentum X Equation', residual_data['vx_residual']),\r\n            ('Momentum Y Equation', residual_data['vy_residual']),\r\n            ('Poisson Equation', residual_data['phi_residual'])\r\n        ]\r\n        \r\n        for idx, (name, resid) in enumerate(residuals):\r\n            ax = fig.add_subplot(gs[idx // 2, idx % 2])\r\n            \r\n            # Slice at fixed y\r\n            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow\r\n            \r\n            # Use log scale for better visualization\r\n            resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n            data_min = np.min(resid_log)\r\n            data_max = np.max(resid_log)\r\n            \r\n            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                          extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                 residual_data['t'][0], residual_data['t'][-1]],\r\n                          vmin=data_min, vmax=data_max)\r\n            ax.set_xlabel('x', fontsize=11)\r\n            ax.set_ylabel('Time', fontsize=11)\r\n            ax.set_title(\r\n                f'{name} Residual\\nMin: 10^{data_min:.1f}, Max: 10^{data_max:.1f}',\r\n                fontsize=12,\r\n                fontweight='bold'\r\n            )\r\n            cbar = plt.colorbar(im, ax=ax)\r\n            cbar.set_label('log₁₀|residual|', fontsize=10)\r\n        \r\n        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data[\"y\"][slice_idx]:.2f})', \r\n                    fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):\r\n        \"\"\"\r\n        Check if mass and momentum are conserved over time.\r\n        Critical for physical consistency - should be flat lines!\r\n        \r\n        Note: Uses higher resolution for power spectrum to ensure accurate integration.\r\n        \"\"\"\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum\r\n        if n_grid is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_grid = 100\r\n        \r\n        print(f\"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...\")\r\n        \r\n        conservation_data = {\r\n            'times': [],\r\n            'total_mass': [],\r\n            'total_momentum_x': [],\r\n            'total_momentum_y': []\r\n        }\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # Integrate using trapezoidal rule\r\n                dx = 1.0 / (n_grid - 1)\r\n                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, use coarser grid\r\n                n_grid_3d = max(40, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                z = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                dx = 1.0 / (n_grid_3d - 1)\r\n                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n        \r\n        return conservation_data\r\n    \r\n    def plot_conservation_laws(self, conservation_data):\r\n        \"\"\"Plot conserved quantities over time - should be flat lines!\"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = conservation_data['times']\r\n        \r\n        # Mass conservation\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        mass = np.array(conservation_data['total_mass'])\r\n        mass_initial = mass[0]\r\n        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift\r\n        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)\r\n        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Mass drift (%)', fontsize=11)\r\n        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Momentum X conservation\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        px = np.array(conservation_data['total_momentum_x'])\r\n        px_initial = px[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(px_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum X absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            px_drift = ((px - px_initial) / abs(px_initial)) * 100\r\n            ylabel = 'Momentum X drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax2.plot(times, px_drift, 'g-', linewidth=2.5)\r\n        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel(ylabel, fontsize=11)\r\n        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Momentum Y conservation\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        py = np.array(conservation_data['total_momentum_y'])\r\n        py_initial = py[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(py_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum Y absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            py_drift = ((py - py_initial) / abs(py_initial)) * 100\r\n            ylabel = 'Momentum Y drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax3.plot(times, py_drift, 'orange', linewidth=2.5)\r\n        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel(ylabel, fontsize=11)\r\n        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):\r\n        \"\"\"\r\n        Compute FFT of density field at different times.\r\n        Reveals spectral bias and high-frequency mode damping.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing spectral content at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        spectra = []\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # 2D FFT\r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                # Create wavenumber grid\r\n                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5\r\n                n_grid_3d = max(64, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n        \r\n        return spectra\r\n    \r\n    def plot_spectral_evolution(self, spectra_data):\r\n        \"\"\"\r\n        Plot power spectrum evolution over time.\r\n        Shows if high-frequency modes are being damped (spectral bias).\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        # Left: Radially averaged power spectra\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        \r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], \r\n                          linewidth=2, markersize=4, label=f\"t={spec['time']:.2f}\")\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Right: 2D power spectrum at final time\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        final_spec = spectra_data[-1]\r\n        power_log = np.log10(final_spec['power'] + 1e-12)\r\n        \r\n        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',\r\n                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),\r\n                              -np.max(final_spec['k']), np.max(final_spec['k'])])\r\n        ax2.set_xlabel('k_x', fontsize=11)\r\n        ax2.set_ylabel('k_y', fontsize=11)\r\n        ax2.set_title(f'2D Power Spectrum at t={final_spec[\"time\"]:.2f}', \r\n                     fontsize=12, fontweight='bold')\r\n        cbar = plt.colorbar(im, ax=ax2)\r\n        cbar.set_label('log₁₀(Power)', fontsize=10)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):\r\n        \"\"\"\r\n        Compute field statistics over time to track error accumulation.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing temporal statistics at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        stats = {\r\n            'times': [],\r\n            'rho_mean': [],\r\n            'rho_std': [],\r\n            'rho_max': [],\r\n            'grad_rho_mean': [],\r\n            'grad_rho_max': []\r\n        }\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients (retain_graph=True for first call since we need multiple gradients)\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5 for computational efficiency\r\n                n_spatial_3d = max(50, n_spatial // 2)\r\n                x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                Z_flat = Z.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                Z_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n        \r\n        return stats\r\n    \r\n    def plot_temporal_statistics(self, stats_data):\r\n        \"\"\"\r\n        Plot field statistics over time.\r\n        Shows if solution becomes unphysical or gradients explode.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = stats_data['times']\r\n        \r\n        # Density statistics\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax1.fill_between(times,\r\n                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),\r\n                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),\r\n                         alpha=0.3, color='b', label='±1 std')\r\n        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Gradient statistics\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)\r\n        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Growth rate analysis\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        rho_max = np.array(stats_data['rho_max'])\r\n        if len(rho_max) &gt; 5:\r\n            # Fit exponential growth\r\n            log_rho = np.log(rho_max + 1e-10)\r\n            valid_idx = np.isfinite(log_rho)\r\n            if valid_idx.sum() &gt; 5:\r\n                times_valid = np.array(times)[valid_idx]\r\n                log_rho_valid = log_rho[valid_idx]\r\n                coeffs = np.polyfit(times_valid, log_rho_valid, 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                \r\n                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, \r\n                           label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.set_xlabel('Time', fontsize=11)\r\n                ax3.set_ylabel('Max Density (log scale)', fontsize=11)\r\n                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')\r\n                ax3.legend(fontsize=10)\r\n                ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):\r\n        \"\"\"\r\n        Track how prediction errors grow and accumulate over time.\r\n        Helps identify when the solution starts diverging from physical behavior.\r\n        \r\n        Now computes ACTUAL PDE residuals and gradient norms including phi.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        \r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        error_data = {\r\n            'times': time_points,\r\n            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},\r\n            'field_ranges': {'rho': [], 'vx': [], 'vy': []},\r\n            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},\r\n            'max_residuals': []\r\n        }\r\n        \r\n        if dimension == 3:\r\n            error_data['pde_residuals']['momentum_z'] = []\r\n            error_data['field_ranges']['vz'] = []\r\n            error_data['gradient_norms']['vz'] = []\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n            else:  # dimension == 3\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                Z_flat = Z.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n            \r\n            # Compute ACTUAL PDE residuals using the proper residue function\r\n            if dimension == 2:\r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n                \r\n            else:  # 3D\r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n            \r\n            # Get predictions for field statistics\r\n            pred = model(colloc)\r\n            rho = pred[:, 0:1]\r\n            vx = pred[:, 1:2]\r\n            vy = pred[:, 2:3]\r\n            phi = pred[:, -1:]  # Last column is phi\r\n            \r\n            # Track field statistics\r\n            error_data['field_ranges']['rho'].append(torch.std(rho).item())\r\n            error_data['field_ranges']['vx'].append(torch.std(vx).item())\r\n            error_data['field_ranges']['vy'].append(torch.std(vy).item())\r\n            if dimension == 3:\r\n                vz = pred[:, 3:4]\r\n                error_data['field_ranges']['vz'].append(torch.std(vz).item())\r\n            \r\n            # Track gradient norms for ALL fields including phi\r\n            with torch.enable_grad():\r\n                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                \r\n                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())\r\n                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())\r\n                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())\r\n                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())\r\n                \r\n                if dimension == 3:\r\n                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())\r\n            \r\n            # Max residual magnitude (now including Poisson!)\r\n            max_res = max(\r\n                error_data['pde_residuals']['continuity'][-1],\r\n                error_data['pde_residuals']['momentum_x'][-1],\r\n                error_data['pde_residuals']['momentum_y'][-1],\r\n                error_data['pde_residuals']['poisson'][-1]\r\n            )\r\n            if dimension == 3:\r\n                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])\r\n            \r\n            error_data['max_residuals'].append(max_res)\r\n        \r\n        return error_data\r\n    \r\n    def plot_temporal_error_accumulation(self, error_data):\r\n        \"\"\"\r\n        Visualize how errors accumulate and grow over time.\r\n        Shows which equations fail first and error growth rates.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = error_data['times']\r\n        \r\n        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)\r\n        # IMPORTANT: Now showing actual Poisson residuals!\r\n        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)\r\n        if 'momentum_z' in error_data['pde_residuals']:\r\n            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)\r\n        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Field Range Evolution (variance growth)\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)\r\n        if 'vz' in error_data['field_ranges']:\r\n            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Field Standard Deviation', fontsize=11)\r\n        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=9)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: Maximum Residual Growth\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)\r\n        if len(times) &gt; 5:\r\n            # Fit exponential growth\r\n            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)\r\n            valid_idx = np.isfinite(log_res)\r\n            if valid_idx.sum() &gt; 3:\r\n                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.legend(fontsize=10)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Max Residual (log)', fontsize=11)\r\n        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)\r\n        if 'vz' in error_data['gradient_norms']:\r\n            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)\r\n        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax4.legend(fontsize=9, loc='best')\r\n        ax4.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 5: Cumulative Error Metric\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        cumulative_error = np.cumsum(error_data['max_residuals'])\r\n        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Cumulative Error', fontsize=11)\r\n        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3)\r\n        \r\n        # Plot 6: Error Growth Rate\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        if len(times) &gt; 1:\r\n            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)\r\n            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n            ax6.set_xlabel('Time', fontsize=11)\r\n            ax6.set_ylabel('Error Growth Rate', fontsize=11)\r\n            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')\r\n            ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_spectral_bias_analysis(self, spectra_data):\r\n        \"\"\"\r\n        Enhanced spectral analysis focused on detecting spectral bias.\r\n        Shows frequency damping and mode evolution over time.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: Radially averaged power spectra at different times\r\n        ax1 = fig.add_subplot(gs[0, :2])\r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.sum() &gt; 0:\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,\r\n                          label=f't={spec[\"time\"]:.2f}', marker='o', markersize=3)\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power P(k)', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=8, ncol=2)\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Spectral Damping Rate\r\n        ax2 = fig.add_subplot(gs[0, 2])\r\n        if len(spectra_data) &gt;= 2:\r\n            # Compare first and last spectra\r\n            spec_0 = spectra_data[0]\r\n            spec_f = spectra_data[-1]\r\n            \r\n            # Compute power ratio (damping)\r\n            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))\r\n            k_bins = np.linspace(0, k_max, 20)\r\n            damping_ratio = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask_0 = (spec_0['k'] &gt;= k_bins[i]) &amp; (spec_0['k'] &lt; k_bins[i+1])\r\n                mask_f = (spec_f['k'] &gt;= k_bins[i]) &amp; (spec_f['k'] &lt; k_bins[i+1])\r\n                if mask_0.sum() &gt; 0 and mask_f.sum() &gt; 0:\r\n                    p0 = np.mean(spec_0['power'][mask_0])\r\n                    pf = np.mean(spec_f['power'][mask_f])\r\n                    if p0 &gt; 0:\r\n                        damping_ratio.append(pf / p0)\r\n                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)\r\n                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')\r\n                ax2.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)\r\n                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')\r\n                ax2.legend(fontsize=10)\r\n                ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: High-k Mode Tracking\r\n        ax3 = fig.add_subplot(gs[1, 0])\r\n        times = [spec['time'] for spec in spectra_data]\r\n        \r\n        # Track power in different k ranges\r\n        low_k_power = []\r\n        mid_k_power = []\r\n        high_k_power = []\r\n        \r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            k_max = np.max(k_flat)\r\n            low_k_power.append(np.mean(p_flat[k_flat &lt; k_max/3]))\r\n            mid_k_power.append(np.mean(p_flat[(k_flat &gt;= k_max/3) &amp; (k_flat &lt; 2*k_max/3)]))\r\n            high_k_power.append(np.mean(p_flat[k_flat &gt;= 2*k_max/3]))\r\n        \r\n        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)\r\n        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)\r\n        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Average Power (log)', fontsize=11)\r\n        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Spectral Index Evolution\r\n        ax4 = fig.add_subplot(gs[1, 1])\r\n        spectral_indices = []\r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            # Fit power law in log space\r\n            valid = (k_flat &gt; 0) &amp; (p_flat &gt; 0)\r\n            if valid.sum() &gt; 10:\r\n                log_k = np.log(k_flat[valid])\r\n                log_p = np.log(p_flat[valid])\r\n                coeffs = np.polyfit(log_k, log_p, 1)\r\n                spectral_indices.append(coeffs[0])  # Slope = spectral index\r\n            else:\r\n                spectral_indices.append(np.nan)\r\n        \r\n        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)\r\n        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)\r\n        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Density field snapshots\r\n        ax5 = fig.add_subplot(gs[1, 2])\r\n        # Show first, middle, last density fields\r\n        n_show = min(3, len(spectra_data))\r\n        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]\r\n        \r\n        for i, idx in enumerate(indices[:n_show]):\r\n            rho = spectra_data[idx]['rho']\r\n            # Compute structure function or variance\r\n            rho_var = np.var(rho)\r\n            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)\r\n            ax5.text(i, rho_var, f't={spectra_data[idx][\"time\"]:.2f}', \r\n                    ha='center', va='bottom', fontsize=9)\r\n        \r\n        ax5.set_xlabel('Time Snapshot', fontsize=11)\r\n        ax5.set_ylabel('Density Variance', fontsize=11)\r\n        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')\r\n        ax5.set_xticks(range(n_show))\r\n        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])\r\n        ax5.grid(True, alpha=0.3, axis='y')\r\n        \r\n        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):\r\n        \"\"\"\r\n        Check for signs of solution instability or unphysical behavior.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_check)\r\n        \r\n        stability_data = {\r\n            'times': time_points,\r\n            'density_bounds': {'min': [], 'max': [], 'mean': []},\r\n            'velocity_norms': [],\r\n            'negative_density_fraction': [],\r\n            'extreme_values': [],\r\n            'solution_smoothness': []\r\n        }\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n            else:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n            \r\n            with torch.no_grad():\r\n                pred = model(colloc)\r\n                rho = pred[:, 0].cpu().numpy()\r\n                vx = pred[:, 1].cpu().numpy()\r\n                vy = pred[:, 2].cpu().numpy()\r\n                \r\n                # Density bounds\r\n                stability_data['density_bounds']['min'].append(np.min(rho))\r\n                stability_data['density_bounds']['max'].append(np.max(rho))\r\n                stability_data['density_bounds']['mean'].append(np.mean(rho))\r\n                \r\n                # Velocity norms\r\n                v_norm = np.sqrt(vx**2 + vy**2)\r\n                if dimension == 3:\r\n                    vz = pred[:, 3].cpu().numpy()\r\n                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)\r\n                stability_data['velocity_norms'].append(np.max(v_norm))\r\n                \r\n                # Negative density fraction\r\n                neg_frac = np.sum(rho &lt; 0) / len(rho)\r\n                stability_data['negative_density_fraction'].append(neg_frac)\r\n                \r\n                # Extreme value detection (more than 10x mean)\r\n                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) &gt; 10 * np.std(rho)) / len(rho)\r\n                stability_data['extreme_values'].append(extreme_frac)\r\n                \r\n                # Solution smoothness (Laplacian magnitude as proxy)\r\n                if dimension == 2:\r\n                    rho_2d = rho.reshape(n_spatial, n_spatial)\r\n                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + \r\n                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))\r\n                    stability_data['solution_smoothness'].append(np.mean(laplacian))\r\n                else:\r\n                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D\r\n        \r\n        return stability_data\r\n    \r\n    def plot_solution_stability(self, stability_data):\r\n        \"\"\"\r\n        Visualize solution stability metrics to detect unphysical behavior.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = stability_data['times']\r\n        \r\n        # Plot 1: Density Bounds\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)\r\n        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Plot 2: Velocity Norms\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)\r\n        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 3: Negative Density Fraction\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)\r\n        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Negative Density (%)', fontsize=11)\r\n        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        # Plot 4: Extreme Values\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)\r\n        ax4.set_title('Outlier Detection (&gt;10σ)', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Solution Smoothness\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)\r\n        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 6: Stability Summary\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        # Compute overall stability score (0-1, higher is worse)\r\n        neg_dens_score = np.array(stability_data['negative_density_fraction'])\r\n        extreme_score = np.array(stability_data['extreme_values'])\r\n        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score\r\n        \r\n        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)\r\n        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')\r\n        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')\r\n        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')\r\n        ax6.set_xlabel('Time', fontsize=11)\r\n        ax6.set_ylabel('Instability Score (%)', fontsize=11)\r\n        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')\r\n        ax6.legend(fontsize=9)\r\n        ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_comprehensive_diagnostics(self, model, dimension, tmax):\r\n        \"\"\"\r\n        Run all post-training diagnostics for long-term evolution analysis.\r\n        \r\n        Generates 6 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail\r\n        3. Conservation law violations - Mass/momentum drift over time\r\n        4. Temporal error accumulation - How errors compound over time\r\n        5. Spectral bias analysis - Frequency damping and mode evolution\r\n        6. Solution stability - Detecting unphysical behavior\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            dimension: Spatial dimension (2 or 3)\r\n            tmax: Maximum time to analyze\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Long-Term Evolution Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: PDE Residual Evolution\r\n        print(\"\\n[1/5] Computing PDE residual evolution...\")\r\n        residuals = self.compute_residual_heatmap(model, dimension, tmax)\r\n        self.plot_residual_heatmaps(residuals)\r\n        print(\"      [OK] PDE residual evolution saved\")\r\n        \r\n        # Plot 3: Conservation Laws\r\n        print(\"\\n[2/5] Analyzing conservation law violations...\")\r\n        conservation = self.check_conservation_laws(model, dimension, tmax)\r\n        self.plot_conservation_laws(conservation)\r\n        print(\"      [OK] Conservation violations plot saved\")\r\n        \r\n        # Plot 4: Temporal Error Accumulation\r\n        print(\"\\n[3/5] Computing temporal error accumulation...\")\r\n        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)\r\n        self.plot_temporal_error_accumulation(error_data)\r\n        print(\"      [OK] Error accumulation plot saved\")\r\n        \r\n        # Plot 5: Spectral Bias Analysis\r\n        print(\"\\n[4/5] Analyzing spectral bias and mode damping...\")\r\n        spectra = self.compute_spectral_content(model, dimension, tmax)\r\n        self.plot_spectral_bias_analysis(spectra)\r\n        print(\"      [OK] Spectral bias analysis saved\")\r\n        \r\n        # Plot 6: Solution Stability\r\n        print(\"\\n[5/5] Checking solution stability...\")\r\n        stats = self.compute_temporal_statistics(model, dimension, tmax)\r\n        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)\r\n        self.plot_solution_stability(stability_data)\r\n        print(\"      [OK] Solution stability diagnostics saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nLong-Term Evolution Diagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")\r\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns\")\r\n        print(\"  3. conservation_laws.png - Conservation law drift over time\")\r\n        print(\"  4. temporal_error_accumulation.png - Error growth and compounding\")\r\n        print(\"  5. spectral_bias_analysis.png - Frequency damping and spectral issues\")\r\n        print(\"  6. solution_stability.png - Physical validity and stability metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n    \r\n    \r\n    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):\r\n        \"\"\"\r\n        Smart dispatcher: runs appropriate diagnostics based on case type.\r\n        \r\n        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics\r\n        - 3D power spectrum: IC fitting diagnostics  \r\n        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)\r\n            true_ic_data: Dict with IC data (required for 3D power spectrum)\r\n                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        if self.is_3d_power_spectrum:\r\n            # 3D power spectrum: Focus on IC fitting\r\n            if true_ic_data is None:\r\n                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")\r\n            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)\r\n        else:\r\n            # All other cases: Focus on temporal evolution\r\n            if tmax is None:\r\n                raise ValueError(\"tmax required for temporal evolution diagnostics\")\r\n            self.run_comprehensive_diagnostics(model, self.dimension, tmax)\r\n    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================\r\n    \r\n    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):\r\n        \"\"\"\r\n        Compare predicted vs true ICs spatially for both 2D and 3D cases.\r\n        Critical for diagnosing WHERE the network fails to fit the IC structure.\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC collocation points and true values\r\n                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}\r\n                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            n_grid: Grid resolution per dimension (for visualization grid)\r\n        \r\n        Returns:\r\n            Dict with predicted and true fields, plus errors\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing IC spatial comparison on training IC points...\")\r\n        \r\n        # Use the actual IC collocation points from training\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions at IC points\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n                phi_pred = pred[:, 4].cpu().numpy()\r\n            else:\r\n                phi_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # For visualization, create a regular grid\r\n        x = torch.linspace(0, 1, n_grid, device=device)\r\n        y = torch.linspace(0, 1, n_grid, device=device)\r\n        X, Y = torch.meshgrid(x, y, indexing='ij')\r\n        T = torch.zeros_like(X)\r\n        \r\n        if self.dimension == 3:\r\n            # For 3D, take z=0.5 slice\r\n            Z = torch.full_like(X, 0.5)\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n        else:\r\n            # For 2D\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n        \r\n        # Get predictions on visualization grid\r\n        with torch.no_grad():\r\n            pred_viz = model(colloc_viz)\r\n            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()\r\n            else:\r\n                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n        \r\n        # For \"true\" visualization, we need to interpolate from IC points to regular grid\r\n        # Extract spatial coordinates from IC collocation points (detach first)\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Find points near z=0.5 for visualization\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        if mask_z.sum() &gt; 100:  # Need enough points for interpolation\r\n            from scipy.interpolate import griddata\r\n            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n            X_viz = X.cpu().numpy()\r\n            Y_viz = Y.cpu().numpy()\r\n            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])\r\n            \r\n            # Interpolate with nearest neighbor fallback for NaN values\r\n            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            \r\n            # For uniform density (all values the same), fill NaNs with the constant value\r\n            # For varying fields, use nearest neighbor interpolation for NaN regions\r\n            if np.isnan(rho_true_viz).any():\r\n                if np.std(rho_true[mask_z]) &lt; 1e-10:\r\n                    # Uniform field - fill with mean\r\n                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))\r\n                else:\r\n                    # Varying field - use nearest neighbor for gaps\r\n                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)\r\n            \r\n            if np.isnan(vx_true_viz).any():\r\n                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)\r\n            \r\n            if np.isnan(vy_true_viz).any():\r\n                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)\r\n            \r\n            if self.dimension == 3:\r\n                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n                if np.isnan(vz_true_viz).any():\r\n                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)\r\n        else:\r\n            # Not enough points for interpolation, use predicted as reference\r\n            print(\"  [WARN] Not enough IC points for ground truth visualization\")\r\n            rho_true_viz = rho_pred_viz\r\n            vx_true_viz = vx_pred_viz\r\n            vy_true_viz = vy_pred_viz\r\n            if self.dimension == 3:\r\n                vz_true_viz = vz_pred_viz\r\n        \r\n        result = {\r\n            'x': x.cpu().numpy(),\r\n            'y': y.cpu().numpy(),\r\n            'rho_pred': rho_pred_viz,\r\n            'rho_true': rho_true_viz,\r\n            'vx_pred': vx_pred_viz,\r\n            'vx_true': vx_true_viz,\r\n            'vy_pred': vy_pred_viz,\r\n            'vy_true': vy_true_viz,\r\n            'phi_pred': phi_pred_viz,\r\n            # Also store scatter point data for metrics\r\n            'rho_pred_scatter': rho_pred,\r\n            'rho_true_scatter': rho_true,\r\n            'vx_pred_scatter': vx_pred,\r\n            'vx_true_scatter': vx_true,\r\n            'vy_pred_scatter': vy_pred,\r\n            'vy_true_scatter': vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            result.update({\r\n                'vz_pred': vz_pred_viz,\r\n                'vz_true': vz_true_viz,\r\n                'vz_pred_scatter': vz_pred,\r\n                'vz_true_scatter': vz_true,\r\n            })\r\n        \r\n        return result\r\n    \r\n    def plot_ic_power_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot power spectrum comparison for all velocity components.\r\n        Shows which scales (k-modes) are missing or damped.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_ic_component_convergence(self):\r\n        \"\"\"\r\n        Plot IC component loss convergence over training.\r\n        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.\r\n        \"\"\"\r\n        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:\r\n            print(\"IC component tracking not available.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        \r\n        # Replace zeros and very small values with a minimum threshold for log plotting\r\n        def safe_log_data(data):\r\n            \"\"\"Replace zeros/negative values with small epsilon for log plotting.\"\"\"\r\n            data_array = np.array(data)\r\n            data_array = np.where(data_array &lt;= 0, 1e-12, data_array)\r\n            return data_array\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: All IC components on same plot\r\n        ax1 = fig.add_subplot(gs[0, :])\r\n        \r\n        # Check if rho is zero/constant (uniform field case)\r\n        rho_is_uniform = np.max(self.history['ic_rho']) &lt; 1e-10\r\n        \r\n        if not rho_is_uniform:\r\n            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', \r\n                        marker='o', markersize=3, markevery=max(1, len(iters)//20))\r\n        else:\r\n            # Plot a dashed line at the bottom to indicate uniform/zero loss\r\n            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)\r\n        \r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', \r\n                    marker='s', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', \r\n                    marker='^', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', \r\n                    marker='d', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', \r\n                    marker='v', markersize=3, markevery=max(1, len(iters)//20))\r\n        \r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10, ncol=5)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Individual component plots\r\n        components = [\r\n            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),\r\n            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),\r\n            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),\r\n        ]\r\n        \r\n        for hist_key, title, grid_pos, is_uniform in components:\r\n            ax = fig.add_subplot(grid_pos)\r\n            if is_uniform:\r\n                # Show a flat line at bottom with annotation\r\n                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)\r\n                ax.text(0.5, 0.5, 'Uniform IC\\n(no spatial variation)', \r\n                       transform=ax.transAxes, ha='center', va='center',\r\n                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\r\n                ax.set_ylim([1e-13, 1e-10])\r\n            else:\r\n                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')\r\n            \r\n            ax.set_xlabel('Iteration', fontsize=10)\r\n            ax.set_ylabel('Loss (log scale)', fontsize=10)\r\n            ax.set_title(title, fontsize=11, fontweight='bold')\r\n            ax.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_ic_correlation_metrics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Compute spatial correlation between predicted and true ICs.\r\n        High correlation = good spatial structure capture.\r\n        \"\"\"\r\n        print(f\"  Computing IC correlation metrics on training IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy().flatten()\r\n            vx_pred = pred[:, 1].cpu().numpy().flatten()\r\n            vy_pred = pred[:, 2].cpu().numpy().flatten()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy().flatten()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()\r\n        \r\n        # Compute correlations - handle uniform fields (zero variance)\r\n        def safe_corrcoef(pred, true):\r\n            \"\"\"Compute correlation, handling constant fields gracefully.\"\"\"\r\n            # Check if either field is constant\r\n            if np.std(pred) &lt; 1e-10 or np.std(true) &lt; 1e-10:\r\n                # For uniform fields, correlation is undefined\r\n                # If both are uniform and equal, perfect match (1.0)\r\n                # If different constants, poor match (use normalized error)\r\n                if np.std(pred) &lt; 1e-10 and np.std(true) &lt; 1e-10:\r\n                    # Both uniform - check if they match\r\n                    mean_diff = abs(np.mean(pred) - np.mean(true))\r\n                    return 1.0 if mean_diff &lt; 1e-6 else 0.0\r\n                else:\r\n                    # One uniform, one not - cannot capture structure\r\n                    return 0.0\r\n            else:\r\n                return np.corrcoef(pred, true)[0, 1]\r\n        \r\n        correlations = {\r\n            'rho': safe_corrcoef(rho_pred, rho_true),\r\n            'vx': safe_corrcoef(vx_pred, vx_true),\r\n            'vy': safe_corrcoef(vy_pred, vy_true),\r\n        }\r\n        \r\n        # Compute RMS errors\r\n        rms_errors = {\r\n            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),\r\n            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),\r\n            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)\r\n            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))\r\n        \r\n        return correlations, rms_errors\r\n    \r\n    def plot_ic_metrics_summary(self, correlations, rms_errors):\r\n        \"\"\"\r\n        Summary plot: correlation and RMS error for each IC component.\r\n        Quick visual diagnostic of IC fitting quality.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        components = ['rho', 'vx', 'vy', 'vz']\r\n        labels = ['ρ', 'vx', 'vy', 'vz']\r\n        \r\n        # Correlation plot\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        corr_vals = [correlations[c] for c in components]\r\n        \r\n        # Handle NaN/inf correlations from uniform fields\r\n        corr_vals_safe = []\r\n        for val in corr_vals:\r\n            if np.isnan(val) or np.isinf(val):\r\n                corr_vals_safe.append(0.0)  # Show as 0 for visualization\r\n            else:\r\n                corr_vals_safe.append(val)\r\n        \r\n        colors = ['green' if c &gt; 0.9 else 'orange' if c &gt; 0.7 else 'red' for c in corr_vals_safe]\r\n        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')\r\n        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')\r\n        ax1.set_ylabel('Correlation Coefficient', fontsize=11)\r\n        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax1.set_ylim([0, 1.05])\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):\r\n            height = bar.get_height()\r\n            if np.isnan(orig_val) or np.isinf(orig_val):\r\n                label_text = 'N/A\\n(uniform)'\r\n                fontsize = 8\r\n            else:\r\n                label_text = f'{val:.3f}'\r\n                fontsize = 10\r\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\r\n                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')\r\n        \r\n        # RMS Error plot\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        rms_vals = [rms_errors[c] for c in components]\r\n        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax2.set_ylabel('RMS Error', fontsize=11)\r\n        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val in zip(bars2, rms_vals):\r\n            height = bar.get_height()\r\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\r\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\r\n        \r\n        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Run diagnostics for 3D power spectrum case.\r\n        Focus on IC fitting quality, not temporal evolution.\r\n        \r\n        Generates 5 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum comparison (scale-by-scale)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running 3D Power Spectrum IC Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[1/4] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power Spectrum Comparison\r\n        print(\"\\n[2/4] Computing IC power spectrum comparison...\")\r\n        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n        self.plot_ic_power_spectrum_comparison(ps_data)\r\n        print(\"      [OK] IC power spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[3/4] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[4/4] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit\")\r\n        print(\"  4. ic_component_convergence.png - Which IC component fails\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        for comp in ['rho', 'vx', 'vy', 'vz']:\r\n            print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================\r\n    \r\n    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):\r\n        \"\"\"\r\n        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).\r\n        \r\n        The 5 plots are:\r\n        1. Training diagnostics (loss convergence and balance)\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            final_iteration: Final iteration number\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Unified Training Diagnostics (5 plots)\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 1: Training diagnostics\r\n        print(\"\\n[1/5] Plotting training diagnostics...\")\r\n        self.plot_diagnostics(final_iteration)\r\n        print(\"      [OK] Training diagnostics saved\")\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[2/5] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power/Field Spectrum Comparison\r\n        print(\"\\n[3/5] Computing spectrum comparison...\")\r\n        if self.perturbation_type == 'power_spectrum':\r\n            # For power spectrum: compare power spectra\r\n            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n            self.plot_ic_power_spectrum_comparison(ps_data)\r\n        else:\r\n            # For sinusoidal: compute field spectra from predictions\r\n            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)\r\n            self.plot_field_spectrum_comparison(ps_data)\r\n        print(\"      [OK] Spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[4/5] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[5/5] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Spectrum comparison\")\r\n        print(\"  4. ic_component_convergence.png - IC component convergence\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']\r\n        for comp in comps:\r\n            if comp in correlations:\r\n                print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):\r\n        \"\"\"\r\n        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.\r\n        Similar to power spectrum comparison but computed from actual fields.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing field spectrum comparison on IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # Need to interpolate to regular grid for FFT\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Use points near z=0.5 for 2D slice\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n            if mask_z.sum() &lt; 100:\r\n                mask_z = np.ones(len(z_ic), dtype=bool)\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        # Create regular grid for FFT\r\n        x_grid = np.linspace(0, 1, n_grid)\r\n        y_grid = np.linspace(0, 1, n_grid)\r\n        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')\r\n        \r\n        # Interpolate from scattered IC points to regular grid\r\n        from scipy.interpolate import griddata\r\n        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])\r\n        \r\n        # Interpolate density\r\n        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Interpolate velocities\r\n        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        if self.dimension == 3:\r\n            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Compute power spectra\r\n        def compute_power_spectrum(field):\r\n            fft = np.fft.fft2(field)\r\n            power = np.abs(np.fft.fftshift(fft))**2\r\n            \r\n            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            kx_shift = np.fft.fftshift(kx)\r\n            ky_shift = np.fft.fftshift(ky)\r\n            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n            K = np.sqrt(KX**2 + KY**2)\r\n            \r\n            return power, K\r\n        \r\n        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)\r\n        power_rho_true, _ = compute_power_spectrum(rho_true_grid)\r\n        \r\n        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)\r\n        power_vx_true, _ = compute_power_spectrum(vx_true_grid)\r\n        \r\n        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)\r\n        power_vy_true, _ = compute_power_spectrum(vy_true_grid)\r\n        \r\n        result = {\r\n            'K': K,\r\n            'power_rho_pred': power_rho_pred,\r\n            'power_rho_true': power_rho_true,\r\n            'power_vx_pred': power_vx_pred,\r\n            'power_vx_true': power_vx_true,\r\n            'power_vy_pred': power_vy_pred,\r\n            'power_vy_true': power_vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)\r\n            power_vz_true, _ = compute_power_spectrum(vz_true_grid)\r\n            result['power_vz_pred'] = power_vz_pred\r\n            result['power_vz_true'] = power_vz_true\r\n        \r\n        return result\r\n    \r\n    def plot_field_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot field spectrum comparison for sinusoidal cases.\r\n        Shows density and velocity spectra.\r\n        \"\"\"\r\n        if self.dimension == 3:\r\n            fig = plt.figure(figsize=(18, 5))\r\n            gs = GridSpec(1, 4, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        else:\r\n            fig = plt.figure(figsize=(14, 5))\r\n            gs = GridSpec(1, 3, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "              # Create meshgrid\r\n              X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n-             colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n-             \r\n-             with torch.no_grad():\r\n-                 rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n+             colloc = [\r\n+                 X.reshape(-1, 1).requires_grad_(True),\r\n+                 Y.reshape(-1, 1).requires_grad_(True),\r\n+                 T.reshape(-1, 1).requires_grad_(True)\r\n+             ]\r\n+             \r\n+             rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n              \r\n              # Reshape back to grid\r",
        "oldStart": 190,
        "newStart": 190,
        "oldLines": 8,
        "newLines": 11,
        "linesRemoved": 4,
        "linesAdded": 7
      },
      {
        "diffString": "              z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)\r\n              X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n-             colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), z_slice.reshape(-1, 1), T.reshape(-1, 1)]\r\n-             \r\n-             with torch.no_grad():\r\n-                 rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n+             colloc = [\r\n+                 X.reshape(-1, 1).requires_grad_(True),\r\n+                 Y.reshape(-1, 1).requires_grad_(True),\r\n+                 z_slice.reshape(-1, 1).requires_grad_(True),\r\n+                 T.reshape(-1, 1).requires_grad_(True)\r\n+             ]\r\n+             \r\n+             rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n              \r\n              rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r",
        "oldStart": 222,
        "newStart": 225,
        "oldLines": 8,
        "newLines": 12,
        "linesRemoved": 4,
        "linesAdded": 8
      },
      {
        "diffString": "              \r\n              # Use log scale for better visualization\r\n-             resid_log = np.log10(np.abs(resid_slice) + 1e-12)\r\n+             resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n+             data_min = np.min(resid_log)\r\n+             data_max = np.max(resid_log)\r\n              \r\n              im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                            extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                   residual_data['t'][0], residual_data['t'][-1]],\r\n-                           vmin=-10, vmax=0)\r\n+                           vmin=data_min, vmax=data_max)\r\n              ax.set_xlabel('x', fontsize=11)\r\n              ax.set_ylabel('Time', fontsize=11)\r\n-             ax.set_title(f'{name} Residual (log₁₀)', fontsize=12, fontweight='bold')\r\n+             ax.set_title(\r\n+                 f'{name} Residual\\nMin: 10^{data_min:.1f}, Max: 10^{data_max:.1f}',\r\n+                 fontsize=12,\r\n+                 fontweight='bold'\r\n+             )\r\n              cbar = plt.colorbar(im, ax=ax)\r\n              cbar.set_label('log₁₀|residual|', fontsize=10)\r",
        "oldStart": 267,
        "newStart": 274,
        "oldLines": 13,
        "newLines": 19,
        "linesRemoved": 3,
        "linesAdded": 9
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM training/training_diagnostics.py",
  "contentsAfterEdit": "import os\r\nimport numpy as np\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\n\r\nclass TrainingDiagnostics:\r\n    \"\"\"\r\n    Comprehensive diagnostics for PINN performance analysis.\r\n    \r\n    TWO DIAGNOSTIC MODES:\r\n    \r\n    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t&gt;0)\r\n       Focus: Why does PINN fail at long times?\r\n       Generates 6 diagnostic plots:\r\n         - Training diagnostics: Loss convergence and balance\r\n         - PDE residual evolution: When/where/which equations fail\r\n         - Conservation violations: Mass and momentum drift\r\n         - Temporal error accumulation: How errors compound over time\r\n         - Spectral bias analysis: Frequency damping and mode evolution\r\n         - Solution stability: Detecting unphysical behavior\r\n    \r\n    2. IC FITTING MODE (3D power spectrum only)\r\n       Focus: Why can't PINN fit complex initial conditions?\r\n       Generates 5 diagnostic plots:\r\n         - Training diagnostics: Loss convergence with IC component breakdown\r\n         - IC spatial comparison: Predicted vs true IC fields\r\n         - IC power spectrum: Scale-by-scale fit quality\r\n         - IC component convergence: Which component fails to converge\r\n         - IC metrics summary: Correlation and RMS error statistics\r\n    \r\n    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True\r\n    \"\"\"\r\n\r\n    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):\r\n        self.save_dir = save_dir\r\n        self.dimension = dimension\r\n        self.perturbation_type = str(perturbation_type).lower()\r\n        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')\r\n        \r\n        os.makedirs(save_dir, exist_ok=True)\r\n\r\n        # Storage for tracking metrics over iterations\r\n        self.history = {\r\n            'iteration': [],\r\n            'total_loss': [],\r\n            'pde_loss': [],\r\n            'ic_loss': [],\r\n            'mean_rho': [],\r\n            'max_rho': [],\r\n        }\r\n        \r\n        # Additional tracking for 3D power spectrum IC diagnostics\r\n        if self.is_3d_power_spectrum:\r\n            self.history.update({\r\n                'ic_rho': [],\r\n                'ic_vx': [],\r\n                'ic_vy': [],\r\n                'ic_vz': [],\r\n                'ic_phi': [],\r\n            })\r\n\r\n    def _prepare_inputs(self, geomtime_col):\r\n        \"\"\"Normalize collocation inputs to the model's expected format.\"\"\"\r\n        if isinstance(geomtime_col, (list, tuple)):\r\n            return geomtime_col\r\n        # Single tensor [N, D] -&gt; list of [N,1]\r\n        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]\r\n\r\n    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):\r\n        \"\"\"\r\n        Call this every N iterations during training.\r\n        \r\n        Args:\r\n            iteration: Current iteration number\r\n            model: PINN model\r\n            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses\r\n            geomtime_col: Collocation points\r\n            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum\r\n        \"\"\"\r\n        self.history['iteration'].append(iteration)\r\n        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))\r\n        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))\r\n        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))\r\n\r\n        # Compute diagnostics from current model state\r\n        with torch.no_grad():\r\n            inputs = self._prepare_inputs(geomtime_col)\r\n            pred = model(inputs)\r\n            # Use first channel as density/log-density proxy\r\n            rho = pred[:, 0].detach().cpu().numpy()\r\n\r\n            # Density statistics\r\n            self.history['mean_rho'].append(np.nanmean(rho))\r\n            self.history['max_rho'].append(np.nanmax(rho))\r\n        \r\n        # Track component IC losses for 3D power spectrum\r\n        if self.is_3d_power_spectrum and ic_component_losses is not None:\r\n            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))\r\n            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))\r\n            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))\r\n            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))\r\n            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))\r\n\r\n    def plot_diagnostics(self, iteration=None):\r\n        \"\"\"\r\n        Generate streamlined training diagnostic plot.\r\n        Combined loss components and balance in one figure.\r\n        \"\"\"\r\n        if len(self.history['iteration']) == 0:\r\n            print(\"No diagnostic data to plot.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        suffix = f\"_iter_{iteration}\" if iteration is not None else \"_final\"\r\n\r\n        # Combined training diagnostics plot\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        # Subplot 1: Loss evolution\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')\r\n        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')\r\n        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')\r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 2: Loss balance ratio\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ic_arr = np.array(self.history['ic_loss'])\r\n        pde_arr = np.array(self.history['pde_loss'])\r\n        ratio = pde_arr / (ic_arr + 1e-10)\r\n        ax2.plot(iters, ratio, 'purple', linewidth=2)\r\n        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')\r\n        ax2.set_xlabel('Iteration', fontsize=11)\r\n        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)\r\n        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 3: Density evolution\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')\r\n        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')\r\n        ax3.set_xlabel('Iteration', fontsize=11)\r\n        ax3.set_ylabel('Density', fontsize=11)\r\n        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n        \r\n        print(f\"Training diagnostic plot saved to {self.save_dir}\")\r\n\r\n    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================\r\n    \r\n    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n        \"\"\"\r\n        Compute PDE residuals on a regular grid over space-time.\r\n        Critical for identifying WHERE and WHEN physics breaks down.\r\n        \r\n        Note: For power spectrum cases, uses higher resolution to match IC generation grid.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum to match IC generation\r\n        if n_spatial is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_spatial = 80\r\n        \r\n        print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...\")\r\n        \r\n        if dimension == 2:\r\n            x = torch.linspace(0, 1, n_spatial, device=device)\r\n            y = torch.linspace(0, 1, n_spatial, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # Create meshgrid\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n            \r\n            # Reshape back to grid\r\n            rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n        \r\n        elif dimension == 3:\r\n            # For 3D, use coarser grid to save memory\r\n            n_spatial_3d = max(32, n_spatial // 2)\r\n            x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            z = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # For 3D, we'll slice at z=0.5 to create 2D heatmaps\r\n            z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                z_slice.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n            \r\n            rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n    \r\n    def plot_residual_heatmaps(self, residual_data, slice_idx=None):\r\n        \"\"\"\r\n        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.\r\n        Shows WHEN and WHERE each equation fails.\r\n        \"\"\"\r\n        if slice_idx is None:\r\n            slice_idx = residual_data['rho_residual'].shape[1] // 2\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        residuals = [\r\n            ('Continuity Equation', residual_data['rho_residual']),\r\n            ('Momentum X Equation', residual_data['vx_residual']),\r\n            ('Momentum Y Equation', residual_data['vy_residual']),\r\n            ('Poisson Equation', residual_data['phi_residual'])\r\n        ]\r\n        \r\n        for idx, (name, resid) in enumerate(residuals):\r\n            ax = fig.add_subplot(gs[idx // 2, idx % 2])\r\n            \r\n            # Slice at fixed y\r\n            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow\r\n            \r\n            # Use log scale for better visualization\r\n            resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n            data_min = np.min(resid_log)\r\n            data_max = np.max(resid_log)\r\n            \r\n            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                          extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                 residual_data['t'][0], residual_data['t'][-1]],\r\n                          vmin=data_min, vmax=data_max)\r\n            ax.set_xlabel('x', fontsize=11)\r\n            ax.set_ylabel('Time', fontsize=11)\r\n            ax.set_title(\r\n                f'{name} Residual\\nMin: 10^{data_min:.1f}, Max: 10^{data_max:.1f}',\r\n                fontsize=12,\r\n                fontweight='bold'\r\n            )\r\n            cbar = plt.colorbar(im, ax=ax)\r\n            cbar.set_label('log₁₀|residual|', fontsize=10)\r\n        \r\n        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data[\"y\"][slice_idx]:.2f})', \r\n                    fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):\r\n        \"\"\"\r\n        Check if mass and momentum are conserved over time.\r\n        Critical for physical consistency - should be flat lines!\r\n        \r\n        Note: Uses higher resolution for power spectrum to ensure accurate integration.\r\n        \"\"\"\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum\r\n        if n_grid is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_grid = 100\r\n        \r\n        print(f\"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...\")\r\n        \r\n        conservation_data = {\r\n            'times': [],\r\n            'total_mass': [],\r\n            'total_momentum_x': [],\r\n            'total_momentum_y': []\r\n        }\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # Integrate using trapezoidal rule\r\n                dx = 1.0 / (n_grid - 1)\r\n                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, use coarser grid\r\n                n_grid_3d = max(40, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                z = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                dx = 1.0 / (n_grid_3d - 1)\r\n                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n        \r\n        return conservation_data\r\n    \r\n    def plot_conservation_laws(self, conservation_data):\r\n        \"\"\"Plot conserved quantities over time - should be flat lines!\"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = conservation_data['times']\r\n        \r\n        # Mass conservation\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        mass = np.array(conservation_data['total_mass'])\r\n        mass_initial = mass[0]\r\n        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift\r\n        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)\r\n        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Mass drift (%)', fontsize=11)\r\n        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Momentum X conservation\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        px = np.array(conservation_data['total_momentum_x'])\r\n        px_initial = px[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(px_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum X absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            px_drift = ((px - px_initial) / abs(px_initial)) * 100\r\n            ylabel = 'Momentum X drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax2.plot(times, px_drift, 'g-', linewidth=2.5)\r\n        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel(ylabel, fontsize=11)\r\n        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Momentum Y conservation\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        py = np.array(conservation_data['total_momentum_y'])\r\n        py_initial = py[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(py_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum Y absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            py_drift = ((py - py_initial) / abs(py_initial)) * 100\r\n            ylabel = 'Momentum Y drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax3.plot(times, py_drift, 'orange', linewidth=2.5)\r\n        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel(ylabel, fontsize=11)\r\n        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):\r\n        \"\"\"\r\n        Compute FFT of density field at different times.\r\n        Reveals spectral bias and high-frequency mode damping.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing spectral content at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        spectra = []\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # 2D FFT\r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                # Create wavenumber grid\r\n                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5\r\n                n_grid_3d = max(64, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n        \r\n        return spectra\r\n    \r\n    def plot_spectral_evolution(self, spectra_data):\r\n        \"\"\"\r\n        Plot power spectrum evolution over time.\r\n        Shows if high-frequency modes are being damped (spectral bias).\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        # Left: Radially averaged power spectra\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        \r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], \r\n                          linewidth=2, markersize=4, label=f\"t={spec['time']:.2f}\")\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Right: 2D power spectrum at final time\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        final_spec = spectra_data[-1]\r\n        power_log = np.log10(final_spec['power'] + 1e-12)\r\n        \r\n        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',\r\n                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),\r\n                              -np.max(final_spec['k']), np.max(final_spec['k'])])\r\n        ax2.set_xlabel('k_x', fontsize=11)\r\n        ax2.set_ylabel('k_y', fontsize=11)\r\n        ax2.set_title(f'2D Power Spectrum at t={final_spec[\"time\"]:.2f}', \r\n                     fontsize=12, fontweight='bold')\r\n        cbar = plt.colorbar(im, ax=ax2)\r\n        cbar.set_label('log₁₀(Power)', fontsize=10)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):\r\n        \"\"\"\r\n        Compute field statistics over time to track error accumulation.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing temporal statistics at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        stats = {\r\n            'times': [],\r\n            'rho_mean': [],\r\n            'rho_std': [],\r\n            'rho_max': [],\r\n            'grad_rho_mean': [],\r\n            'grad_rho_max': []\r\n        }\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients (retain_graph=True for first call since we need multiple gradients)\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5 for computational efficiency\r\n                n_spatial_3d = max(50, n_spatial // 2)\r\n                x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                Z_flat = Z.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                Z_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n        \r\n        return stats\r\n    \r\n    def plot_temporal_statistics(self, stats_data):\r\n        \"\"\"\r\n        Plot field statistics over time.\r\n        Shows if solution becomes unphysical or gradients explode.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = stats_data['times']\r\n        \r\n        # Density statistics\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax1.fill_between(times,\r\n                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),\r\n                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),\r\n                         alpha=0.3, color='b', label='±1 std')\r\n        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Gradient statistics\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)\r\n        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Growth rate analysis\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        rho_max = np.array(stats_data['rho_max'])\r\n        if len(rho_max) &gt; 5:\r\n            # Fit exponential growth\r\n            log_rho = np.log(rho_max + 1e-10)\r\n            valid_idx = np.isfinite(log_rho)\r\n            if valid_idx.sum() &gt; 5:\r\n                times_valid = np.array(times)[valid_idx]\r\n                log_rho_valid = log_rho[valid_idx]\r\n                coeffs = np.polyfit(times_valid, log_rho_valid, 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                \r\n                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, \r\n                           label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.set_xlabel('Time', fontsize=11)\r\n                ax3.set_ylabel('Max Density (log scale)', fontsize=11)\r\n                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')\r\n                ax3.legend(fontsize=10)\r\n                ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):\r\n        \"\"\"\r\n        Track how prediction errors grow and accumulate over time.\r\n        Helps identify when the solution starts diverging from physical behavior.\r\n        \r\n        Now computes ACTUAL PDE residuals and gradient norms including phi.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        \r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        error_data = {\r\n            'times': time_points,\r\n            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},\r\n            'field_ranges': {'rho': [], 'vx': [], 'vy': []},\r\n            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},\r\n            'max_residuals': []\r\n        }\r\n        \r\n        if dimension == 3:\r\n            error_data['pde_residuals']['momentum_z'] = []\r\n            error_data['field_ranges']['vz'] = []\r\n            error_data['gradient_norms']['vz'] = []\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n            else:  # dimension == 3\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                Z_flat = Z.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n            \r\n            # Compute ACTUAL PDE residuals using the proper residue function\r\n            if dimension == 2:\r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n                \r\n            else:  # 3D\r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n            \r\n            # Get predictions for field statistics\r\n            pred = model(colloc)\r\n            rho = pred[:, 0:1]\r\n            vx = pred[:, 1:2]\r\n            vy = pred[:, 2:3]\r\n            phi = pred[:, -1:]  # Last column is phi\r\n            \r\n            # Track field statistics\r\n            error_data['field_ranges']['rho'].append(torch.std(rho).item())\r\n            error_data['field_ranges']['vx'].append(torch.std(vx).item())\r\n            error_data['field_ranges']['vy'].append(torch.std(vy).item())\r\n            if dimension == 3:\r\n                vz = pred[:, 3:4]\r\n                error_data['field_ranges']['vz'].append(torch.std(vz).item())\r\n            \r\n            # Track gradient norms for ALL fields including phi\r\n            with torch.enable_grad():\r\n                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                \r\n                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())\r\n                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())\r\n                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())\r\n                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())\r\n                \r\n                if dimension == 3:\r\n                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())\r\n            \r\n            # Max residual magnitude (now including Poisson!)\r\n            max_res = max(\r\n                error_data['pde_residuals']['continuity'][-1],\r\n                error_data['pde_residuals']['momentum_x'][-1],\r\n                error_data['pde_residuals']['momentum_y'][-1],\r\n                error_data['pde_residuals']['poisson'][-1]\r\n            )\r\n            if dimension == 3:\r\n                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])\r\n            \r\n            error_data['max_residuals'].append(max_res)\r\n        \r\n        return error_data\r\n    \r\n    def plot_temporal_error_accumulation(self, error_data):\r\n        \"\"\"\r\n        Visualize how errors accumulate and grow over time.\r\n        Shows which equations fail first and error growth rates.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = error_data['times']\r\n        \r\n        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)\r\n        # IMPORTANT: Now showing actual Poisson residuals!\r\n        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)\r\n        if 'momentum_z' in error_data['pde_residuals']:\r\n            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)\r\n        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Field Range Evolution (variance growth)\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)\r\n        if 'vz' in error_data['field_ranges']:\r\n            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Field Standard Deviation', fontsize=11)\r\n        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=9)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: Maximum Residual Growth\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)\r\n        if len(times) &gt; 5:\r\n            # Fit exponential growth\r\n            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)\r\n            valid_idx = np.isfinite(log_res)\r\n            if valid_idx.sum() &gt; 3:\r\n                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.legend(fontsize=10)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Max Residual (log)', fontsize=11)\r\n        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)\r\n        if 'vz' in error_data['gradient_norms']:\r\n            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)\r\n        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax4.legend(fontsize=9, loc='best')\r\n        ax4.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 5: Cumulative Error Metric\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        cumulative_error = np.cumsum(error_data['max_residuals'])\r\n        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Cumulative Error', fontsize=11)\r\n        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3)\r\n        \r\n        # Plot 6: Error Growth Rate\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        if len(times) &gt; 1:\r\n            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)\r\n            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n            ax6.set_xlabel('Time', fontsize=11)\r\n            ax6.set_ylabel('Error Growth Rate', fontsize=11)\r\n            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')\r\n            ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_spectral_bias_analysis(self, spectra_data):\r\n        \"\"\"\r\n        Enhanced spectral analysis focused on detecting spectral bias.\r\n        Shows frequency damping and mode evolution over time.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: Radially averaged power spectra at different times\r\n        ax1 = fig.add_subplot(gs[0, :2])\r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.sum() &gt; 0:\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,\r\n                          label=f't={spec[\"time\"]:.2f}', marker='o', markersize=3)\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power P(k)', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=8, ncol=2)\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Spectral Damping Rate\r\n        ax2 = fig.add_subplot(gs[0, 2])\r\n        if len(spectra_data) &gt;= 2:\r\n            # Compare first and last spectra\r\n            spec_0 = spectra_data[0]\r\n            spec_f = spectra_data[-1]\r\n            \r\n            # Compute power ratio (damping)\r\n            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))\r\n            k_bins = np.linspace(0, k_max, 20)\r\n            damping_ratio = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask_0 = (spec_0['k'] &gt;= k_bins[i]) &amp; (spec_0['k'] &lt; k_bins[i+1])\r\n                mask_f = (spec_f['k'] &gt;= k_bins[i]) &amp; (spec_f['k'] &lt; k_bins[i+1])\r\n                if mask_0.sum() &gt; 0 and mask_f.sum() &gt; 0:\r\n                    p0 = np.mean(spec_0['power'][mask_0])\r\n                    pf = np.mean(spec_f['power'][mask_f])\r\n                    if p0 &gt; 0:\r\n                        damping_ratio.append(pf / p0)\r\n                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)\r\n                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')\r\n                ax2.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)\r\n                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')\r\n                ax2.legend(fontsize=10)\r\n                ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: High-k Mode Tracking\r\n        ax3 = fig.add_subplot(gs[1, 0])\r\n        times = [spec['time'] for spec in spectra_data]\r\n        \r\n        # Track power in different k ranges\r\n        low_k_power = []\r\n        mid_k_power = []\r\n        high_k_power = []\r\n        \r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            k_max = np.max(k_flat)\r\n            low_k_power.append(np.mean(p_flat[k_flat &lt; k_max/3]))\r\n            mid_k_power.append(np.mean(p_flat[(k_flat &gt;= k_max/3) &amp; (k_flat &lt; 2*k_max/3)]))\r\n            high_k_power.append(np.mean(p_flat[k_flat &gt;= 2*k_max/3]))\r\n        \r\n        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)\r\n        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)\r\n        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Average Power (log)', fontsize=11)\r\n        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Spectral Index Evolution\r\n        ax4 = fig.add_subplot(gs[1, 1])\r\n        spectral_indices = []\r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            # Fit power law in log space\r\n            valid = (k_flat &gt; 0) &amp; (p_flat &gt; 0)\r\n            if valid.sum() &gt; 10:\r\n                log_k = np.log(k_flat[valid])\r\n                log_p = np.log(p_flat[valid])\r\n                coeffs = np.polyfit(log_k, log_p, 1)\r\n                spectral_indices.append(coeffs[0])  # Slope = spectral index\r\n            else:\r\n                spectral_indices.append(np.nan)\r\n        \r\n        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)\r\n        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)\r\n        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Density field snapshots\r\n        ax5 = fig.add_subplot(gs[1, 2])\r\n        # Show first, middle, last density fields\r\n        n_show = min(3, len(spectra_data))\r\n        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]\r\n        \r\n        for i, idx in enumerate(indices[:n_show]):\r\n            rho = spectra_data[idx]['rho']\r\n            # Compute structure function or variance\r\n            rho_var = np.var(rho)\r\n            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)\r\n            ax5.text(i, rho_var, f't={spectra_data[idx][\"time\"]:.2f}', \r\n                    ha='center', va='bottom', fontsize=9)\r\n        \r\n        ax5.set_xlabel('Time Snapshot', fontsize=11)\r\n        ax5.set_ylabel('Density Variance', fontsize=11)\r\n        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')\r\n        ax5.set_xticks(range(n_show))\r\n        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])\r\n        ax5.grid(True, alpha=0.3, axis='y')\r\n        \r\n        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):\r\n        \"\"\"\r\n        Check for signs of solution instability or unphysical behavior.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_check)\r\n        \r\n        stability_data = {\r\n            'times': time_points,\r\n            'density_bounds': {'min': [], 'max': [], 'mean': []},\r\n            'velocity_norms': [],\r\n            'negative_density_fraction': [],\r\n            'extreme_values': [],\r\n            'solution_smoothness': []\r\n        }\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n            else:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n            \r\n            with torch.no_grad():\r\n                pred = model(colloc)\r\n                rho = pred[:, 0].cpu().numpy()\r\n                vx = pred[:, 1].cpu().numpy()\r\n                vy = pred[:, 2].cpu().numpy()\r\n                \r\n                # Density bounds\r\n                stability_data['density_bounds']['min'].append(np.min(rho))\r\n                stability_data['density_bounds']['max'].append(np.max(rho))\r\n                stability_data['density_bounds']['mean'].append(np.mean(rho))\r\n                \r\n                # Velocity norms\r\n                v_norm = np.sqrt(vx**2 + vy**2)\r\n                if dimension == 3:\r\n                    vz = pred[:, 3].cpu().numpy()\r\n                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)\r\n                stability_data['velocity_norms'].append(np.max(v_norm))\r\n                \r\n                # Negative density fraction\r\n                neg_frac = np.sum(rho &lt; 0) / len(rho)\r\n                stability_data['negative_density_fraction'].append(neg_frac)\r\n                \r\n                # Extreme value detection (more than 10x mean)\r\n                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) &gt; 10 * np.std(rho)) / len(rho)\r\n                stability_data['extreme_values'].append(extreme_frac)\r\n                \r\n                # Solution smoothness (Laplacian magnitude as proxy)\r\n                if dimension == 2:\r\n                    rho_2d = rho.reshape(n_spatial, n_spatial)\r\n                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + \r\n                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))\r\n                    stability_data['solution_smoothness'].append(np.mean(laplacian))\r\n                else:\r\n                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D\r\n        \r\n        return stability_data\r\n    \r\n    def plot_solution_stability(self, stability_data):\r\n        \"\"\"\r\n        Visualize solution stability metrics to detect unphysical behavior.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = stability_data['times']\r\n        \r\n        # Plot 1: Density Bounds\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)\r\n        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Plot 2: Velocity Norms\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)\r\n        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 3: Negative Density Fraction\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)\r\n        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Negative Density (%)', fontsize=11)\r\n        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        # Plot 4: Extreme Values\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)\r\n        ax4.set_title('Outlier Detection (&gt;10σ)', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Solution Smoothness\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)\r\n        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 6: Stability Summary\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        # Compute overall stability score (0-1, higher is worse)\r\n        neg_dens_score = np.array(stability_data['negative_density_fraction'])\r\n        extreme_score = np.array(stability_data['extreme_values'])\r\n        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score\r\n        \r\n        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)\r\n        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')\r\n        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')\r\n        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')\r\n        ax6.set_xlabel('Time', fontsize=11)\r\n        ax6.set_ylabel('Instability Score (%)', fontsize=11)\r\n        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')\r\n        ax6.legend(fontsize=9)\r\n        ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_comprehensive_diagnostics(self, model, dimension, tmax):\r\n        \"\"\"\r\n        Run all post-training diagnostics for long-term evolution analysis.\r\n        \r\n        Generates 6 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail\r\n        3. Conservation law violations - Mass/momentum drift over time\r\n        4. Temporal error accumulation - How errors compound over time\r\n        5. Spectral bias analysis - Frequency damping and mode evolution\r\n        6. Solution stability - Detecting unphysical behavior\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            dimension: Spatial dimension (2 or 3)\r\n            tmax: Maximum time to analyze\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Long-Term Evolution Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: PDE Residual Evolution\r\n        print(\"\\n[1/5] Computing PDE residual evolution...\")\r\n        residuals = self.compute_residual_heatmap(model, dimension, tmax)\r\n        self.plot_residual_heatmaps(residuals)\r\n        print(\"      [OK] PDE residual evolution saved\")\r\n        \r\n        # Plot 3: Conservation Laws\r\n        print(\"\\n[2/5] Analyzing conservation law violations...\")\r\n        conservation = self.check_conservation_laws(model, dimension, tmax)\r\n        self.plot_conservation_laws(conservation)\r\n        print(\"      [OK] Conservation violations plot saved\")\r\n        \r\n        # Plot 4: Temporal Error Accumulation\r\n        print(\"\\n[3/5] Computing temporal error accumulation...\")\r\n        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)\r\n        self.plot_temporal_error_accumulation(error_data)\r\n        print(\"      [OK] Error accumulation plot saved\")\r\n        \r\n        # Plot 5: Spectral Bias Analysis\r\n        print(\"\\n[4/5] Analyzing spectral bias and mode damping...\")\r\n        spectra = self.compute_spectral_content(model, dimension, tmax)\r\n        self.plot_spectral_bias_analysis(spectra)\r\n        print(\"      [OK] Spectral bias analysis saved\")\r\n        \r\n        # Plot 6: Solution Stability\r\n        print(\"\\n[5/5] Checking solution stability...\")\r\n        stats = self.compute_temporal_statistics(model, dimension, tmax)\r\n        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)\r\n        self.plot_solution_stability(stability_data)\r\n        print(\"      [OK] Solution stability diagnostics saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nLong-Term Evolution Diagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")\r\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns\")\r\n        print(\"  3. conservation_laws.png - Conservation law drift over time\")\r\n        print(\"  4. temporal_error_accumulation.png - Error growth and compounding\")\r\n        print(\"  5. spectral_bias_analysis.png - Frequency damping and spectral issues\")\r\n        print(\"  6. solution_stability.png - Physical validity and stability metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n    \r\n    \r\n    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):\r\n        \"\"\"\r\n        Smart dispatcher: runs appropriate diagnostics based on case type.\r\n        \r\n        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics\r\n        - 3D power spectrum: IC fitting diagnostics  \r\n        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)\r\n            true_ic_data: Dict with IC data (required for 3D power spectrum)\r\n                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        if self.is_3d_power_spectrum:\r\n            # 3D power spectrum: Focus on IC fitting\r\n            if true_ic_data is None:\r\n                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")\r\n            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)\r\n        else:\r\n            # All other cases: Focus on temporal evolution\r\n            if tmax is None:\r\n                raise ValueError(\"tmax required for temporal evolution diagnostics\")\r\n            self.run_comprehensive_diagnostics(model, self.dimension, tmax)\r\n    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================\r\n    \r\n    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):\r\n        \"\"\"\r\n        Compare predicted vs true ICs spatially for both 2D and 3D cases.\r\n        Critical for diagnosing WHERE the network fails to fit the IC structure.\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC collocation points and true values\r\n                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}\r\n                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            n_grid: Grid resolution per dimension (for visualization grid)\r\n        \r\n        Returns:\r\n            Dict with predicted and true fields, plus errors\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing IC spatial comparison on training IC points...\")\r\n        \r\n        # Use the actual IC collocation points from training\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions at IC points\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n                phi_pred = pred[:, 4].cpu().numpy()\r\n            else:\r\n                phi_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # For visualization, create a regular grid\r\n        x = torch.linspace(0, 1, n_grid, device=device)\r\n        y = torch.linspace(0, 1, n_grid, device=device)\r\n        X, Y = torch.meshgrid(x, y, indexing='ij')\r\n        T = torch.zeros_like(X)\r\n        \r\n        if self.dimension == 3:\r\n            # For 3D, take z=0.5 slice\r\n            Z = torch.full_like(X, 0.5)\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n        else:\r\n            # For 2D\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n        \r\n        # Get predictions on visualization grid\r\n        with torch.no_grad():\r\n            pred_viz = model(colloc_viz)\r\n            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()\r\n            else:\r\n                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n        \r\n        # For \"true\" visualization, we need to interpolate from IC points to regular grid\r\n        # Extract spatial coordinates from IC collocation points (detach first)\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Find points near z=0.5 for visualization\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        if mask_z.sum() &gt; 100:  # Need enough points for interpolation\r\n            from scipy.interpolate import griddata\r\n            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n            X_viz = X.cpu().numpy()\r\n            Y_viz = Y.cpu().numpy()\r\n            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])\r\n            \r\n            # Interpolate with nearest neighbor fallback for NaN values\r\n            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            \r\n            # For uniform density (all values the same), fill NaNs with the constant value\r\n            # For varying fields, use nearest neighbor interpolation for NaN regions\r\n            if np.isnan(rho_true_viz).any():\r\n                if np.std(rho_true[mask_z]) &lt; 1e-10:\r\n                    # Uniform field - fill with mean\r\n                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))\r\n                else:\r\n                    # Varying field - use nearest neighbor for gaps\r\n                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)\r\n            \r\n            if np.isnan(vx_true_viz).any():\r\n                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)\r\n            \r\n            if np.isnan(vy_true_viz).any():\r\n                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)\r\n            \r\n            if self.dimension == 3:\r\n                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n                if np.isnan(vz_true_viz).any():\r\n                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)\r\n        else:\r\n            # Not enough points for interpolation, use predicted as reference\r\n            print(\"  [WARN] Not enough IC points for ground truth visualization\")\r\n            rho_true_viz = rho_pred_viz\r\n            vx_true_viz = vx_pred_viz\r\n            vy_true_viz = vy_pred_viz\r\n            if self.dimension == 3:\r\n                vz_true_viz = vz_pred_viz\r\n        \r\n        result = {\r\n            'x': x.cpu().numpy(),\r\n            'y': y.cpu().numpy(),\r\n            'rho_pred': rho_pred_viz,\r\n            'rho_true': rho_true_viz,\r\n            'vx_pred': vx_pred_viz,\r\n            'vx_true': vx_true_viz,\r\n            'vy_pred': vy_pred_viz,\r\n            'vy_true': vy_true_viz,\r\n            'phi_pred': phi_pred_viz,\r\n            # Also store scatter point data for metrics\r\n            'rho_pred_scatter': rho_pred,\r\n            'rho_true_scatter': rho_true,\r\n            'vx_pred_scatter': vx_pred,\r\n            'vx_true_scatter': vx_true,\r\n            'vy_pred_scatter': vy_pred,\r\n            'vy_true_scatter': vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            result.update({\r\n                'vz_pred': vz_pred_viz,\r\n                'vz_true': vz_true_viz,\r\n                'vz_pred_scatter': vz_pred,\r\n                'vz_true_scatter': vz_true,\r\n            })\r\n        \r\n        return result\r\n    \r\n    def plot_ic_power_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot power spectrum comparison for all velocity components.\r\n        Shows which scales (k-modes) are missing or damped.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_ic_component_convergence(self):\r\n        \"\"\"\r\n        Plot IC component loss convergence over training.\r\n        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.\r\n        \"\"\"\r\n        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:\r\n            print(\"IC component tracking not available.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        \r\n        # Replace zeros and very small values with a minimum threshold for log plotting\r\n        def safe_log_data(data):\r\n            \"\"\"Replace zeros/negative values with small epsilon for log plotting.\"\"\"\r\n            data_array = np.array(data)\r\n            data_array = np.where(data_array &lt;= 0, 1e-12, data_array)\r\n            return data_array\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: All IC components on same plot\r\n        ax1 = fig.add_subplot(gs[0, :])\r\n        \r\n        # Check if rho is zero/constant (uniform field case)\r\n        rho_is_uniform = np.max(self.history['ic_rho']) &lt; 1e-10\r\n        \r\n        if not rho_is_uniform:\r\n            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', \r\n                        marker='o', markersize=3, markevery=max(1, len(iters)//20))\r\n        else:\r\n            # Plot a dashed line at the bottom to indicate uniform/zero loss\r\n            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)\r\n        \r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', \r\n                    marker='s', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', \r\n                    marker='^', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', \r\n                    marker='d', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', \r\n                    marker='v', markersize=3, markevery=max(1, len(iters)//20))\r\n        \r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10, ncol=5)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Individual component plots\r\n        components = [\r\n            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),\r\n            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),\r\n            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),\r\n        ]\r\n        \r\n        for hist_key, title, grid_pos, is_uniform in components:\r\n            ax = fig.add_subplot(grid_pos)\r\n            if is_uniform:\r\n                # Show a flat line at bottom with annotation\r\n                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)\r\n                ax.text(0.5, 0.5, 'Uniform IC\\n(no spatial variation)', \r\n                       transform=ax.transAxes, ha='center', va='center',\r\n                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\r\n                ax.set_ylim([1e-13, 1e-10])\r\n            else:\r\n                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')\r\n            \r\n            ax.set_xlabel('Iteration', fontsize=10)\r\n            ax.set_ylabel('Loss (log scale)', fontsize=10)\r\n            ax.set_title(title, fontsize=11, fontweight='bold')\r\n            ax.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_ic_correlation_metrics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Compute spatial correlation between predicted and true ICs.\r\n        High correlation = good spatial structure capture.\r\n        \"\"\"\r\n        print(f\"  Computing IC correlation metrics on training IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy().flatten()\r\n            vx_pred = pred[:, 1].cpu().numpy().flatten()\r\n            vy_pred = pred[:, 2].cpu().numpy().flatten()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy().flatten()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()\r\n        \r\n        # Compute correlations - handle uniform fields (zero variance)\r\n        def safe_corrcoef(pred, true):\r\n            \"\"\"Compute correlation, handling constant fields gracefully.\"\"\"\r\n            # Check if either field is constant\r\n            if np.std(pred) &lt; 1e-10 or np.std(true) &lt; 1e-10:\r\n                # For uniform fields, correlation is undefined\r\n                # If both are uniform and equal, perfect match (1.0)\r\n                # If different constants, poor match (use normalized error)\r\n                if np.std(pred) &lt; 1e-10 and np.std(true) &lt; 1e-10:\r\n                    # Both uniform - check if they match\r\n                    mean_diff = abs(np.mean(pred) - np.mean(true))\r\n                    return 1.0 if mean_diff &lt; 1e-6 else 0.0\r\n                else:\r\n                    # One uniform, one not - cannot capture structure\r\n                    return 0.0\r\n            else:\r\n                return np.corrcoef(pred, true)[0, 1]\r\n        \r\n        correlations = {\r\n            'rho': safe_corrcoef(rho_pred, rho_true),\r\n            'vx': safe_corrcoef(vx_pred, vx_true),\r\n            'vy': safe_corrcoef(vy_pred, vy_true),\r\n        }\r\n        \r\n        # Compute RMS errors\r\n        rms_errors = {\r\n            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),\r\n            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),\r\n            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)\r\n            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))\r\n        \r\n        return correlations, rms_errors\r\n    \r\n    def plot_ic_metrics_summary(self, correlations, rms_errors):\r\n        \"\"\"\r\n        Summary plot: correlation and RMS error for each IC component.\r\n        Quick visual diagnostic of IC fitting quality.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        components = ['rho', 'vx', 'vy', 'vz']\r\n        labels = ['ρ', 'vx', 'vy', 'vz']\r\n        \r\n        # Correlation plot\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        corr_vals = [correlations[c] for c in components]\r\n        \r\n        # Handle NaN/inf correlations from uniform fields\r\n        corr_vals_safe = []\r\n        for val in corr_vals:\r\n            if np.isnan(val) or np.isinf(val):\r\n                corr_vals_safe.append(0.0)  # Show as 0 for visualization\r\n            else:\r\n                corr_vals_safe.append(val)\r\n        \r\n        colors = ['green' if c &gt; 0.9 else 'orange' if c &gt; 0.7 else 'red' for c in corr_vals_safe]\r\n        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')\r\n        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')\r\n        ax1.set_ylabel('Correlation Coefficient', fontsize=11)\r\n        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax1.set_ylim([0, 1.05])\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):\r\n            height = bar.get_height()\r\n            if np.isnan(orig_val) or np.isinf(orig_val):\r\n                label_text = 'N/A\\n(uniform)'\r\n                fontsize = 8\r\n            else:\r\n                label_text = f'{val:.3f}'\r\n                fontsize = 10\r\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\r\n                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')\r\n        \r\n        # RMS Error plot\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        rms_vals = [rms_errors[c] for c in components]\r\n        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax2.set_ylabel('RMS Error', fontsize=11)\r\n        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val in zip(bars2, rms_vals):\r\n            height = bar.get_height()\r\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\r\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\r\n        \r\n        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Run diagnostics for 3D power spectrum case.\r\n        Focus on IC fitting quality, not temporal evolution.\r\n        \r\n        Generates 5 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum comparison (scale-by-scale)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running 3D Power Spectrum IC Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[1/4] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power Spectrum Comparison\r\n        print(\"\\n[2/4] Computing IC power spectrum comparison...\")\r\n        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n        self.plot_ic_power_spectrum_comparison(ps_data)\r\n        print(\"      [OK] IC power spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[3/4] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[4/4] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit\")\r\n        print(\"  4. ic_component_convergence.png - Which IC component fails\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        for comp in ['rho', 'vx', 'vy', 'vz']:\r\n            print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================\r\n    \r\n    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):\r\n        \"\"\"\r\n        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).\r\n        \r\n        The 5 plots are:\r\n        1. Training diagnostics (loss convergence and balance)\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            final_iteration: Final iteration number\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Unified Training Diagnostics (5 plots)\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 1: Training diagnostics\r\n        print(\"\\n[1/5] Plotting training diagnostics...\")\r\n        self.plot_diagnostics(final_iteration)\r\n        print(\"      [OK] Training diagnostics saved\")\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[2/5] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power/Field Spectrum Comparison\r\n        print(\"\\n[3/5] Computing spectrum comparison...\")\r\n        if self.perturbation_type == 'power_spectrum':\r\n            # For power spectrum: compare power spectra\r\n            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n            self.plot_ic_power_spectrum_comparison(ps_data)\r\n        else:\r\n            # For sinusoidal: compute field spectra from predictions\r\n            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)\r\n            self.plot_field_spectrum_comparison(ps_data)\r\n        print(\"      [OK] Spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[4/5] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[5/5] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Spectrum comparison\")\r\n        print(\"  4. ic_component_convergence.png - IC component convergence\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']\r\n        for comp in comps:\r\n            if comp in correlations:\r\n                print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):\r\n        \"\"\"\r\n        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.\r\n        Similar to power spectrum comparison but computed from actual fields.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing field spectrum comparison on IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # Need to interpolate to regular grid for FFT\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Use points near z=0.5 for 2D slice\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n            if mask_z.sum() &lt; 100:\r\n                mask_z = np.ones(len(z_ic), dtype=bool)\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        # Create regular grid for FFT\r\n        x_grid = np.linspace(0, 1, n_grid)\r\n        y_grid = np.linspace(0, 1, n_grid)\r\n        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')\r\n        \r\n        # Interpolate from scattered IC points to regular grid\r\n        from scipy.interpolate import griddata\r\n        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])\r\n        \r\n        # Interpolate density\r\n        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Interpolate velocities\r\n        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        if self.dimension == 3:\r\n            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Compute power spectra\r\n        def compute_power_spectrum(field):\r\n            fft = np.fft.fft2(field)\r\n            power = np.abs(np.fft.fftshift(fft))**2\r\n            \r\n            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            kx_shift = np.fft.fftshift(kx)\r\n            ky_shift = np.fft.fftshift(ky)\r\n            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n            K = np.sqrt(KX**2 + KY**2)\r\n            \r\n            return power, K\r\n        \r\n        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)\r\n        power_rho_true, _ = compute_power_spectrum(rho_true_grid)\r\n        \r\n        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)\r\n        power_vx_true, _ = compute_power_spectrum(vx_true_grid)\r\n        \r\n        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)\r\n        power_vy_true, _ = compute_power_spectrum(vy_true_grid)\r\n        \r\n        result = {\r\n            'K': K,\r\n            'power_rho_pred': power_rho_pred,\r\n            'power_rho_true': power_rho_true,\r\n            'power_vx_pred': power_vx_pred,\r\n            'power_vx_true': power_vx_true,\r\n            'power_vy_pred': power_vy_pred,\r\n            'power_vy_true': power_vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)\r\n            power_vz_true, _ = compute_power_spectrum(vz_true_grid)\r\n            result['power_vz_pred'] = power_vz_pred\r\n            result['power_vz_true'] = power_vz_true\r\n        \r\n        return result\r\n    \r\n    def plot_field_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot field spectrum comparison for sinusoidal cases.\r\n        Shows density and velocity spectra.\r\n        \"\"\"\r\n        if self.dimension == 3:\r\n            fig = plt.figure(figsize=(18, 5))\r\n            gs = GridSpec(1, 4, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        else:\r\n            fig = plt.figure(figsize=(14, 5))\r\n            gs = GridSpec(1, 3, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()"
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 1 file</summary>
      
Lint paths:

- `training/training_diagnostics.py`

Lint results

Lint errors for `training/training_diagnostics.py`


| Message | Range | Severity |
|-------|-------|----------|
| Variable "z" is not accessed | 218:13-218:14 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "vz_r" is not accessed | 227:36-227:40 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 332:33-332:38 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 332:42-332:47 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 333:31-333:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 333:40-333:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 334:31-334:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 334:40-334:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 359:33-359:38 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 359:42-359:47 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 359:51-359:56 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 360:31-360:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 360:40-360:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 360:49-360:54 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 361:31-361:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 361:40-361:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 361:49-361:54 | DIAGNOSTIC_SEVERITY_HINT |
| "stats_data" is not accessed | 1074:66-1074:76 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "phi_pred" is not accessed | 1340:17-1340:25 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "phi_pred" is not accessed | 1342:17-1342:25 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "device" is not accessed | 1866:9-1866:15 | DIAGNOSTIC_SEVERITY_HINT |


</details>
</tool-use>

---

Applied the gradient fixes in `compute_residual_heatmap` and switched the residual heatmap to a dynamic color scale with range annotations in `plot_residual_heatmaps`, so derivatives are computed correctly and the plots reveal actual structure.

### Changes
- `training/training_diagnostics.py`: added `.requires_grad_(True)` collocation tensors and removed `torch.no_grad()` in both 2D and 3D residual computations.
- `training/training_diagnostics.py`: updated residual log scaling and color limits; titles now include min/max ranges.

If you want the adaptive-floor variant (Option B) instead of fully dynamic scaling, I can swap that in.

---

_**User (2026-01-24 16:39Z)**_

Which would be better for diagnosis purposes? Option A or B?

---

_**Agent (model claude-4.5-opus-high-thinking, mode Ask)**_

<think><details><summary>Thought Process</summary>
The user is asking which option would be better for diagnosis purposes - Option A (fully dynamic scale) or Option B (adaptive with floor).

Let me think about the trade-offs:

**Option A - Fully Dynamic Scale:**
- Shows the full range of whatever data is present
- Pros: Never hides any structure, always uses the full color range
- Cons: Can be misleading when comparing across different plots or runs - if one run has residuals from 1e-8 to 1e-6 and another has 1e-4 to 1e-2, they'll both look "similar" with full color range but represent very different physics

**Option B - Adaptive with Floor:**
- Shows 4 orders of magnitude from the max, with a floor at 1e-12
- Pros: 
  - Gives context about absolute magnitude (you can see if residuals are small vs large)
  - Consistent across plots within the same figure (all 4 PDE residual plots use same logic)
  - Highlights the structure while still giving sense of scale
- Cons: If all residuals are very uniform (within less than 4 orders of magnitude), you might have some "unused" color range

For **diagnostic purposes**, I would recommend **Option B** because:

1. **Absolute scale matters**: When diagnosing PINN failures, knowing whether residuals are O(1e-6) vs O(1e-2) is critical. Option A would make both look identical.

2. **Comparing equations**: You want to see which PDE (continuity, momentum, Poisson) has the worst residuals. With Option A, each subplot has its own scale, making comparison harder. With Option B, since it's based on max, you can see relative magnitudes.

3. **Temporal evolution insight**: You want to see if residuals grow over time. With a fixed floor but adaptive max, you can see this growth pattern. With fully dynamic, early and late times would look similar even if residuals grew 100x.
   
The 4-order-of-magnitude window typically captures the spatial structure where residuals concentrate while preserving absolute context. When diagnosing where physics breaks down, Option B prevents numerical noise from dominating the visualization during good training phases, while still revealing problematic regions when residuals are genuinely large. I'd suggest including the actual residual range in the plot titles so you have both the visual pattern and the quantitative reference.</details></think>

---

**Option B (Adaptive with floor)** is better for diagnostic purposes. Here's why:

### Why Option B is Superior for Diagnosis

1. **Absolute Scale Context**
   - When diagnosing PINN failures, you need to know *how big* the residuals actually are
   - Option A: Residuals of 10⁻⁸ to 10⁻⁶ and 10⁻² to 10⁰ would look identical (both fill the color range)
   - Option B: You can immediately see "these residuals are O(10⁻²) — that's a problem!"

2. **Cross-Equation Comparison**
   - You're plotting 4 equations (Continuity, Momentum X/Y, Poisson) side by side
   - With Option B's consistent logic, you can visually compare which equation is failing worst
   - Option A makes each subplot look "full" regardless of absolute magnitude

3. **Temporal Pattern Detection**
   - Key diagnostic question: "Do residuals grow over time?"
   - Option B: Late-time plots will show brighter colors if residuals grew
   - Option A: Early and late times look similar even if residuals grew 1000×

4. **Noise Suppression**
   - If training succeeded and residuals are all O(10⁻¹⁰), Option A would amplify numerical noise into false "structure"
   - Option B's floor at 10⁻¹² prevents this

### The Current Implementation

Since I applied Option A (fully dynamic), you may want to switch to Option B. The change would be:

```python
# In plot_residual_heatmaps, replace:
data_min = np.min(resid_log)
data_max = np.max(resid_log)

# With Option B:
data_max = np.max(resid_log)
data_min = max(data_max - 4, -12)  # 4 orders of magnitude, floor at 1e-12
```

This shows 4 orders of magnitude of structure while maintaining absolute scale context. The title still displays actual min/max so you have both pieces of information.

Would you like me to show the full edit to switch to Option B?

---

_**User (2026-01-24 16:39Z)**_

Implement option B then instead of A

---

_**Agent (model gpt-5.2-codex-low, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Adjusting min/max logic and title for option B**

I'm switching to option B's data_min calculation using max(data_max - 4, -12) but tracking actual_min separately for the title, which will display actual_min and data_max as requested. I'll apply this patch and then proceed to read_lints.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "training/training_diagnostics.py"
}
```
Additional data:

```json
{
  "codeblockId": "16e741be-ee4f-4301-a2ae-da2a07b3eced",
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/training/training_diagnostics.py",
      "query": "",
      "fragment": "",
      "_formatted": "file:///c%3A/Users/tirth/Documents/Projects/GRINN/Power%20spectrum%20perturbations/training/training_diagnostics.py",
      "_fsPath": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\training_diagnostics.py"
    },
    "codeblockId": "16e741be-ee4f-4301-a2ae-da2a07b3eced",
    "codeBlockIdx": 0,
    "content": "import os\r\nimport numpy as np\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\n\r\nclass TrainingDiagnostics:\r\n    \"\"\"\r\n    Comprehensive diagnostics for PINN performance analysis.\r\n    \r\n    TWO DIAGNOSTIC MODES:\r\n    \r\n    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t&gt;0)\r\n       Focus: Why does PINN fail at long times?\r\n       Generates 6 diagnostic plots:\r\n         - Training diagnostics: Loss convergence and balance\r\n         - PDE residual evolution: When/where/which equations fail\r\n         - Conservation violations: Mass and momentum drift\r\n         - Temporal error accumulation: How errors compound over time\r\n         - Spectral bias analysis: Frequency damping and mode evolution\r\n         - Solution stability: Detecting unphysical behavior\r\n    \r\n    2. IC FITTING MODE (3D power spectrum only)\r\n       Focus: Why can't PINN fit complex initial conditions?\r\n       Generates 5 diagnostic plots:\r\n         - Training diagnostics: Loss convergence with IC component breakdown\r\n         - IC spatial comparison: Predicted vs true IC fields\r\n         - IC power spectrum: Scale-by-scale fit quality\r\n         - IC component convergence: Which component fails to converge\r\n         - IC metrics summary: Correlation and RMS error statistics\r\n    \r\n    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True\r\n    \"\"\"\r\n\r\n    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):\r\n        self.save_dir = save_dir\r\n        self.dimension = dimension\r\n        self.perturbation_type = str(perturbation_type).lower()\r\n        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')\r\n        \r\n        os.makedirs(save_dir, exist_ok=True)\r\n\r\n        # Storage for tracking metrics over iterations\r\n        self.history = {\r\n            'iteration': [],\r\n            'total_loss': [],\r\n            'pde_loss': [],\r\n            'ic_loss': [],\r\n            'mean_rho': [],\r\n            'max_rho': [],\r\n        }\r\n        \r\n        # Additional tracking for 3D power spectrum IC diagnostics\r\n        if self.is_3d_power_spectrum:\r\n            self.history.update({\r\n                'ic_rho': [],\r\n                'ic_vx': [],\r\n                'ic_vy': [],\r\n                'ic_vz': [],\r\n                'ic_phi': [],\r\n            })\r\n\r\n    def _prepare_inputs(self, geomtime_col):\r\n        \"\"\"Normalize collocation inputs to the model's expected format.\"\"\"\r\n        if isinstance(geomtime_col, (list, tuple)):\r\n            return geomtime_col\r\n        # Single tensor [N, D] -&gt; list of [N,1]\r\n        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]\r\n\r\n    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):\r\n        \"\"\"\r\n        Call this every N iterations during training.\r\n        \r\n        Args:\r\n            iteration: Current iteration number\r\n            model: PINN model\r\n            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses\r\n            geomtime_col: Collocation points\r\n            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum\r\n        \"\"\"\r\n        self.history['iteration'].append(iteration)\r\n        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))\r\n        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))\r\n        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))\r\n\r\n        # Compute diagnostics from current model state\r\n        with torch.no_grad():\r\n            inputs = self._prepare_inputs(geomtime_col)\r\n            pred = model(inputs)\r\n            # Use first channel as density/log-density proxy\r\n            rho = pred[:, 0].detach().cpu().numpy()\r\n\r\n            # Density statistics\r\n            self.history['mean_rho'].append(np.nanmean(rho))\r\n            self.history['max_rho'].append(np.nanmax(rho))\r\n        \r\n        # Track component IC losses for 3D power spectrum\r\n        if self.is_3d_power_spectrum and ic_component_losses is not None:\r\n            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))\r\n            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))\r\n            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))\r\n            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))\r\n            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))\r\n\r\n    def plot_diagnostics(self, iteration=None):\r\n        \"\"\"\r\n        Generate streamlined training diagnostic plot.\r\n        Combined loss components and balance in one figure.\r\n        \"\"\"\r\n        if len(self.history['iteration']) == 0:\r\n            print(\"No diagnostic data to plot.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        suffix = f\"_iter_{iteration}\" if iteration is not None else \"_final\"\r\n\r\n        # Combined training diagnostics plot\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        # Subplot 1: Loss evolution\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')\r\n        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')\r\n        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')\r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 2: Loss balance ratio\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ic_arr = np.array(self.history['ic_loss'])\r\n        pde_arr = np.array(self.history['pde_loss'])\r\n        ratio = pde_arr / (ic_arr + 1e-10)\r\n        ax2.plot(iters, ratio, 'purple', linewidth=2)\r\n        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')\r\n        ax2.set_xlabel('Iteration', fontsize=11)\r\n        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)\r\n        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 3: Density evolution\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')\r\n        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')\r\n        ax3.set_xlabel('Iteration', fontsize=11)\r\n        ax3.set_ylabel('Density', fontsize=11)\r\n        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n        \r\n        print(f\"Training diagnostic plot saved to {self.save_dir}\")\r\n\r\n    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================\r\n    \r\n    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n        \"\"\"\r\n        Compute PDE residuals on a regular grid over space-time.\r\n        Critical for identifying WHERE and WHEN physics breaks down.\r\n        \r\n        Note: For power spectrum cases, uses higher resolution to match IC generation grid.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum to match IC generation\r\n        if n_spatial is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_spatial = 80\r\n        \r\n        print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...\")\r\n        \r\n        if dimension == 2:\r\n            x = torch.linspace(0, 1, n_spatial, device=device)\r\n            y = torch.linspace(0, 1, n_spatial, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # Create meshgrid\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n            \r\n            # Reshape back to grid\r\n            rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n        \r\n        elif dimension == 3:\r\n            # For 3D, use coarser grid to save memory\r\n            n_spatial_3d = max(32, n_spatial // 2)\r\n            x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            z = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # For 3D, we'll slice at z=0.5 to create 2D heatmaps\r\n            z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                z_slice.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n            \r\n            rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n    \r\n    def plot_residual_heatmaps(self, residual_data, slice_idx=None):\r\n        \"\"\"\r\n        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.\r\n        Shows WHEN and WHERE each equation fails.\r\n        \"\"\"\r\n        if slice_idx is None:\r\n            slice_idx = residual_data['rho_residual'].shape[1] // 2\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        residuals = [\r\n            ('Continuity Equation', residual_data['rho_residual']),\r\n            ('Momentum X Equation', residual_data['vx_residual']),\r\n            ('Momentum Y Equation', residual_data['vy_residual']),\r\n            ('Poisson Equation', residual_data['phi_residual'])\r\n        ]\r\n        \r\n        for idx, (name, resid) in enumerate(residuals):\r\n            ax = fig.add_subplot(gs[idx // 2, idx % 2])\r\n            \r\n            # Slice at fixed y\r\n            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow\r\n            \r\n            # Use log scale for better visualization\r\n            resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n            actual_min = np.min(resid_log)\r\n            data_max = np.max(resid_log)\r\n            data_min = max(data_max - 4, -12)\r\n            \r\n            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                          extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                 residual_data['t'][0], residual_data['t'][-1]],\r\n                          vmin=data_min, vmax=data_max)\r\n            ax.set_xlabel('x', fontsize=11)\r\n            ax.set_ylabel('Time', fontsize=11)\r\n            ax.set_title(\r\n                f'{name} Residual\\nMin: 10^{actual_min:.1f}, Max: 10^{data_max:.1f}',\r\n                fontsize=12,\r\n                fontweight='bold'\r\n            )\r\n            cbar = plt.colorbar(im, ax=ax)\r\n            cbar.set_label('log₁₀|residual|', fontsize=10)\r\n        \r\n        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data[\"y\"][slice_idx]:.2f})', \r\n                    fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):\r\n        \"\"\"\r\n        Check if mass and momentum are conserved over time.\r\n        Critical for physical consistency - should be flat lines!\r\n        \r\n        Note: Uses higher resolution for power spectrum to ensure accurate integration.\r\n        \"\"\"\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum\r\n        if n_grid is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_grid = 100\r\n        \r\n        print(f\"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...\")\r\n        \r\n        conservation_data = {\r\n            'times': [],\r\n            'total_mass': [],\r\n            'total_momentum_x': [],\r\n            'total_momentum_y': []\r\n        }\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # Integrate using trapezoidal rule\r\n                dx = 1.0 / (n_grid - 1)\r\n                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, use coarser grid\r\n                n_grid_3d = max(40, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                z = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                dx = 1.0 / (n_grid_3d - 1)\r\n                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n        \r\n        return conservation_data\r\n    \r\n    def plot_conservation_laws(self, conservation_data):\r\n        \"\"\"Plot conserved quantities over time - should be flat lines!\"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = conservation_data['times']\r\n        \r\n        # Mass conservation\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        mass = np.array(conservation_data['total_mass'])\r\n        mass_initial = mass[0]\r\n        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift\r\n        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)\r\n        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Mass drift (%)', fontsize=11)\r\n        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Momentum X conservation\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        px = np.array(conservation_data['total_momentum_x'])\r\n        px_initial = px[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(px_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum X absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            px_drift = ((px - px_initial) / abs(px_initial)) * 100\r\n            ylabel = 'Momentum X drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax2.plot(times, px_drift, 'g-', linewidth=2.5)\r\n        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel(ylabel, fontsize=11)\r\n        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Momentum Y conservation\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        py = np.array(conservation_data['total_momentum_y'])\r\n        py_initial = py[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(py_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum Y absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            py_drift = ((py - py_initial) / abs(py_initial)) * 100\r\n            ylabel = 'Momentum Y drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax3.plot(times, py_drift, 'orange', linewidth=2.5)\r\n        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel(ylabel, fontsize=11)\r\n        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):\r\n        \"\"\"\r\n        Compute FFT of density field at different times.\r\n        Reveals spectral bias and high-frequency mode damping.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing spectral content at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        spectra = []\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # 2D FFT\r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                # Create wavenumber grid\r\n                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5\r\n                n_grid_3d = max(64, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n        \r\n        return spectra\r\n    \r\n    def plot_spectral_evolution(self, spectra_data):\r\n        \"\"\"\r\n        Plot power spectrum evolution over time.\r\n        Shows if high-frequency modes are being damped (spectral bias).\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        # Left: Radially averaged power spectra\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        \r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], \r\n                          linewidth=2, markersize=4, label=f\"t={spec['time']:.2f}\")\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Right: 2D power spectrum at final time\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        final_spec = spectra_data[-1]\r\n        power_log = np.log10(final_spec['power'] + 1e-12)\r\n        \r\n        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',\r\n                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),\r\n                              -np.max(final_spec['k']), np.max(final_spec['k'])])\r\n        ax2.set_xlabel('k_x', fontsize=11)\r\n        ax2.set_ylabel('k_y', fontsize=11)\r\n        ax2.set_title(f'2D Power Spectrum at t={final_spec[\"time\"]:.2f}', \r\n                     fontsize=12, fontweight='bold')\r\n        cbar = plt.colorbar(im, ax=ax2)\r\n        cbar.set_label('log₁₀(Power)', fontsize=10)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):\r\n        \"\"\"\r\n        Compute field statistics over time to track error accumulation.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing temporal statistics at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        stats = {\r\n            'times': [],\r\n            'rho_mean': [],\r\n            'rho_std': [],\r\n            'rho_max': [],\r\n            'grad_rho_mean': [],\r\n            'grad_rho_max': []\r\n        }\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients (retain_graph=True for first call since we need multiple gradients)\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5 for computational efficiency\r\n                n_spatial_3d = max(50, n_spatial // 2)\r\n                x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                Z_flat = Z.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                Z_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n        \r\n        return stats\r\n    \r\n    def plot_temporal_statistics(self, stats_data):\r\n        \"\"\"\r\n        Plot field statistics over time.\r\n        Shows if solution becomes unphysical or gradients explode.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = stats_data['times']\r\n        \r\n        # Density statistics\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax1.fill_between(times,\r\n                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),\r\n                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),\r\n                         alpha=0.3, color='b', label='±1 std')\r\n        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Gradient statistics\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)\r\n        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Growth rate analysis\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        rho_max = np.array(stats_data['rho_max'])\r\n        if len(rho_max) &gt; 5:\r\n            # Fit exponential growth\r\n            log_rho = np.log(rho_max + 1e-10)\r\n            valid_idx = np.isfinite(log_rho)\r\n            if valid_idx.sum() &gt; 5:\r\n                times_valid = np.array(times)[valid_idx]\r\n                log_rho_valid = log_rho[valid_idx]\r\n                coeffs = np.polyfit(times_valid, log_rho_valid, 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                \r\n                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, \r\n                           label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.set_xlabel('Time', fontsize=11)\r\n                ax3.set_ylabel('Max Density (log scale)', fontsize=11)\r\n                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')\r\n                ax3.legend(fontsize=10)\r\n                ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):\r\n        \"\"\"\r\n        Track how prediction errors grow and accumulate over time.\r\n        Helps identify when the solution starts diverging from physical behavior.\r\n        \r\n        Now computes ACTUAL PDE residuals and gradient norms including phi.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        \r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        error_data = {\r\n            'times': time_points,\r\n            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},\r\n            'field_ranges': {'rho': [], 'vx': [], 'vy': []},\r\n            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},\r\n            'max_residuals': []\r\n        }\r\n        \r\n        if dimension == 3:\r\n            error_data['pde_residuals']['momentum_z'] = []\r\n            error_data['field_ranges']['vz'] = []\r\n            error_data['gradient_norms']['vz'] = []\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n            else:  # dimension == 3\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                Z_flat = Z.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n            \r\n            # Compute ACTUAL PDE residuals using the proper residue function\r\n            if dimension == 2:\r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n                \r\n            else:  # 3D\r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n            \r\n            # Get predictions for field statistics\r\n            pred = model(colloc)\r\n            rho = pred[:, 0:1]\r\n            vx = pred[:, 1:2]\r\n            vy = pred[:, 2:3]\r\n            phi = pred[:, -1:]  # Last column is phi\r\n            \r\n            # Track field statistics\r\n            error_data['field_ranges']['rho'].append(torch.std(rho).item())\r\n            error_data['field_ranges']['vx'].append(torch.std(vx).item())\r\n            error_data['field_ranges']['vy'].append(torch.std(vy).item())\r\n            if dimension == 3:\r\n                vz = pred[:, 3:4]\r\n                error_data['field_ranges']['vz'].append(torch.std(vz).item())\r\n            \r\n            # Track gradient norms for ALL fields including phi\r\n            with torch.enable_grad():\r\n                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                \r\n                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())\r\n                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())\r\n                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())\r\n                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())\r\n                \r\n                if dimension == 3:\r\n                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())\r\n            \r\n            # Max residual magnitude (now including Poisson!)\r\n            max_res = max(\r\n                error_data['pde_residuals']['continuity'][-1],\r\n                error_data['pde_residuals']['momentum_x'][-1],\r\n                error_data['pde_residuals']['momentum_y'][-1],\r\n                error_data['pde_residuals']['poisson'][-1]\r\n            )\r\n            if dimension == 3:\r\n                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])\r\n            \r\n            error_data['max_residuals'].append(max_res)\r\n        \r\n        return error_data\r\n    \r\n    def plot_temporal_error_accumulation(self, error_data):\r\n        \"\"\"\r\n        Visualize how errors accumulate and grow over time.\r\n        Shows which equations fail first and error growth rates.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = error_data['times']\r\n        \r\n        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)\r\n        # IMPORTANT: Now showing actual Poisson residuals!\r\n        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)\r\n        if 'momentum_z' in error_data['pde_residuals']:\r\n            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)\r\n        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Field Range Evolution (variance growth)\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)\r\n        if 'vz' in error_data['field_ranges']:\r\n            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Field Standard Deviation', fontsize=11)\r\n        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=9)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: Maximum Residual Growth\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)\r\n        if len(times) &gt; 5:\r\n            # Fit exponential growth\r\n            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)\r\n            valid_idx = np.isfinite(log_res)\r\n            if valid_idx.sum() &gt; 3:\r\n                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.legend(fontsize=10)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Max Residual (log)', fontsize=11)\r\n        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)\r\n        if 'vz' in error_data['gradient_norms']:\r\n            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)\r\n        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax4.legend(fontsize=9, loc='best')\r\n        ax4.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 5: Cumulative Error Metric\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        cumulative_error = np.cumsum(error_data['max_residuals'])\r\n        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Cumulative Error', fontsize=11)\r\n        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3)\r\n        \r\n        # Plot 6: Error Growth Rate\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        if len(times) &gt; 1:\r\n            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)\r\n            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n            ax6.set_xlabel('Time', fontsize=11)\r\n            ax6.set_ylabel('Error Growth Rate', fontsize=11)\r\n            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')\r\n            ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_spectral_bias_analysis(self, spectra_data):\r\n        \"\"\"\r\n        Enhanced spectral analysis focused on detecting spectral bias.\r\n        Shows frequency damping and mode evolution over time.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: Radially averaged power spectra at different times\r\n        ax1 = fig.add_subplot(gs[0, :2])\r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.sum() &gt; 0:\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,\r\n                          label=f't={spec[\"time\"]:.2f}', marker='o', markersize=3)\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power P(k)', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=8, ncol=2)\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Spectral Damping Rate\r\n        ax2 = fig.add_subplot(gs[0, 2])\r\n        if len(spectra_data) &gt;= 2:\r\n            # Compare first and last spectra\r\n            spec_0 = spectra_data[0]\r\n            spec_f = spectra_data[-1]\r\n            \r\n            # Compute power ratio (damping)\r\n            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))\r\n            k_bins = np.linspace(0, k_max, 20)\r\n            damping_ratio = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask_0 = (spec_0['k'] &gt;= k_bins[i]) &amp; (spec_0['k'] &lt; k_bins[i+1])\r\n                mask_f = (spec_f['k'] &gt;= k_bins[i]) &amp; (spec_f['k'] &lt; k_bins[i+1])\r\n                if mask_0.sum() &gt; 0 and mask_f.sum() &gt; 0:\r\n                    p0 = np.mean(spec_0['power'][mask_0])\r\n                    pf = np.mean(spec_f['power'][mask_f])\r\n                    if p0 &gt; 0:\r\n                        damping_ratio.append(pf / p0)\r\n                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)\r\n                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')\r\n                ax2.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)\r\n                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')\r\n                ax2.legend(fontsize=10)\r\n                ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: High-k Mode Tracking\r\n        ax3 = fig.add_subplot(gs[1, 0])\r\n        times = [spec['time'] for spec in spectra_data]\r\n        \r\n        # Track power in different k ranges\r\n        low_k_power = []\r\n        mid_k_power = []\r\n        high_k_power = []\r\n        \r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            k_max = np.max(k_flat)\r\n            low_k_power.append(np.mean(p_flat[k_flat &lt; k_max/3]))\r\n            mid_k_power.append(np.mean(p_flat[(k_flat &gt;= k_max/3) &amp; (k_flat &lt; 2*k_max/3)]))\r\n            high_k_power.append(np.mean(p_flat[k_flat &gt;= 2*k_max/3]))\r\n        \r\n        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)\r\n        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)\r\n        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Average Power (log)', fontsize=11)\r\n        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Spectral Index Evolution\r\n        ax4 = fig.add_subplot(gs[1, 1])\r\n        spectral_indices = []\r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            # Fit power law in log space\r\n            valid = (k_flat &gt; 0) &amp; (p_flat &gt; 0)\r\n            if valid.sum() &gt; 10:\r\n                log_k = np.log(k_flat[valid])\r\n                log_p = np.log(p_flat[valid])\r\n                coeffs = np.polyfit(log_k, log_p, 1)\r\n                spectral_indices.append(coeffs[0])  # Slope = spectral index\r\n            else:\r\n                spectral_indices.append(np.nan)\r\n        \r\n        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)\r\n        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)\r\n        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Density field snapshots\r\n        ax5 = fig.add_subplot(gs[1, 2])\r\n        # Show first, middle, last density fields\r\n        n_show = min(3, len(spectra_data))\r\n        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]\r\n        \r\n        for i, idx in enumerate(indices[:n_show]):\r\n            rho = spectra_data[idx]['rho']\r\n            # Compute structure function or variance\r\n            rho_var = np.var(rho)\r\n            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)\r\n            ax5.text(i, rho_var, f't={spectra_data[idx][\"time\"]:.2f}', \r\n                    ha='center', va='bottom', fontsize=9)\r\n        \r\n        ax5.set_xlabel('Time Snapshot', fontsize=11)\r\n        ax5.set_ylabel('Density Variance', fontsize=11)\r\n        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')\r\n        ax5.set_xticks(range(n_show))\r\n        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])\r\n        ax5.grid(True, alpha=0.3, axis='y')\r\n        \r\n        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):\r\n        \"\"\"\r\n        Check for signs of solution instability or unphysical behavior.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_check)\r\n        \r\n        stability_data = {\r\n            'times': time_points,\r\n            'density_bounds': {'min': [], 'max': [], 'mean': []},\r\n            'velocity_norms': [],\r\n            'negative_density_fraction': [],\r\n            'extreme_values': [],\r\n            'solution_smoothness': []\r\n        }\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n            else:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n            \r\n            with torch.no_grad():\r\n                pred = model(colloc)\r\n                rho = pred[:, 0].cpu().numpy()\r\n                vx = pred[:, 1].cpu().numpy()\r\n                vy = pred[:, 2].cpu().numpy()\r\n                \r\n                # Density bounds\r\n                stability_data['density_bounds']['min'].append(np.min(rho))\r\n                stability_data['density_bounds']['max'].append(np.max(rho))\r\n                stability_data['density_bounds']['mean'].append(np.mean(rho))\r\n                \r\n                # Velocity norms\r\n                v_norm = np.sqrt(vx**2 + vy**2)\r\n                if dimension == 3:\r\n                    vz = pred[:, 3].cpu().numpy()\r\n                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)\r\n                stability_data['velocity_norms'].append(np.max(v_norm))\r\n                \r\n                # Negative density fraction\r\n                neg_frac = np.sum(rho &lt; 0) / len(rho)\r\n                stability_data['negative_density_fraction'].append(neg_frac)\r\n                \r\n                # Extreme value detection (more than 10x mean)\r\n                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) &gt; 10 * np.std(rho)) / len(rho)\r\n                stability_data['extreme_values'].append(extreme_frac)\r\n                \r\n                # Solution smoothness (Laplacian magnitude as proxy)\r\n                if dimension == 2:\r\n                    rho_2d = rho.reshape(n_spatial, n_spatial)\r\n                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + \r\n                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))\r\n                    stability_data['solution_smoothness'].append(np.mean(laplacian))\r\n                else:\r\n                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D\r\n        \r\n        return stability_data\r\n    \r\n    def plot_solution_stability(self, stability_data):\r\n        \"\"\"\r\n        Visualize solution stability metrics to detect unphysical behavior.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = stability_data['times']\r\n        \r\n        # Plot 1: Density Bounds\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)\r\n        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Plot 2: Velocity Norms\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)\r\n        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 3: Negative Density Fraction\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)\r\n        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Negative Density (%)', fontsize=11)\r\n        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        # Plot 4: Extreme Values\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)\r\n        ax4.set_title('Outlier Detection (&gt;10σ)', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Solution Smoothness\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)\r\n        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 6: Stability Summary\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        # Compute overall stability score (0-1, higher is worse)\r\n        neg_dens_score = np.array(stability_data['negative_density_fraction'])\r\n        extreme_score = np.array(stability_data['extreme_values'])\r\n        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score\r\n        \r\n        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)\r\n        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')\r\n        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')\r\n        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')\r\n        ax6.set_xlabel('Time', fontsize=11)\r\n        ax6.set_ylabel('Instability Score (%)', fontsize=11)\r\n        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')\r\n        ax6.legend(fontsize=9)\r\n        ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_comprehensive_diagnostics(self, model, dimension, tmax):\r\n        \"\"\"\r\n        Run all post-training diagnostics for long-term evolution analysis.\r\n        \r\n        Generates 6 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail\r\n        3. Conservation law violations - Mass/momentum drift over time\r\n        4. Temporal error accumulation - How errors compound over time\r\n        5. Spectral bias analysis - Frequency damping and mode evolution\r\n        6. Solution stability - Detecting unphysical behavior\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            dimension: Spatial dimension (2 or 3)\r\n            tmax: Maximum time to analyze\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Long-Term Evolution Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: PDE Residual Evolution\r\n        print(\"\\n[1/5] Computing PDE residual evolution...\")\r\n        residuals = self.compute_residual_heatmap(model, dimension, tmax)\r\n        self.plot_residual_heatmaps(residuals)\r\n        print(\"      [OK] PDE residual evolution saved\")\r\n        \r\n        # Plot 3: Conservation Laws\r\n        print(\"\\n[2/5] Analyzing conservation law violations...\")\r\n        conservation = self.check_conservation_laws(model, dimension, tmax)\r\n        self.plot_conservation_laws(conservation)\r\n        print(\"      [OK] Conservation violations plot saved\")\r\n        \r\n        # Plot 4: Temporal Error Accumulation\r\n        print(\"\\n[3/5] Computing temporal error accumulation...\")\r\n        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)\r\n        self.plot_temporal_error_accumulation(error_data)\r\n        print(\"      [OK] Error accumulation plot saved\")\r\n        \r\n        # Plot 5: Spectral Bias Analysis\r\n        print(\"\\n[4/5] Analyzing spectral bias and mode damping...\")\r\n        spectra = self.compute_spectral_content(model, dimension, tmax)\r\n        self.plot_spectral_bias_analysis(spectra)\r\n        print(\"      [OK] Spectral bias analysis saved\")\r\n        \r\n        # Plot 6: Solution Stability\r\n        print(\"\\n[5/5] Checking solution stability...\")\r\n        stats = self.compute_temporal_statistics(model, dimension, tmax)\r\n        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)\r\n        self.plot_solution_stability(stability_data)\r\n        print(\"      [OK] Solution stability diagnostics saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nLong-Term Evolution Diagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")\r\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns\")\r\n        print(\"  3. conservation_laws.png - Conservation law drift over time\")\r\n        print(\"  4. temporal_error_accumulation.png - Error growth and compounding\")\r\n        print(\"  5. spectral_bias_analysis.png - Frequency damping and spectral issues\")\r\n        print(\"  6. solution_stability.png - Physical validity and stability metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n    \r\n    \r\n    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):\r\n        \"\"\"\r\n        Smart dispatcher: runs appropriate diagnostics based on case type.\r\n        \r\n        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics\r\n        - 3D power spectrum: IC fitting diagnostics  \r\n        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)\r\n            true_ic_data: Dict with IC data (required for 3D power spectrum)\r\n                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        if self.is_3d_power_spectrum:\r\n            # 3D power spectrum: Focus on IC fitting\r\n            if true_ic_data is None:\r\n                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")\r\n            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)\r\n        else:\r\n            # All other cases: Focus on temporal evolution\r\n            if tmax is None:\r\n                raise ValueError(\"tmax required for temporal evolution diagnostics\")\r\n            self.run_comprehensive_diagnostics(model, self.dimension, tmax)\r\n    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================\r\n    \r\n    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):\r\n        \"\"\"\r\n        Compare predicted vs true ICs spatially for both 2D and 3D cases.\r\n        Critical for diagnosing WHERE the network fails to fit the IC structure.\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC collocation points and true values\r\n                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}\r\n                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            n_grid: Grid resolution per dimension (for visualization grid)\r\n        \r\n        Returns:\r\n            Dict with predicted and true fields, plus errors\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing IC spatial comparison on training IC points...\")\r\n        \r\n        # Use the actual IC collocation points from training\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions at IC points\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n                phi_pred = pred[:, 4].cpu().numpy()\r\n            else:\r\n                phi_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # For visualization, create a regular grid\r\n        x = torch.linspace(0, 1, n_grid, device=device)\r\n        y = torch.linspace(0, 1, n_grid, device=device)\r\n        X, Y = torch.meshgrid(x, y, indexing='ij')\r\n        T = torch.zeros_like(X)\r\n        \r\n        if self.dimension == 3:\r\n            # For 3D, take z=0.5 slice\r\n            Z = torch.full_like(X, 0.5)\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n        else:\r\n            # For 2D\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n        \r\n        # Get predictions on visualization grid\r\n        with torch.no_grad():\r\n            pred_viz = model(colloc_viz)\r\n            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()\r\n            else:\r\n                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n        \r\n        # For \"true\" visualization, we need to interpolate from IC points to regular grid\r\n        # Extract spatial coordinates from IC collocation points (detach first)\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Find points near z=0.5 for visualization\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        if mask_z.sum() &gt; 100:  # Need enough points for interpolation\r\n            from scipy.interpolate import griddata\r\n            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n            X_viz = X.cpu().numpy()\r\n            Y_viz = Y.cpu().numpy()\r\n            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])\r\n            \r\n            # Interpolate with nearest neighbor fallback for NaN values\r\n            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            \r\n            # For uniform density (all values the same), fill NaNs with the constant value\r\n            # For varying fields, use nearest neighbor interpolation for NaN regions\r\n            if np.isnan(rho_true_viz).any():\r\n                if np.std(rho_true[mask_z]) &lt; 1e-10:\r\n                    # Uniform field - fill with mean\r\n                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))\r\n                else:\r\n                    # Varying field - use nearest neighbor for gaps\r\n                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)\r\n            \r\n            if np.isnan(vx_true_viz).any():\r\n                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)\r\n            \r\n            if np.isnan(vy_true_viz).any():\r\n                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)\r\n            \r\n            if self.dimension == 3:\r\n                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n                if np.isnan(vz_true_viz).any():\r\n                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)\r\n        else:\r\n            # Not enough points for interpolation, use predicted as reference\r\n            print(\"  [WARN] Not enough IC points for ground truth visualization\")\r\n            rho_true_viz = rho_pred_viz\r\n            vx_true_viz = vx_pred_viz\r\n            vy_true_viz = vy_pred_viz\r\n            if self.dimension == 3:\r\n                vz_true_viz = vz_pred_viz\r\n        \r\n        result = {\r\n            'x': x.cpu().numpy(),\r\n            'y': y.cpu().numpy(),\r\n            'rho_pred': rho_pred_viz,\r\n            'rho_true': rho_true_viz,\r\n            'vx_pred': vx_pred_viz,\r\n            'vx_true': vx_true_viz,\r\n            'vy_pred': vy_pred_viz,\r\n            'vy_true': vy_true_viz,\r\n            'phi_pred': phi_pred_viz,\r\n            # Also store scatter point data for metrics\r\n            'rho_pred_scatter': rho_pred,\r\n            'rho_true_scatter': rho_true,\r\n            'vx_pred_scatter': vx_pred,\r\n            'vx_true_scatter': vx_true,\r\n            'vy_pred_scatter': vy_pred,\r\n            'vy_true_scatter': vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            result.update({\r\n                'vz_pred': vz_pred_viz,\r\n                'vz_true': vz_true_viz,\r\n                'vz_pred_scatter': vz_pred,\r\n                'vz_true_scatter': vz_true,\r\n            })\r\n        \r\n        return result\r\n    \r\n    def plot_ic_power_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot power spectrum comparison for all velocity components.\r\n        Shows which scales (k-modes) are missing or damped.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_ic_component_convergence(self):\r\n        \"\"\"\r\n        Plot IC component loss convergence over training.\r\n        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.\r\n        \"\"\"\r\n        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:\r\n            print(\"IC component tracking not available.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        \r\n        # Replace zeros and very small values with a minimum threshold for log plotting\r\n        def safe_log_data(data):\r\n            \"\"\"Replace zeros/negative values with small epsilon for log plotting.\"\"\"\r\n            data_array = np.array(data)\r\n            data_array = np.where(data_array &lt;= 0, 1e-12, data_array)\r\n            return data_array\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: All IC components on same plot\r\n        ax1 = fig.add_subplot(gs[0, :])\r\n        \r\n        # Check if rho is zero/constant (uniform field case)\r\n        rho_is_uniform = np.max(self.history['ic_rho']) &lt; 1e-10\r\n        \r\n        if not rho_is_uniform:\r\n            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', \r\n                        marker='o', markersize=3, markevery=max(1, len(iters)//20))\r\n        else:\r\n            # Plot a dashed line at the bottom to indicate uniform/zero loss\r\n            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)\r\n        \r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', \r\n                    marker='s', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', \r\n                    marker='^', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', \r\n                    marker='d', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', \r\n                    marker='v', markersize=3, markevery=max(1, len(iters)//20))\r\n        \r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10, ncol=5)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Individual component plots\r\n        components = [\r\n            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),\r\n            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),\r\n            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),\r\n        ]\r\n        \r\n        for hist_key, title, grid_pos, is_uniform in components:\r\n            ax = fig.add_subplot(grid_pos)\r\n            if is_uniform:\r\n                # Show a flat line at bottom with annotation\r\n                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)\r\n                ax.text(0.5, 0.5, 'Uniform IC\\n(no spatial variation)', \r\n                       transform=ax.transAxes, ha='center', va='center',\r\n                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\r\n                ax.set_ylim([1e-13, 1e-10])\r\n            else:\r\n                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')\r\n            \r\n            ax.set_xlabel('Iteration', fontsize=10)\r\n            ax.set_ylabel('Loss (log scale)', fontsize=10)\r\n            ax.set_title(title, fontsize=11, fontweight='bold')\r\n            ax.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_ic_correlation_metrics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Compute spatial correlation between predicted and true ICs.\r\n        High correlation = good spatial structure capture.\r\n        \"\"\"\r\n        print(f\"  Computing IC correlation metrics on training IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy().flatten()\r\n            vx_pred = pred[:, 1].cpu().numpy().flatten()\r\n            vy_pred = pred[:, 2].cpu().numpy().flatten()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy().flatten()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()\r\n        \r\n        # Compute correlations - handle uniform fields (zero variance)\r\n        def safe_corrcoef(pred, true):\r\n            \"\"\"Compute correlation, handling constant fields gracefully.\"\"\"\r\n            # Check if either field is constant\r\n            if np.std(pred) &lt; 1e-10 or np.std(true) &lt; 1e-10:\r\n                # For uniform fields, correlation is undefined\r\n                # If both are uniform and equal, perfect match (1.0)\r\n                # If different constants, poor match (use normalized error)\r\n                if np.std(pred) &lt; 1e-10 and np.std(true) &lt; 1e-10:\r\n                    # Both uniform - check if they match\r\n                    mean_diff = abs(np.mean(pred) - np.mean(true))\r\n                    return 1.0 if mean_diff &lt; 1e-6 else 0.0\r\n                else:\r\n                    # One uniform, one not - cannot capture structure\r\n                    return 0.0\r\n            else:\r\n                return np.corrcoef(pred, true)[0, 1]\r\n        \r\n        correlations = {\r\n            'rho': safe_corrcoef(rho_pred, rho_true),\r\n            'vx': safe_corrcoef(vx_pred, vx_true),\r\n            'vy': safe_corrcoef(vy_pred, vy_true),\r\n        }\r\n        \r\n        # Compute RMS errors\r\n        rms_errors = {\r\n            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),\r\n            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),\r\n            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)\r\n            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))\r\n        \r\n        return correlations, rms_errors\r\n    \r\n    def plot_ic_metrics_summary(self, correlations, rms_errors):\r\n        \"\"\"\r\n        Summary plot: correlation and RMS error for each IC component.\r\n        Quick visual diagnostic of IC fitting quality.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        components = ['rho', 'vx', 'vy', 'vz']\r\n        labels = ['ρ', 'vx', 'vy', 'vz']\r\n        \r\n        # Correlation plot\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        corr_vals = [correlations[c] for c in components]\r\n        \r\n        # Handle NaN/inf correlations from uniform fields\r\n        corr_vals_safe = []\r\n        for val in corr_vals:\r\n            if np.isnan(val) or np.isinf(val):\r\n                corr_vals_safe.append(0.0)  # Show as 0 for visualization\r\n            else:\r\n                corr_vals_safe.append(val)\r\n        \r\n        colors = ['green' if c &gt; 0.9 else 'orange' if c &gt; 0.7 else 'red' for c in corr_vals_safe]\r\n        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')\r\n        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')\r\n        ax1.set_ylabel('Correlation Coefficient', fontsize=11)\r\n        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax1.set_ylim([0, 1.05])\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):\r\n            height = bar.get_height()\r\n            if np.isnan(orig_val) or np.isinf(orig_val):\r\n                label_text = 'N/A\\n(uniform)'\r\n                fontsize = 8\r\n            else:\r\n                label_text = f'{val:.3f}'\r\n                fontsize = 10\r\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\r\n                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')\r\n        \r\n        # RMS Error plot\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        rms_vals = [rms_errors[c] for c in components]\r\n        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax2.set_ylabel('RMS Error', fontsize=11)\r\n        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val in zip(bars2, rms_vals):\r\n            height = bar.get_height()\r\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\r\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\r\n        \r\n        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Run diagnostics for 3D power spectrum case.\r\n        Focus on IC fitting quality, not temporal evolution.\r\n        \r\n        Generates 5 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum comparison (scale-by-scale)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running 3D Power Spectrum IC Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[1/4] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power Spectrum Comparison\r\n        print(\"\\n[2/4] Computing IC power spectrum comparison...\")\r\n        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n        self.plot_ic_power_spectrum_comparison(ps_data)\r\n        print(\"      [OK] IC power spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[3/4] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[4/4] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit\")\r\n        print(\"  4. ic_component_convergence.png - Which IC component fails\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        for comp in ['rho', 'vx', 'vy', 'vz']:\r\n            print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================\r\n    \r\n    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):\r\n        \"\"\"\r\n        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).\r\n        \r\n        The 5 plots are:\r\n        1. Training diagnostics (loss convergence and balance)\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            final_iteration: Final iteration number\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Unified Training Diagnostics (5 plots)\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 1: Training diagnostics\r\n        print(\"\\n[1/5] Plotting training diagnostics...\")\r\n        self.plot_diagnostics(final_iteration)\r\n        print(\"      [OK] Training diagnostics saved\")\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[2/5] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power/Field Spectrum Comparison\r\n        print(\"\\n[3/5] Computing spectrum comparison...\")\r\n        if self.perturbation_type == 'power_spectrum':\r\n            # For power spectrum: compare power spectra\r\n            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n            self.plot_ic_power_spectrum_comparison(ps_data)\r\n        else:\r\n            # For sinusoidal: compute field spectra from predictions\r\n            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)\r\n            self.plot_field_spectrum_comparison(ps_data)\r\n        print(\"      [OK] Spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[4/5] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[5/5] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Spectrum comparison\")\r\n        print(\"  4. ic_component_convergence.png - IC component convergence\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']\r\n        for comp in comps:\r\n            if comp in correlations:\r\n                print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):\r\n        \"\"\"\r\n        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.\r\n        Similar to power spectrum comparison but computed from actual fields.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing field spectrum comparison on IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # Need to interpolate to regular grid for FFT\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Use points near z=0.5 for 2D slice\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n            if mask_z.sum() &lt; 100:\r\n                mask_z = np.ones(len(z_ic), dtype=bool)\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        # Create regular grid for FFT\r\n        x_grid = np.linspace(0, 1, n_grid)\r\n        y_grid = np.linspace(0, 1, n_grid)\r\n        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')\r\n        \r\n        # Interpolate from scattered IC points to regular grid\r\n        from scipy.interpolate import griddata\r\n        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])\r\n        \r\n        # Interpolate density\r\n        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Interpolate velocities\r\n        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        if self.dimension == 3:\r\n            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Compute power spectra\r\n        def compute_power_spectrum(field):\r\n            fft = np.fft.fft2(field)\r\n            power = np.abs(np.fft.fftshift(fft))**2\r\n            \r\n            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            kx_shift = np.fft.fftshift(kx)\r\n            ky_shift = np.fft.fftshift(ky)\r\n            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n            K = np.sqrt(KX**2 + KY**2)\r\n            \r\n            return power, K\r\n        \r\n        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)\r\n        power_rho_true, _ = compute_power_spectrum(rho_true_grid)\r\n        \r\n        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)\r\n        power_vx_true, _ = compute_power_spectrum(vx_true_grid)\r\n        \r\n        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)\r\n        power_vy_true, _ = compute_power_spectrum(vy_true_grid)\r\n        \r\n        result = {\r\n            'K': K,\r\n            'power_rho_pred': power_rho_pred,\r\n            'power_rho_true': power_rho_true,\r\n            'power_vx_pred': power_vx_pred,\r\n            'power_vx_true': power_vx_true,\r\n            'power_vy_pred': power_vy_pred,\r\n            'power_vy_true': power_vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)\r\n            power_vz_true, _ = compute_power_spectrum(vz_true_grid)\r\n            result['power_vz_pred'] = power_vz_pred\r\n            result['power_vz_true'] = power_vz_true\r\n        \r\n        return result\r\n    \r\n    def plot_field_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot field spectrum comparison for sinusoidal cases.\r\n        Shows density and velocity spectra.\r\n        \"\"\"\r\n        if self.dimension == 3:\r\n            fig = plt.figure(figsize=(18, 5))\r\n            gs = GridSpec(1, 4, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        else:\r\n            fig = plt.figure(figsize=(14, 5))\r\n            gs = GridSpec(1, 3, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "              # Use log scale for better visualization\r\n              resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n-             data_min = np.min(resid_log)\r\n+             actual_min = np.min(resid_log)\r\n              data_max = np.max(resid_log)\r\n+             data_min = max(data_max - 4, -12)\r\n              \r\n              im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r",
        "oldStart": 275,
        "newStart": 275,
        "oldLines": 6,
        "newLines": 7,
        "linesRemoved": 1,
        "linesAdded": 2
      },
      {
        "diffString": "              ax.set_ylabel('Time', fontsize=11)\r\n              ax.set_title(\r\n-                 f'{name} Residual\\nMin: 10^{data_min:.1f}, Max: 10^{data_max:.1f}',\r\n+                 f'{name} Residual\\nMin: 10^{actual_min:.1f}, Max: 10^{data_max:.1f}',\r\n                  fontsize=12,\r\n                  fontweight='bold'\r",
        "oldStart": 285,
        "newStart": 286,
        "oldLines": 5,
        "newLines": 5,
        "linesRemoved": 1,
        "linesAdded": 1
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM training/training_diagnostics.py",
  "contentsAfterEdit": "import os\r\nimport numpy as np\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\n\r\nclass TrainingDiagnostics:\r\n    \"\"\"\r\n    Comprehensive diagnostics for PINN performance analysis.\r\n    \r\n    TWO DIAGNOSTIC MODES:\r\n    \r\n    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t&gt;0)\r\n       Focus: Why does PINN fail at long times?\r\n       Generates 6 diagnostic plots:\r\n         - Training diagnostics: Loss convergence and balance\r\n         - PDE residual evolution: When/where/which equations fail\r\n         - Conservation violations: Mass and momentum drift\r\n         - Temporal error accumulation: How errors compound over time\r\n         - Spectral bias analysis: Frequency damping and mode evolution\r\n         - Solution stability: Detecting unphysical behavior\r\n    \r\n    2. IC FITTING MODE (3D power spectrum only)\r\n       Focus: Why can't PINN fit complex initial conditions?\r\n       Generates 5 diagnostic plots:\r\n         - Training diagnostics: Loss convergence with IC component breakdown\r\n         - IC spatial comparison: Predicted vs true IC fields\r\n         - IC power spectrum: Scale-by-scale fit quality\r\n         - IC component convergence: Which component fails to converge\r\n         - IC metrics summary: Correlation and RMS error statistics\r\n    \r\n    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True\r\n    \"\"\"\r\n\r\n    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):\r\n        self.save_dir = save_dir\r\n        self.dimension = dimension\r\n        self.perturbation_type = str(perturbation_type).lower()\r\n        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')\r\n        \r\n        os.makedirs(save_dir, exist_ok=True)\r\n\r\n        # Storage for tracking metrics over iterations\r\n        self.history = {\r\n            'iteration': [],\r\n            'total_loss': [],\r\n            'pde_loss': [],\r\n            'ic_loss': [],\r\n            'mean_rho': [],\r\n            'max_rho': [],\r\n        }\r\n        \r\n        # Additional tracking for 3D power spectrum IC diagnostics\r\n        if self.is_3d_power_spectrum:\r\n            self.history.update({\r\n                'ic_rho': [],\r\n                'ic_vx': [],\r\n                'ic_vy': [],\r\n                'ic_vz': [],\r\n                'ic_phi': [],\r\n            })\r\n\r\n    def _prepare_inputs(self, geomtime_col):\r\n        \"\"\"Normalize collocation inputs to the model's expected format.\"\"\"\r\n        if isinstance(geomtime_col, (list, tuple)):\r\n            return geomtime_col\r\n        # Single tensor [N, D] -&gt; list of [N,1]\r\n        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]\r\n\r\n    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):\r\n        \"\"\"\r\n        Call this every N iterations during training.\r\n        \r\n        Args:\r\n            iteration: Current iteration number\r\n            model: PINN model\r\n            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses\r\n            geomtime_col: Collocation points\r\n            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum\r\n        \"\"\"\r\n        self.history['iteration'].append(iteration)\r\n        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))\r\n        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))\r\n        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))\r\n\r\n        # Compute diagnostics from current model state\r\n        with torch.no_grad():\r\n            inputs = self._prepare_inputs(geomtime_col)\r\n            pred = model(inputs)\r\n            # Use first channel as density/log-density proxy\r\n            rho = pred[:, 0].detach().cpu().numpy()\r\n\r\n            # Density statistics\r\n            self.history['mean_rho'].append(np.nanmean(rho))\r\n            self.history['max_rho'].append(np.nanmax(rho))\r\n        \r\n        # Track component IC losses for 3D power spectrum\r\n        if self.is_3d_power_spectrum and ic_component_losses is not None:\r\n            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))\r\n            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))\r\n            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))\r\n            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))\r\n            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))\r\n\r\n    def plot_diagnostics(self, iteration=None):\r\n        \"\"\"\r\n        Generate streamlined training diagnostic plot.\r\n        Combined loss components and balance in one figure.\r\n        \"\"\"\r\n        if len(self.history['iteration']) == 0:\r\n            print(\"No diagnostic data to plot.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        suffix = f\"_iter_{iteration}\" if iteration is not None else \"_final\"\r\n\r\n        # Combined training diagnostics plot\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        # Subplot 1: Loss evolution\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')\r\n        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')\r\n        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')\r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 2: Loss balance ratio\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ic_arr = np.array(self.history['ic_loss'])\r\n        pde_arr = np.array(self.history['pde_loss'])\r\n        ratio = pde_arr / (ic_arr + 1e-10)\r\n        ax2.plot(iters, ratio, 'purple', linewidth=2)\r\n        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')\r\n        ax2.set_xlabel('Iteration', fontsize=11)\r\n        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)\r\n        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 3: Density evolution\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')\r\n        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')\r\n        ax3.set_xlabel('Iteration', fontsize=11)\r\n        ax3.set_ylabel('Density', fontsize=11)\r\n        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n        \r\n        print(f\"Training diagnostic plot saved to {self.save_dir}\")\r\n\r\n    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================\r\n    \r\n    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n        \"\"\"\r\n        Compute PDE residuals on a regular grid over space-time.\r\n        Critical for identifying WHERE and WHEN physics breaks down.\r\n        \r\n        Note: For power spectrum cases, uses higher resolution to match IC generation grid.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum to match IC generation\r\n        if n_spatial is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_spatial = 80\r\n        \r\n        print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...\")\r\n        \r\n        if dimension == 2:\r\n            x = torch.linspace(0, 1, n_spatial, device=device)\r\n            y = torch.linspace(0, 1, n_spatial, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # Create meshgrid\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n            \r\n            # Reshape back to grid\r\n            rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n        \r\n        elif dimension == 3:\r\n            # For 3D, use coarser grid to save memory\r\n            n_spatial_3d = max(32, n_spatial // 2)\r\n            x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            z = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            # For 3D, we'll slice at z=0.5 to create 2D heatmaps\r\n            z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            colloc = [\r\n                X.reshape(-1, 1).requires_grad_(True),\r\n                Y.reshape(-1, 1).requires_grad_(True),\r\n                z_slice.reshape(-1, 1).requires_grad_(True),\r\n                T.reshape(-1, 1).requires_grad_(True)\r\n            ]\r\n            \r\n            rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n            \r\n            rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_r,\r\n                'vx_residual': vx_r,\r\n                'vy_residual': vy_r,\r\n                'phi_residual': phi_r\r\n            }\r\n    \r\n    def plot_residual_heatmaps(self, residual_data, slice_idx=None):\r\n        \"\"\"\r\n        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.\r\n        Shows WHEN and WHERE each equation fails.\r\n        \"\"\"\r\n        if slice_idx is None:\r\n            slice_idx = residual_data['rho_residual'].shape[1] // 2\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        residuals = [\r\n            ('Continuity Equation', residual_data['rho_residual']),\r\n            ('Momentum X Equation', residual_data['vx_residual']),\r\n            ('Momentum Y Equation', residual_data['vy_residual']),\r\n            ('Poisson Equation', residual_data['phi_residual'])\r\n        ]\r\n        \r\n        for idx, (name, resid) in enumerate(residuals):\r\n            ax = fig.add_subplot(gs[idx // 2, idx % 2])\r\n            \r\n            # Slice at fixed y\r\n            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow\r\n            \r\n            # Use log scale for better visualization\r\n            resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n            actual_min = np.min(resid_log)\r\n            data_max = np.max(resid_log)\r\n            data_min = max(data_max - 4, -12)\r\n            \r\n            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                          extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                 residual_data['t'][0], residual_data['t'][-1]],\r\n                          vmin=data_min, vmax=data_max)\r\n            ax.set_xlabel('x', fontsize=11)\r\n            ax.set_ylabel('Time', fontsize=11)\r\n            ax.set_title(\r\n                f'{name} Residual\\nMin: 10^{actual_min:.1f}, Max: 10^{data_max:.1f}',\r\n                fontsize=12,\r\n                fontweight='bold'\r\n            )\r\n            cbar = plt.colorbar(im, ax=ax)\r\n            cbar.set_label('log₁₀|residual|', fontsize=10)\r\n        \r\n        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data[\"y\"][slice_idx]:.2f})', \r\n                    fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):\r\n        \"\"\"\r\n        Check if mass and momentum are conserved over time.\r\n        Critical for physical consistency - should be flat lines!\r\n        \r\n        Note: Uses higher resolution for power spectrum to ensure accurate integration.\r\n        \"\"\"\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum\r\n        if n_grid is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_grid = 100\r\n        \r\n        print(f\"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...\")\r\n        \r\n        conservation_data = {\r\n            'times': [],\r\n            'total_mass': [],\r\n            'total_momentum_x': [],\r\n            'total_momentum_y': []\r\n        }\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # Integrate using trapezoidal rule\r\n                dx = 1.0 / (n_grid - 1)\r\n                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, use coarser grid\r\n                n_grid_3d = max(40, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                z = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                dx = 1.0 / (n_grid_3d - 1)\r\n                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n        \r\n        return conservation_data\r\n    \r\n    def plot_conservation_laws(self, conservation_data):\r\n        \"\"\"Plot conserved quantities over time - should be flat lines!\"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = conservation_data['times']\r\n        \r\n        # Mass conservation\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        mass = np.array(conservation_data['total_mass'])\r\n        mass_initial = mass[0]\r\n        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift\r\n        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)\r\n        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Mass drift (%)', fontsize=11)\r\n        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Momentum X conservation\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        px = np.array(conservation_data['total_momentum_x'])\r\n        px_initial = px[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(px_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum X absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            px_drift = ((px - px_initial) / abs(px_initial)) * 100\r\n            ylabel = 'Momentum X drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax2.plot(times, px_drift, 'g-', linewidth=2.5)\r\n        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel(ylabel, fontsize=11)\r\n        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Momentum Y conservation\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        py = np.array(conservation_data['total_momentum_y'])\r\n        py_initial = py[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(py_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum Y absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            py_drift = ((py - py_initial) / abs(py_initial)) * 100\r\n            ylabel = 'Momentum Y drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax3.plot(times, py_drift, 'orange', linewidth=2.5)\r\n        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel(ylabel, fontsize=11)\r\n        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):\r\n        \"\"\"\r\n        Compute FFT of density field at different times.\r\n        Reveals spectral bias and high-frequency mode damping.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing spectral content at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        spectra = []\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # 2D FFT\r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                # Create wavenumber grid\r\n                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5\r\n                n_grid_3d = max(64, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n        \r\n        return spectra\r\n    \r\n    def plot_spectral_evolution(self, spectra_data):\r\n        \"\"\"\r\n        Plot power spectrum evolution over time.\r\n        Shows if high-frequency modes are being damped (spectral bias).\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        # Left: Radially averaged power spectra\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        \r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], \r\n                          linewidth=2, markersize=4, label=f\"t={spec['time']:.2f}\")\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Right: 2D power spectrum at final time\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        final_spec = spectra_data[-1]\r\n        power_log = np.log10(final_spec['power'] + 1e-12)\r\n        \r\n        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',\r\n                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),\r\n                              -np.max(final_spec['k']), np.max(final_spec['k'])])\r\n        ax2.set_xlabel('k_x', fontsize=11)\r\n        ax2.set_ylabel('k_y', fontsize=11)\r\n        ax2.set_title(f'2D Power Spectrum at t={final_spec[\"time\"]:.2f}', \r\n                     fontsize=12, fontweight='bold')\r\n        cbar = plt.colorbar(im, ax=ax2)\r\n        cbar.set_label('log₁₀(Power)', fontsize=10)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):\r\n        \"\"\"\r\n        Compute field statistics over time to track error accumulation.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing temporal statistics at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        stats = {\r\n            'times': [],\r\n            'rho_mean': [],\r\n            'rho_std': [],\r\n            'rho_max': [],\r\n            'grad_rho_mean': [],\r\n            'grad_rho_max': []\r\n        }\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients (retain_graph=True for first call since we need multiple gradients)\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5 for computational efficiency\r\n                n_spatial_3d = max(50, n_spatial // 2)\r\n                x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                Z_flat = Z.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                Z_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n        \r\n        return stats\r\n    \r\n    def plot_temporal_statistics(self, stats_data):\r\n        \"\"\"\r\n        Plot field statistics over time.\r\n        Shows if solution becomes unphysical or gradients explode.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = stats_data['times']\r\n        \r\n        # Density statistics\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax1.fill_between(times,\r\n                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),\r\n                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),\r\n                         alpha=0.3, color='b', label='±1 std')\r\n        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Gradient statistics\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)\r\n        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Growth rate analysis\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        rho_max = np.array(stats_data['rho_max'])\r\n        if len(rho_max) &gt; 5:\r\n            # Fit exponential growth\r\n            log_rho = np.log(rho_max + 1e-10)\r\n            valid_idx = np.isfinite(log_rho)\r\n            if valid_idx.sum() &gt; 5:\r\n                times_valid = np.array(times)[valid_idx]\r\n                log_rho_valid = log_rho[valid_idx]\r\n                coeffs = np.polyfit(times_valid, log_rho_valid, 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                \r\n                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, \r\n                           label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.set_xlabel('Time', fontsize=11)\r\n                ax3.set_ylabel('Max Density (log scale)', fontsize=11)\r\n                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')\r\n                ax3.legend(fontsize=10)\r\n                ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):\r\n        \"\"\"\r\n        Track how prediction errors grow and accumulate over time.\r\n        Helps identify when the solution starts diverging from physical behavior.\r\n        \r\n        Now computes ACTUAL PDE residuals and gradient norms including phi.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        \r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        error_data = {\r\n            'times': time_points,\r\n            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},\r\n            'field_ranges': {'rho': [], 'vx': [], 'vy': []},\r\n            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},\r\n            'max_residuals': []\r\n        }\r\n        \r\n        if dimension == 3:\r\n            error_data['pde_residuals']['momentum_z'] = []\r\n            error_data['field_ranges']['vz'] = []\r\n            error_data['gradient_norms']['vz'] = []\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n            else:  # dimension == 3\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                Z_flat = Z.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n            \r\n            # Compute ACTUAL PDE residuals using the proper residue function\r\n            if dimension == 2:\r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n                \r\n            else:  # 3D\r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n            \r\n            # Get predictions for field statistics\r\n            pred = model(colloc)\r\n            rho = pred[:, 0:1]\r\n            vx = pred[:, 1:2]\r\n            vy = pred[:, 2:3]\r\n            phi = pred[:, -1:]  # Last column is phi\r\n            \r\n            # Track field statistics\r\n            error_data['field_ranges']['rho'].append(torch.std(rho).item())\r\n            error_data['field_ranges']['vx'].append(torch.std(vx).item())\r\n            error_data['field_ranges']['vy'].append(torch.std(vy).item())\r\n            if dimension == 3:\r\n                vz = pred[:, 3:4]\r\n                error_data['field_ranges']['vz'].append(torch.std(vz).item())\r\n            \r\n            # Track gradient norms for ALL fields including phi\r\n            with torch.enable_grad():\r\n                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                \r\n                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())\r\n                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())\r\n                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())\r\n                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())\r\n                \r\n                if dimension == 3:\r\n                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())\r\n            \r\n            # Max residual magnitude (now including Poisson!)\r\n            max_res = max(\r\n                error_data['pde_residuals']['continuity'][-1],\r\n                error_data['pde_residuals']['momentum_x'][-1],\r\n                error_data['pde_residuals']['momentum_y'][-1],\r\n                error_data['pde_residuals']['poisson'][-1]\r\n            )\r\n            if dimension == 3:\r\n                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])\r\n            \r\n            error_data['max_residuals'].append(max_res)\r\n        \r\n        return error_data\r\n    \r\n    def plot_temporal_error_accumulation(self, error_data):\r\n        \"\"\"\r\n        Visualize how errors accumulate and grow over time.\r\n        Shows which equations fail first and error growth rates.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = error_data['times']\r\n        \r\n        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)\r\n        # IMPORTANT: Now showing actual Poisson residuals!\r\n        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)\r\n        if 'momentum_z' in error_data['pde_residuals']:\r\n            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)\r\n        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Field Range Evolution (variance growth)\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)\r\n        if 'vz' in error_data['field_ranges']:\r\n            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Field Standard Deviation', fontsize=11)\r\n        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=9)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: Maximum Residual Growth\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)\r\n        if len(times) &gt; 5:\r\n            # Fit exponential growth\r\n            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)\r\n            valid_idx = np.isfinite(log_res)\r\n            if valid_idx.sum() &gt; 3:\r\n                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.legend(fontsize=10)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Max Residual (log)', fontsize=11)\r\n        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)\r\n        if 'vz' in error_data['gradient_norms']:\r\n            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)\r\n        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax4.legend(fontsize=9, loc='best')\r\n        ax4.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 5: Cumulative Error Metric\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        cumulative_error = np.cumsum(error_data['max_residuals'])\r\n        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Cumulative Error', fontsize=11)\r\n        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3)\r\n        \r\n        # Plot 6: Error Growth Rate\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        if len(times) &gt; 1:\r\n            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)\r\n            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n            ax6.set_xlabel('Time', fontsize=11)\r\n            ax6.set_ylabel('Error Growth Rate', fontsize=11)\r\n            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')\r\n            ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_spectral_bias_analysis(self, spectra_data):\r\n        \"\"\"\r\n        Enhanced spectral analysis focused on detecting spectral bias.\r\n        Shows frequency damping and mode evolution over time.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: Radially averaged power spectra at different times\r\n        ax1 = fig.add_subplot(gs[0, :2])\r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.sum() &gt; 0:\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,\r\n                          label=f't={spec[\"time\"]:.2f}', marker='o', markersize=3)\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power P(k)', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=8, ncol=2)\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Spectral Damping Rate\r\n        ax2 = fig.add_subplot(gs[0, 2])\r\n        if len(spectra_data) &gt;= 2:\r\n            # Compare first and last spectra\r\n            spec_0 = spectra_data[0]\r\n            spec_f = spectra_data[-1]\r\n            \r\n            # Compute power ratio (damping)\r\n            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))\r\n            k_bins = np.linspace(0, k_max, 20)\r\n            damping_ratio = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask_0 = (spec_0['k'] &gt;= k_bins[i]) &amp; (spec_0['k'] &lt; k_bins[i+1])\r\n                mask_f = (spec_f['k'] &gt;= k_bins[i]) &amp; (spec_f['k'] &lt; k_bins[i+1])\r\n                if mask_0.sum() &gt; 0 and mask_f.sum() &gt; 0:\r\n                    p0 = np.mean(spec_0['power'][mask_0])\r\n                    pf = np.mean(spec_f['power'][mask_f])\r\n                    if p0 &gt; 0:\r\n                        damping_ratio.append(pf / p0)\r\n                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)\r\n                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')\r\n                ax2.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)\r\n                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')\r\n                ax2.legend(fontsize=10)\r\n                ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: High-k Mode Tracking\r\n        ax3 = fig.add_subplot(gs[1, 0])\r\n        times = [spec['time'] for spec in spectra_data]\r\n        \r\n        # Track power in different k ranges\r\n        low_k_power = []\r\n        mid_k_power = []\r\n        high_k_power = []\r\n        \r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            k_max = np.max(k_flat)\r\n            low_k_power.append(np.mean(p_flat[k_flat &lt; k_max/3]))\r\n            mid_k_power.append(np.mean(p_flat[(k_flat &gt;= k_max/3) &amp; (k_flat &lt; 2*k_max/3)]))\r\n            high_k_power.append(np.mean(p_flat[k_flat &gt;= 2*k_max/3]))\r\n        \r\n        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)\r\n        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)\r\n        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Average Power (log)', fontsize=11)\r\n        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Spectral Index Evolution\r\n        ax4 = fig.add_subplot(gs[1, 1])\r\n        spectral_indices = []\r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            # Fit power law in log space\r\n            valid = (k_flat &gt; 0) &amp; (p_flat &gt; 0)\r\n            if valid.sum() &gt; 10:\r\n                log_k = np.log(k_flat[valid])\r\n                log_p = np.log(p_flat[valid])\r\n                coeffs = np.polyfit(log_k, log_p, 1)\r\n                spectral_indices.append(coeffs[0])  # Slope = spectral index\r\n            else:\r\n                spectral_indices.append(np.nan)\r\n        \r\n        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)\r\n        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)\r\n        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Density field snapshots\r\n        ax5 = fig.add_subplot(gs[1, 2])\r\n        # Show first, middle, last density fields\r\n        n_show = min(3, len(spectra_data))\r\n        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]\r\n        \r\n        for i, idx in enumerate(indices[:n_show]):\r\n            rho = spectra_data[idx]['rho']\r\n            # Compute structure function or variance\r\n            rho_var = np.var(rho)\r\n            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)\r\n            ax5.text(i, rho_var, f't={spectra_data[idx][\"time\"]:.2f}', \r\n                    ha='center', va='bottom', fontsize=9)\r\n        \r\n        ax5.set_xlabel('Time Snapshot', fontsize=11)\r\n        ax5.set_ylabel('Density Variance', fontsize=11)\r\n        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')\r\n        ax5.set_xticks(range(n_show))\r\n        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])\r\n        ax5.grid(True, alpha=0.3, axis='y')\r\n        \r\n        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):\r\n        \"\"\"\r\n        Check for signs of solution instability or unphysical behavior.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_check)\r\n        \r\n        stability_data = {\r\n            'times': time_points,\r\n            'density_bounds': {'min': [], 'max': [], 'mean': []},\r\n            'velocity_norms': [],\r\n            'negative_density_fraction': [],\r\n            'extreme_values': [],\r\n            'solution_smoothness': []\r\n        }\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n            else:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n            \r\n            with torch.no_grad():\r\n                pred = model(colloc)\r\n                rho = pred[:, 0].cpu().numpy()\r\n                vx = pred[:, 1].cpu().numpy()\r\n                vy = pred[:, 2].cpu().numpy()\r\n                \r\n                # Density bounds\r\n                stability_data['density_bounds']['min'].append(np.min(rho))\r\n                stability_data['density_bounds']['max'].append(np.max(rho))\r\n                stability_data['density_bounds']['mean'].append(np.mean(rho))\r\n                \r\n                # Velocity norms\r\n                v_norm = np.sqrt(vx**2 + vy**2)\r\n                if dimension == 3:\r\n                    vz = pred[:, 3].cpu().numpy()\r\n                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)\r\n                stability_data['velocity_norms'].append(np.max(v_norm))\r\n                \r\n                # Negative density fraction\r\n                neg_frac = np.sum(rho &lt; 0) / len(rho)\r\n                stability_data['negative_density_fraction'].append(neg_frac)\r\n                \r\n                # Extreme value detection (more than 10x mean)\r\n                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) &gt; 10 * np.std(rho)) / len(rho)\r\n                stability_data['extreme_values'].append(extreme_frac)\r\n                \r\n                # Solution smoothness (Laplacian magnitude as proxy)\r\n                if dimension == 2:\r\n                    rho_2d = rho.reshape(n_spatial, n_spatial)\r\n                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + \r\n                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))\r\n                    stability_data['solution_smoothness'].append(np.mean(laplacian))\r\n                else:\r\n                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D\r\n        \r\n        return stability_data\r\n    \r\n    def plot_solution_stability(self, stability_data):\r\n        \"\"\"\r\n        Visualize solution stability metrics to detect unphysical behavior.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = stability_data['times']\r\n        \r\n        # Plot 1: Density Bounds\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)\r\n        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Plot 2: Velocity Norms\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)\r\n        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 3: Negative Density Fraction\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)\r\n        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Negative Density (%)', fontsize=11)\r\n        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        # Plot 4: Extreme Values\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)\r\n        ax4.set_title('Outlier Detection (&gt;10σ)', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Solution Smoothness\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)\r\n        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 6: Stability Summary\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        # Compute overall stability score (0-1, higher is worse)\r\n        neg_dens_score = np.array(stability_data['negative_density_fraction'])\r\n        extreme_score = np.array(stability_data['extreme_values'])\r\n        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score\r\n        \r\n        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)\r\n        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')\r\n        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')\r\n        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')\r\n        ax6.set_xlabel('Time', fontsize=11)\r\n        ax6.set_ylabel('Instability Score (%)', fontsize=11)\r\n        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')\r\n        ax6.legend(fontsize=9)\r\n        ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_comprehensive_diagnostics(self, model, dimension, tmax):\r\n        \"\"\"\r\n        Run all post-training diagnostics for long-term evolution analysis.\r\n        \r\n        Generates 6 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail\r\n        3. Conservation law violations - Mass/momentum drift over time\r\n        4. Temporal error accumulation - How errors compound over time\r\n        5. Spectral bias analysis - Frequency damping and mode evolution\r\n        6. Solution stability - Detecting unphysical behavior\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            dimension: Spatial dimension (2 or 3)\r\n            tmax: Maximum time to analyze\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Long-Term Evolution Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: PDE Residual Evolution\r\n        print(\"\\n[1/5] Computing PDE residual evolution...\")\r\n        residuals = self.compute_residual_heatmap(model, dimension, tmax)\r\n        self.plot_residual_heatmaps(residuals)\r\n        print(\"      [OK] PDE residual evolution saved\")\r\n        \r\n        # Plot 3: Conservation Laws\r\n        print(\"\\n[2/5] Analyzing conservation law violations...\")\r\n        conservation = self.check_conservation_laws(model, dimension, tmax)\r\n        self.plot_conservation_laws(conservation)\r\n        print(\"      [OK] Conservation violations plot saved\")\r\n        \r\n        # Plot 4: Temporal Error Accumulation\r\n        print(\"\\n[3/5] Computing temporal error accumulation...\")\r\n        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)\r\n        self.plot_temporal_error_accumulation(error_data)\r\n        print(\"      [OK] Error accumulation plot saved\")\r\n        \r\n        # Plot 5: Spectral Bias Analysis\r\n        print(\"\\n[4/5] Analyzing spectral bias and mode damping...\")\r\n        spectra = self.compute_spectral_content(model, dimension, tmax)\r\n        self.plot_spectral_bias_analysis(spectra)\r\n        print(\"      [OK] Spectral bias analysis saved\")\r\n        \r\n        # Plot 6: Solution Stability\r\n        print(\"\\n[5/5] Checking solution stability...\")\r\n        stats = self.compute_temporal_statistics(model, dimension, tmax)\r\n        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)\r\n        self.plot_solution_stability(stability_data)\r\n        print(\"      [OK] Solution stability diagnostics saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nLong-Term Evolution Diagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")\r\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns\")\r\n        print(\"  3. conservation_laws.png - Conservation law drift over time\")\r\n        print(\"  4. temporal_error_accumulation.png - Error growth and compounding\")\r\n        print(\"  5. spectral_bias_analysis.png - Frequency damping and spectral issues\")\r\n        print(\"  6. solution_stability.png - Physical validity and stability metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n    \r\n    \r\n    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):\r\n        \"\"\"\r\n        Smart dispatcher: runs appropriate diagnostics based on case type.\r\n        \r\n        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics\r\n        - 3D power spectrum: IC fitting diagnostics  \r\n        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)\r\n            true_ic_data: Dict with IC data (required for 3D power spectrum)\r\n                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        if self.is_3d_power_spectrum:\r\n            # 3D power spectrum: Focus on IC fitting\r\n            if true_ic_data is None:\r\n                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")\r\n            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)\r\n        else:\r\n            # All other cases: Focus on temporal evolution\r\n            if tmax is None:\r\n                raise ValueError(\"tmax required for temporal evolution diagnostics\")\r\n            self.run_comprehensive_diagnostics(model, self.dimension, tmax)\r\n    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================\r\n    \r\n    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):\r\n        \"\"\"\r\n        Compare predicted vs true ICs spatially for both 2D and 3D cases.\r\n        Critical for diagnosing WHERE the network fails to fit the IC structure.\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC collocation points and true values\r\n                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}\r\n                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            n_grid: Grid resolution per dimension (for visualization grid)\r\n        \r\n        Returns:\r\n            Dict with predicted and true fields, plus errors\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing IC spatial comparison on training IC points...\")\r\n        \r\n        # Use the actual IC collocation points from training\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions at IC points\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n                phi_pred = pred[:, 4].cpu().numpy()\r\n            else:\r\n                phi_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # For visualization, create a regular grid\r\n        x = torch.linspace(0, 1, n_grid, device=device)\r\n        y = torch.linspace(0, 1, n_grid, device=device)\r\n        X, Y = torch.meshgrid(x, y, indexing='ij')\r\n        T = torch.zeros_like(X)\r\n        \r\n        if self.dimension == 3:\r\n            # For 3D, take z=0.5 slice\r\n            Z = torch.full_like(X, 0.5)\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n        else:\r\n            # For 2D\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n        \r\n        # Get predictions on visualization grid\r\n        with torch.no_grad():\r\n            pred_viz = model(colloc_viz)\r\n            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()\r\n            else:\r\n                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n        \r\n        # For \"true\" visualization, we need to interpolate from IC points to regular grid\r\n        # Extract spatial coordinates from IC collocation points (detach first)\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Find points near z=0.5 for visualization\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        if mask_z.sum() &gt; 100:  # Need enough points for interpolation\r\n            from scipy.interpolate import griddata\r\n            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n            X_viz = X.cpu().numpy()\r\n            Y_viz = Y.cpu().numpy()\r\n            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])\r\n            \r\n            # Interpolate with nearest neighbor fallback for NaN values\r\n            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            \r\n            # For uniform density (all values the same), fill NaNs with the constant value\r\n            # For varying fields, use nearest neighbor interpolation for NaN regions\r\n            if np.isnan(rho_true_viz).any():\r\n                if np.std(rho_true[mask_z]) &lt; 1e-10:\r\n                    # Uniform field - fill with mean\r\n                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))\r\n                else:\r\n                    # Varying field - use nearest neighbor for gaps\r\n                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)\r\n            \r\n            if np.isnan(vx_true_viz).any():\r\n                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)\r\n            \r\n            if np.isnan(vy_true_viz).any():\r\n                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)\r\n            \r\n            if self.dimension == 3:\r\n                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n                if np.isnan(vz_true_viz).any():\r\n                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)\r\n        else:\r\n            # Not enough points for interpolation, use predicted as reference\r\n            print(\"  [WARN] Not enough IC points for ground truth visualization\")\r\n            rho_true_viz = rho_pred_viz\r\n            vx_true_viz = vx_pred_viz\r\n            vy_true_viz = vy_pred_viz\r\n            if self.dimension == 3:\r\n                vz_true_viz = vz_pred_viz\r\n        \r\n        result = {\r\n            'x': x.cpu().numpy(),\r\n            'y': y.cpu().numpy(),\r\n            'rho_pred': rho_pred_viz,\r\n            'rho_true': rho_true_viz,\r\n            'vx_pred': vx_pred_viz,\r\n            'vx_true': vx_true_viz,\r\n            'vy_pred': vy_pred_viz,\r\n            'vy_true': vy_true_viz,\r\n            'phi_pred': phi_pred_viz,\r\n            # Also store scatter point data for metrics\r\n            'rho_pred_scatter': rho_pred,\r\n            'rho_true_scatter': rho_true,\r\n            'vx_pred_scatter': vx_pred,\r\n            'vx_true_scatter': vx_true,\r\n            'vy_pred_scatter': vy_pred,\r\n            'vy_true_scatter': vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            result.update({\r\n                'vz_pred': vz_pred_viz,\r\n                'vz_true': vz_true_viz,\r\n                'vz_pred_scatter': vz_pred,\r\n                'vz_true_scatter': vz_true,\r\n            })\r\n        \r\n        return result\r\n    \r\n    def plot_ic_power_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot power spectrum comparison for all velocity components.\r\n        Shows which scales (k-modes) are missing or damped.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_ic_component_convergence(self):\r\n        \"\"\"\r\n        Plot IC component loss convergence over training.\r\n        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.\r\n        \"\"\"\r\n        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:\r\n            print(\"IC component tracking not available.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        \r\n        # Replace zeros and very small values with a minimum threshold for log plotting\r\n        def safe_log_data(data):\r\n            \"\"\"Replace zeros/negative values with small epsilon for log plotting.\"\"\"\r\n            data_array = np.array(data)\r\n            data_array = np.where(data_array &lt;= 0, 1e-12, data_array)\r\n            return data_array\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: All IC components on same plot\r\n        ax1 = fig.add_subplot(gs[0, :])\r\n        \r\n        # Check if rho is zero/constant (uniform field case)\r\n        rho_is_uniform = np.max(self.history['ic_rho']) &lt; 1e-10\r\n        \r\n        if not rho_is_uniform:\r\n            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', \r\n                        marker='o', markersize=3, markevery=max(1, len(iters)//20))\r\n        else:\r\n            # Plot a dashed line at the bottom to indicate uniform/zero loss\r\n            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)\r\n        \r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', \r\n                    marker='s', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', \r\n                    marker='^', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', \r\n                    marker='d', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', \r\n                    marker='v', markersize=3, markevery=max(1, len(iters)//20))\r\n        \r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10, ncol=5)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Individual component plots\r\n        components = [\r\n            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),\r\n            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),\r\n            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),\r\n        ]\r\n        \r\n        for hist_key, title, grid_pos, is_uniform in components:\r\n            ax = fig.add_subplot(grid_pos)\r\n            if is_uniform:\r\n                # Show a flat line at bottom with annotation\r\n                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)\r\n                ax.text(0.5, 0.5, 'Uniform IC\\n(no spatial variation)', \r\n                       transform=ax.transAxes, ha='center', va='center',\r\n                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\r\n                ax.set_ylim([1e-13, 1e-10])\r\n            else:\r\n                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')\r\n            \r\n            ax.set_xlabel('Iteration', fontsize=10)\r\n            ax.set_ylabel('Loss (log scale)', fontsize=10)\r\n            ax.set_title(title, fontsize=11, fontweight='bold')\r\n            ax.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_ic_correlation_metrics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Compute spatial correlation between predicted and true ICs.\r\n        High correlation = good spatial structure capture.\r\n        \"\"\"\r\n        print(f\"  Computing IC correlation metrics on training IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy().flatten()\r\n            vx_pred = pred[:, 1].cpu().numpy().flatten()\r\n            vy_pred = pred[:, 2].cpu().numpy().flatten()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy().flatten()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()\r\n        \r\n        # Compute correlations - handle uniform fields (zero variance)\r\n        def safe_corrcoef(pred, true):\r\n            \"\"\"Compute correlation, handling constant fields gracefully.\"\"\"\r\n            # Check if either field is constant\r\n            if np.std(pred) &lt; 1e-10 or np.std(true) &lt; 1e-10:\r\n                # For uniform fields, correlation is undefined\r\n                # If both are uniform and equal, perfect match (1.0)\r\n                # If different constants, poor match (use normalized error)\r\n                if np.std(pred) &lt; 1e-10 and np.std(true) &lt; 1e-10:\r\n                    # Both uniform - check if they match\r\n                    mean_diff = abs(np.mean(pred) - np.mean(true))\r\n                    return 1.0 if mean_diff &lt; 1e-6 else 0.0\r\n                else:\r\n                    # One uniform, one not - cannot capture structure\r\n                    return 0.0\r\n            else:\r\n                return np.corrcoef(pred, true)[0, 1]\r\n        \r\n        correlations = {\r\n            'rho': safe_corrcoef(rho_pred, rho_true),\r\n            'vx': safe_corrcoef(vx_pred, vx_true),\r\n            'vy': safe_corrcoef(vy_pred, vy_true),\r\n        }\r\n        \r\n        # Compute RMS errors\r\n        rms_errors = {\r\n            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),\r\n            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),\r\n            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)\r\n            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))\r\n        \r\n        return correlations, rms_errors\r\n    \r\n    def plot_ic_metrics_summary(self, correlations, rms_errors):\r\n        \"\"\"\r\n        Summary plot: correlation and RMS error for each IC component.\r\n        Quick visual diagnostic of IC fitting quality.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        components = ['rho', 'vx', 'vy', 'vz']\r\n        labels = ['ρ', 'vx', 'vy', 'vz']\r\n        \r\n        # Correlation plot\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        corr_vals = [correlations[c] for c in components]\r\n        \r\n        # Handle NaN/inf correlations from uniform fields\r\n        corr_vals_safe = []\r\n        for val in corr_vals:\r\n            if np.isnan(val) or np.isinf(val):\r\n                corr_vals_safe.append(0.0)  # Show as 0 for visualization\r\n            else:\r\n                corr_vals_safe.append(val)\r\n        \r\n        colors = ['green' if c &gt; 0.9 else 'orange' if c &gt; 0.7 else 'red' for c in corr_vals_safe]\r\n        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')\r\n        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')\r\n        ax1.set_ylabel('Correlation Coefficient', fontsize=11)\r\n        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax1.set_ylim([0, 1.05])\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):\r\n            height = bar.get_height()\r\n            if np.isnan(orig_val) or np.isinf(orig_val):\r\n                label_text = 'N/A\\n(uniform)'\r\n                fontsize = 8\r\n            else:\r\n                label_text = f'{val:.3f}'\r\n                fontsize = 10\r\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\r\n                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')\r\n        \r\n        # RMS Error plot\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        rms_vals = [rms_errors[c] for c in components]\r\n        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax2.set_ylabel('RMS Error', fontsize=11)\r\n        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val in zip(bars2, rms_vals):\r\n            height = bar.get_height()\r\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\r\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\r\n        \r\n        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Run diagnostics for 3D power spectrum case.\r\n        Focus on IC fitting quality, not temporal evolution.\r\n        \r\n        Generates 5 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum comparison (scale-by-scale)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running 3D Power Spectrum IC Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[1/4] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power Spectrum Comparison\r\n        print(\"\\n[2/4] Computing IC power spectrum comparison...\")\r\n        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n        self.plot_ic_power_spectrum_comparison(ps_data)\r\n        print(\"      [OK] IC power spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[3/4] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[4/4] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit\")\r\n        print(\"  4. ic_component_convergence.png - Which IC component fails\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        for comp in ['rho', 'vx', 'vy', 'vz']:\r\n            print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================\r\n    \r\n    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):\r\n        \"\"\"\r\n        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).\r\n        \r\n        The 5 plots are:\r\n        1. Training diagnostics (loss convergence and balance)\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            final_iteration: Final iteration number\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Unified Training Diagnostics (5 plots)\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 1: Training diagnostics\r\n        print(\"\\n[1/5] Plotting training diagnostics...\")\r\n        self.plot_diagnostics(final_iteration)\r\n        print(\"      [OK] Training diagnostics saved\")\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[2/5] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power/Field Spectrum Comparison\r\n        print(\"\\n[3/5] Computing spectrum comparison...\")\r\n        if self.perturbation_type == 'power_spectrum':\r\n            # For power spectrum: compare power spectra\r\n            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n            self.plot_ic_power_spectrum_comparison(ps_data)\r\n        else:\r\n            # For sinusoidal: compute field spectra from predictions\r\n            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)\r\n            self.plot_field_spectrum_comparison(ps_data)\r\n        print(\"      [OK] Spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[4/5] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[5/5] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Spectrum comparison\")\r\n        print(\"  4. ic_component_convergence.png - IC component convergence\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']\r\n        for comp in comps:\r\n            if comp in correlations:\r\n                print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):\r\n        \"\"\"\r\n        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.\r\n        Similar to power spectrum comparison but computed from actual fields.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing field spectrum comparison on IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # Need to interpolate to regular grid for FFT\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Use points near z=0.5 for 2D slice\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n            if mask_z.sum() &lt; 100:\r\n                mask_z = np.ones(len(z_ic), dtype=bool)\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        # Create regular grid for FFT\r\n        x_grid = np.linspace(0, 1, n_grid)\r\n        y_grid = np.linspace(0, 1, n_grid)\r\n        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')\r\n        \r\n        # Interpolate from scattered IC points to regular grid\r\n        from scipy.interpolate import griddata\r\n        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])\r\n        \r\n        # Interpolate density\r\n        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Interpolate velocities\r\n        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        if self.dimension == 3:\r\n            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Compute power spectra\r\n        def compute_power_spectrum(field):\r\n            fft = np.fft.fft2(field)\r\n            power = np.abs(np.fft.fftshift(fft))**2\r\n            \r\n            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            kx_shift = np.fft.fftshift(kx)\r\n            ky_shift = np.fft.fftshift(ky)\r\n            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n            K = np.sqrt(KX**2 + KY**2)\r\n            \r\n            return power, K\r\n        \r\n        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)\r\n        power_rho_true, _ = compute_power_spectrum(rho_true_grid)\r\n        \r\n        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)\r\n        power_vx_true, _ = compute_power_spectrum(vx_true_grid)\r\n        \r\n        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)\r\n        power_vy_true, _ = compute_power_spectrum(vy_true_grid)\r\n        \r\n        result = {\r\n            'K': K,\r\n            'power_rho_pred': power_rho_pred,\r\n            'power_rho_true': power_rho_true,\r\n            'power_vx_pred': power_vx_pred,\r\n            'power_vx_true': power_vx_true,\r\n            'power_vy_pred': power_vy_pred,\r\n            'power_vy_true': power_vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)\r\n            power_vz_true, _ = compute_power_spectrum(vz_true_grid)\r\n            result['power_vz_pred'] = power_vz_pred\r\n            result['power_vz_true'] = power_vz_true\r\n        \r\n        return result\r\n    \r\n    def plot_field_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot field spectrum comparison for sinusoidal cases.\r\n        Shows density and velocity spectra.\r\n        \"\"\"\r\n        if self.dimension == 3:\r\n            fig = plt.figure(figsize=(18, 5))\r\n            gs = GridSpec(1, 4, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        else:\r\n            fig = plt.figure(figsize=(14, 5))\r\n            gs = GridSpec(1, 3, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()"
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 1 file</summary>
      
Lint paths:

- `training/training_diagnostics.py`

Lint results

Lint errors for `training/training_diagnostics.py`


| Message | Range | Severity |
|-------|-------|----------|
| Variable "z" is not accessed | 221:13-221:14 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "vz_r" is not accessed | 234:32-234:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 345:33-345:38 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 345:42-345:47 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 346:31-346:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 346:40-346:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 347:31-347:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 347:40-347:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 372:33-372:38 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 372:42-372:47 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 372:51-372:56 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 373:31-373:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 373:40-373:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 373:49-373:54 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 374:31-374:36 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 374:40-374:45 | DIAGNOSTIC_SEVERITY_HINT |
| The function "trapz" is deprecated<br/>  Use 'trapezoid' instead | 374:49-374:54 | DIAGNOSTIC_SEVERITY_HINT |
| "stats_data" is not accessed | 1087:66-1087:76 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "phi_pred" is not accessed | 1353:17-1353:25 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "phi_pred" is not accessed | 1355:17-1355:25 | DIAGNOSTIC_SEVERITY_HINT |
| Variable "device" is not accessed | 1879:9-1879:15 | DIAGNOSTIC_SEVERITY_HINT |


</details>
</tool-use>

---

Option B is now implemented in `training/training_diagnostics.py`. The color scale uses an adaptive 4‑orders‑of‑magnitude window (floored at 1e‑12), while the title reports the actual min/max residual magnitude for context.

If you want the title to reflect the *displayed* range instead, I can tweak that too.

---

_**User (2026-01-24 16:39Z)**_

"Fix compute_residual_heatmap to use Chunking (Memory Fix)
Replace the existing compute_residual_heatmap method in TrainingDiagnostics with this chunked version. It processes the grid in smaller batches (e.g., 40,960 points) to stay within VRAM limits.
    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):
        """
        Compute PDE residuals on a regular grid over space-time using CHUNKING to avoid OOM.
        """
        from core.losses import pde_residue
        from config import PERTURBATIONTYPE, N_GRID, N_GRID_3D
        
        device = next(model.parameters()).device
        
        # Determine grid size
        if n_spatial is None:
            if str(PERTURBATIONTYPE).lower() == "powerspectrum":
                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)
            else:
                n_spatial = 80  # Default safe size
        
        print(f"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid (Chunked)...")
        
        # 1. GENERATE GRID POINTS
        if dimension == 2:
            x = torch.linspace(0, 1, n_spatial, device=device)
            y = torch.linspace(0, 1, n_spatial, device=device)
            t = torch.linspace(0, tmax, n_temporal, device=device)
            
            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')
            
            # Flatten all points: (N_total, 1)
            X_flat = X.reshape(-1, 1)
            Y_flat = Y.reshape(-1, 1)
            T_flat = T.reshape(-1, 1)
            
            total_points = X_flat.shape[0]
            
            # Initialize result arrays (on CPU to save GPU memory)
            rho_res_all = np.zeros(total_points, dtype=np.float32)
            vx_res_all = np.zeros(total_points, dtype=np.float32)
            vy_res_all = np.zeros(total_points, dtype=np.float32)
            phi_res_all = np.zeros(total_points, dtype=np.float32)
            
            # 2. PROCESS IN CHUNKS
            chunk_size = 40000  # Adjust based on your GPU (40k is safe for 16GB)
            
            model.eval() # Ensure eval mode
            
            for i in range(0, total_points, chunk_size):
                end_idx = min(i + chunk_size, total_points)
                
                # Create chunk tensors WITH GRADIENTS
                x_chunk = X_flat[i:end_idx].clone().detach().requires_grad_(True)
                y_chunk = Y_flat[i:end_idx].clone().detach().requires_grad_(True)
                t_chunk = T_flat[i:end_idx].clone().detach().requires_grad_(True)
                
                colloc_chunk = [x_chunk, y_chunk, t_chunk]
                
                # Compute residuals for chunk
                # Note: pde_residue usually expects training mode behavior for gradients, 
                # but standard autograd works in eval mode too if inputs require_grad.
                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc_chunk, model, dimension=2)
                
                # Store results
                rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()
                vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()
                vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()
                phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()
                
                # Clear GPU memory
                del x_chunk, y_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, phi_r
                torch.cuda.empty_cache()

            # Reshape back to grid
            return {
                'x': x.cpu().numpy(), 'y': y.cpu().numpy(), 't': t.cpu().numpy(),
                'rho_residual': rho_res_all.reshape(n_spatial, n_spatial, n_temporal),
                'vx_residual': vx_res_all.reshape(n_spatial, n_spatial, n_temporal),
                'vy_residual': vy_res_all.reshape(n_spatial, n_spatial, n_temporal),
                'phi_residual': phi_res_all.reshape(n_spatial, n_spatial, n_temporal)
            }

        elif dimension == 3:
            # Similar chunking logic for 3D...
            # (Use n_spatial_3d to reduce memory load)
            n_spatial_3d = max(32, n_spatial // 2) 
            
            x = torch.linspace(0, 1, n_spatial_3d, device=device)
            y = torch.linspace(0, 1, n_spatial_3d, device=device)
            z = torch.linspace(0, 1, n_spatial_3d, device=device)
            t = torch.linspace(0, tmax, n_temporal, device=device)
            
            z_slice_val = 0.5
            Z_slice = torch.full((n_spatial_3d, n_spatial_3d, n_temporal), z_slice_val, device=device)
            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')
            
            X_flat = X.reshape(-1, 1)
            Y_flat = Y.reshape(-1, 1)
            Z_flat = Z_slice.reshape(-1, 1)
            T_flat = T.reshape(-1, 1)
            
            total_points = X_flat.shape[0]
            
            # Result arrays
            rho_res_all = np.zeros(total_points, dtype=np.float32)
            vx_res_all = np.zeros(total_points, dtype=np.float32)
            vy_res_all = np.zeros(total_points, dtype=np.float32)
            vz_res_all = np.zeros(total_points, dtype=np.float32)
            phi_res_all = np.zeros(total_points, dtype=np.float32)
            
            chunk_size = 20000 # Smaller chunk for 3D due to more gradients
            
            for i in range(0, total_points, chunk_size):
                end_idx = min(i + chunk_size, total_points)
                
                x_chunk = X_flat[i:end_idx].clone().detach().requires_grad_(True)
                y_chunk = Y_flat[i:end_idx].clone().detach().requires_grad_(True)
                z_chunk = Z_flat[i:end_idx].clone().detach().requires_grad_(True)
                t_chunk = T_flat[i:end_idx].clone().detach().requires_grad_(True)
                
                colloc_chunk = [x_chunk, y_chunk, z_chunk, t_chunk]
                
                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc_chunk, model, dimension=3)
                
                rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()
                vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()
                vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()
                vz_res_all[i:end_idx] = vz_r.detach().cpu().numpy().flatten()
                phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()
                
                del x_chunk, y_chunk, z_chunk, t_chunk, colloc_chunk
                torch.cuda.empty_cache()

            return {
                'x': x.cpu().numpy(), 'y': y.cpu().numpy(), 't': t.cpu().numpy(),
                'rho_residual': rho_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),
                'vx_residual': vx_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),
                'vy_residual': vy_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),
                'vz_residual': vz_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),
                'phi_residual': phi_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal)
            }
"

I am running into OOM while making plots so make this fix

---

_**Agent (model gpt-5.2-codex, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Applying patch to update heatmap method**

I'm replacing the heatmap computation method fully, correcting the config naming to match existing usage, adding conditional CUDA cache clearing, ensuring proper gradient tracking on chunked inputs, and using a fixed constant for the z-dimension slice as planned.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "training/training_diagnostics.py"
}
```
Additional data:

```json
{
  "codeblockId": "b2aa8d85-29e5-4602-b1e1-10661af42482",
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/training/training_diagnostics.py",
      "query": "",
      "fragment": "",
      "_formatted": "file:///c%3A/Users/tirth/Documents/Projects/GRINN/Power%20spectrum%20perturbations/training/training_diagnostics.py",
      "_fsPath": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\training_diagnostics.py"
    },
    "codeblockId": "b2aa8d85-29e5-4602-b1e1-10661af42482",
    "codeBlockIdx": 0,
    "content": "import os\r\nimport numpy as np\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\n\r\nclass TrainingDiagnostics:\r\n    \"\"\"\r\n    Comprehensive diagnostics for PINN performance analysis.\r\n    \r\n    TWO DIAGNOSTIC MODES:\r\n    \r\n    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t&gt;0)\r\n       Focus: Why does PINN fail at long times?\r\n       Generates 6 diagnostic plots:\r\n         - Training diagnostics: Loss convergence and balance\r\n         - PDE residual evolution: When/where/which equations fail\r\n         - Conservation violations: Mass and momentum drift\r\n         - Temporal error accumulation: How errors compound over time\r\n         - Spectral bias analysis: Frequency damping and mode evolution\r\n         - Solution stability: Detecting unphysical behavior\r\n    \r\n    2. IC FITTING MODE (3D power spectrum only)\r\n       Focus: Why can't PINN fit complex initial conditions?\r\n       Generates 5 diagnostic plots:\r\n         - Training diagnostics: Loss convergence with IC component breakdown\r\n         - IC spatial comparison: Predicted vs true IC fields\r\n         - IC power spectrum: Scale-by-scale fit quality\r\n         - IC component convergence: Which component fails to converge\r\n         - IC metrics summary: Correlation and RMS error statistics\r\n    \r\n    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True\r\n    \"\"\"\r\n\r\n    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):\r\n        self.save_dir = save_dir\r\n        self.dimension = dimension\r\n        self.perturbation_type = str(perturbation_type).lower()\r\n        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')\r\n        \r\n        os.makedirs(save_dir, exist_ok=True)\r\n\r\n        # Storage for tracking metrics over iterations\r\n        self.history = {\r\n            'iteration': [],\r\n            'total_loss': [],\r\n            'pde_loss': [],\r\n            'ic_loss': [],\r\n            'mean_rho': [],\r\n            'max_rho': [],\r\n        }\r\n        \r\n        # Additional tracking for 3D power spectrum IC diagnostics\r\n        if self.is_3d_power_spectrum:\r\n            self.history.update({\r\n                'ic_rho': [],\r\n                'ic_vx': [],\r\n                'ic_vy': [],\r\n                'ic_vz': [],\r\n                'ic_phi': [],\r\n            })\r\n\r\n    def _prepare_inputs(self, geomtime_col):\r\n        \"\"\"Normalize collocation inputs to the model's expected format.\"\"\"\r\n        if isinstance(geomtime_col, (list, tuple)):\r\n            return geomtime_col\r\n        # Single tensor [N, D] -&gt; list of [N,1]\r\n        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]\r\n\r\n    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):\r\n        \"\"\"\r\n        Call this every N iterations during training.\r\n        \r\n        Args:\r\n            iteration: Current iteration number\r\n            model: PINN model\r\n            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses\r\n            geomtime_col: Collocation points\r\n            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum\r\n        \"\"\"\r\n        self.history['iteration'].append(iteration)\r\n        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))\r\n        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))\r\n        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))\r\n\r\n        # Compute diagnostics from current model state\r\n        with torch.no_grad():\r\n            inputs = self._prepare_inputs(geomtime_col)\r\n            pred = model(inputs)\r\n            # Use first channel as density/log-density proxy\r\n            rho = pred[:, 0].detach().cpu().numpy()\r\n\r\n            # Density statistics\r\n            self.history['mean_rho'].append(np.nanmean(rho))\r\n            self.history['max_rho'].append(np.nanmax(rho))\r\n        \r\n        # Track component IC losses for 3D power spectrum\r\n        if self.is_3d_power_spectrum and ic_component_losses is not None:\r\n            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))\r\n            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))\r\n            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))\r\n            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))\r\n            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))\r\n\r\n    def plot_diagnostics(self, iteration=None):\r\n        \"\"\"\r\n        Generate streamlined training diagnostic plot.\r\n        Combined loss components and balance in one figure.\r\n        \"\"\"\r\n        if len(self.history['iteration']) == 0:\r\n            print(\"No diagnostic data to plot.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        suffix = f\"_iter_{iteration}\" if iteration is not None else \"_final\"\r\n\r\n        # Combined training diagnostics plot\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        # Subplot 1: Loss evolution\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')\r\n        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')\r\n        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')\r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 2: Loss balance ratio\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ic_arr = np.array(self.history['ic_loss'])\r\n        pde_arr = np.array(self.history['pde_loss'])\r\n        ratio = pde_arr / (ic_arr + 1e-10)\r\n        ax2.plot(iters, ratio, 'purple', linewidth=2)\r\n        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')\r\n        ax2.set_xlabel('Iteration', fontsize=11)\r\n        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)\r\n        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 3: Density evolution\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')\r\n        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')\r\n        ax3.set_xlabel('Iteration', fontsize=11)\r\n        ax3.set_ylabel('Density', fontsize=11)\r\n        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n        \r\n        print(f\"Training diagnostic plot saved to {self.save_dir}\")\r\n\r\n    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================\r\n    \r\n    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n        \"\"\"\r\n        Compute PDE residuals on a regular grid over space-time using CHUNKING to avoid OOM.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Determine grid size\r\n        if n_spatial is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_spatial = 80\r\n        \r\n        print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid (Chunked)...\")\r\n        \r\n        if dimension == 2:\r\n            x = torch.linspace(0, 1, n_spatial, device=device)\r\n            y = torch.linspace(0, 1, n_spatial, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            \r\n            # Flatten all points: (N_total, 1)\r\n            X_flat = X.reshape(-1, 1)\r\n            Y_flat = Y.reshape(-1, 1)\r\n            T_flat = T.reshape(-1, 1)\r\n            \r\n            total_points = X_flat.shape[0]\r\n            \r\n            # Initialize result arrays (on CPU to save GPU memory)\r\n            rho_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vx_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vy_res_all = np.zeros(total_points, dtype=np.float32)\r\n            phi_res_all = np.zeros(total_points, dtype=np.float32)\r\n            \r\n            # Process in chunks\r\n            chunk_size = 40000\r\n            model.eval()\r\n            \r\n            for i in range(0, total_points, chunk_size):\r\n                end_idx = min(i + chunk_size, total_points)\r\n                \r\n                x_chunk = X_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                y_chunk = Y_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                t_chunk = T_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                \r\n                colloc_chunk = [x_chunk, y_chunk, t_chunk]\r\n                \r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc_chunk, model, dimension=2)\r\n                \r\n                rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()\r\n                vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()\r\n                vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()\r\n                phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()\r\n                \r\n                del x_chunk, y_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, phi_r\r\n                if device.type == \"cuda\":\r\n                    torch.cuda.empty_cache()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n                'vx_residual': vx_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n                'vy_residual': vy_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n                'phi_residual': phi_res_all.reshape(n_spatial, n_spatial, n_temporal)\r\n            }\r\n        \r\n        elif dimension == 3:\r\n            # For 3D, use coarser grid to save memory\r\n            n_spatial_3d = max(32, n_spatial // 2)\r\n            x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            z_slice_val = 0.5\r\n            Z_slice = torch.full((n_spatial_3d, n_spatial_3d, n_temporal), z_slice_val, device=device)\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            \r\n            X_flat = X.reshape(-1, 1)\r\n            Y_flat = Y.reshape(-1, 1)\r\n            Z_flat = Z_slice.reshape(-1, 1)\r\n            T_flat = T.reshape(-1, 1)\r\n            \r\n            total_points = X_flat.shape[0]\r\n            \r\n            rho_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vx_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vy_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vz_res_all = np.zeros(total_points, dtype=np.float32)\r\n            phi_res_all = np.zeros(total_points, dtype=np.float32)\r\n            \r\n            chunk_size = 20000\r\n            model.eval()\r\n            \r\n            for i in range(0, total_points, chunk_size):\r\n                end_idx = min(i + chunk_size, total_points)\r\n                \r\n                x_chunk = X_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                y_chunk = Y_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                z_chunk = Z_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                t_chunk = T_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                \r\n                colloc_chunk = [x_chunk, y_chunk, z_chunk, t_chunk]\r\n                \r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc_chunk, model, dimension=3)\r\n                \r\n                rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()\r\n                vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()\r\n                vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()\r\n                vz_res_all[i:end_idx] = vz_r.detach().cpu().numpy().flatten()\r\n                phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()\r\n                \r\n                del x_chunk, y_chunk, z_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, vz_r, phi_r\r\n                if device.type == \"cuda\":\r\n                    torch.cuda.empty_cache()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'vx_residual': vx_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'vy_residual': vy_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'vz_residual': vz_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'phi_residual': phi_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal)\r\n            }\r\n    \r\n    def plot_residual_heatmaps(self, residual_data, slice_idx=None):\r\n        \"\"\"\r\n        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.\r\n        Shows WHEN and WHERE each equation fails.\r\n        \"\"\"\r\n        if slice_idx is None:\r\n            slice_idx = residual_data['rho_residual'].shape[1] // 2\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        residuals = [\r\n            ('Continuity Equation', residual_data['rho_residual']),\r\n            ('Momentum X Equation', residual_data['vx_residual']),\r\n            ('Momentum Y Equation', residual_data['vy_residual']),\r\n            ('Poisson Equation', residual_data['phi_residual'])\r\n        ]\r\n        \r\n        for idx, (name, resid) in enumerate(residuals):\r\n            ax = fig.add_subplot(gs[idx // 2, idx % 2])\r\n            \r\n            # Slice at fixed y\r\n            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow\r\n            \r\n            # Use log scale for better visualization\r\n            resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n            actual_min = np.min(resid_log)\r\n            data_max = np.max(resid_log)\r\n            data_min = max(data_max - 4, -12)\r\n            \r\n            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                          extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                 residual_data['t'][0], residual_data['t'][-1]],\r\n                          vmin=data_min, vmax=data_max)\r\n            ax.set_xlabel('x', fontsize=11)\r\n            ax.set_ylabel('Time', fontsize=11)\r\n            ax.set_title(\r\n                f'{name} Residual\\nMin: 10^{actual_min:.1f}, Max: 10^{data_max:.1f}',\r\n                fontsize=12,\r\n                fontweight='bold'\r\n            )\r\n            cbar = plt.colorbar(im, ax=ax)\r\n            cbar.set_label('log₁₀|residual|', fontsize=10)\r\n        \r\n        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data[\"y\"][slice_idx]:.2f})', \r\n                    fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):\r\n        \"\"\"\r\n        Check if mass and momentum are conserved over time.\r\n        Critical for physical consistency - should be flat lines!\r\n        \r\n        Note: Uses higher resolution for power spectrum to ensure accurate integration.\r\n        \"\"\"\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum\r\n        if n_grid is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_grid = 100\r\n        \r\n        print(f\"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...\")\r\n        \r\n        conservation_data = {\r\n            'times': [],\r\n            'total_mass': [],\r\n            'total_momentum_x': [],\r\n            'total_momentum_y': []\r\n        }\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # Integrate using trapezoidal rule\r\n                dx = 1.0 / (n_grid - 1)\r\n                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, use coarser grid\r\n                n_grid_3d = max(40, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                z = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                dx = 1.0 / (n_grid_3d - 1)\r\n                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n        \r\n        return conservation_data\r\n    \r\n    def plot_conservation_laws(self, conservation_data):\r\n        \"\"\"Plot conserved quantities over time - should be flat lines!\"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = conservation_data['times']\r\n        \r\n        # Mass conservation\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        mass = np.array(conservation_data['total_mass'])\r\n        mass_initial = mass[0]\r\n        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift\r\n        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)\r\n        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Mass drift (%)', fontsize=11)\r\n        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Momentum X conservation\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        px = np.array(conservation_data['total_momentum_x'])\r\n        px_initial = px[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(px_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum X absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            px_drift = ((px - px_initial) / abs(px_initial)) * 100\r\n            ylabel = 'Momentum X drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax2.plot(times, px_drift, 'g-', linewidth=2.5)\r\n        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel(ylabel, fontsize=11)\r\n        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Momentum Y conservation\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        py = np.array(conservation_data['total_momentum_y'])\r\n        py_initial = py[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(py_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum Y absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            py_drift = ((py - py_initial) / abs(py_initial)) * 100\r\n            ylabel = 'Momentum Y drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax3.plot(times, py_drift, 'orange', linewidth=2.5)\r\n        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel(ylabel, fontsize=11)\r\n        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):\r\n        \"\"\"\r\n        Compute FFT of density field at different times.\r\n        Reveals spectral bias and high-frequency mode damping.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing spectral content at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        spectra = []\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # 2D FFT\r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                # Create wavenumber grid\r\n                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5\r\n                n_grid_3d = max(64, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n        \r\n        return spectra\r\n    \r\n    def plot_spectral_evolution(self, spectra_data):\r\n        \"\"\"\r\n        Plot power spectrum evolution over time.\r\n        Shows if high-frequency modes are being damped (spectral bias).\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        # Left: Radially averaged power spectra\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        \r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], \r\n                          linewidth=2, markersize=4, label=f\"t={spec['time']:.2f}\")\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Right: 2D power spectrum at final time\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        final_spec = spectra_data[-1]\r\n        power_log = np.log10(final_spec['power'] + 1e-12)\r\n        \r\n        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',\r\n                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),\r\n                              -np.max(final_spec['k']), np.max(final_spec['k'])])\r\n        ax2.set_xlabel('k_x', fontsize=11)\r\n        ax2.set_ylabel('k_y', fontsize=11)\r\n        ax2.set_title(f'2D Power Spectrum at t={final_spec[\"time\"]:.2f}', \r\n                     fontsize=12, fontweight='bold')\r\n        cbar = plt.colorbar(im, ax=ax2)\r\n        cbar.set_label('log₁₀(Power)', fontsize=10)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):\r\n        \"\"\"\r\n        Compute field statistics over time to track error accumulation.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing temporal statistics at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        stats = {\r\n            'times': [],\r\n            'rho_mean': [],\r\n            'rho_std': [],\r\n            'rho_max': [],\r\n            'grad_rho_mean': [],\r\n            'grad_rho_max': []\r\n        }\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients (retain_graph=True for first call since we need multiple gradients)\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5 for computational efficiency\r\n                n_spatial_3d = max(50, n_spatial // 2)\r\n                x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                Z_flat = Z.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                Z_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n        \r\n        return stats\r\n    \r\n    def plot_temporal_statistics(self, stats_data):\r\n        \"\"\"\r\n        Plot field statistics over time.\r\n        Shows if solution becomes unphysical or gradients explode.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = stats_data['times']\r\n        \r\n        # Density statistics\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax1.fill_between(times,\r\n                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),\r\n                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),\r\n                         alpha=0.3, color='b', label='±1 std')\r\n        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Gradient statistics\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)\r\n        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Growth rate analysis\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        rho_max = np.array(stats_data['rho_max'])\r\n        if len(rho_max) &gt; 5:\r\n            # Fit exponential growth\r\n            log_rho = np.log(rho_max + 1e-10)\r\n            valid_idx = np.isfinite(log_rho)\r\n            if valid_idx.sum() &gt; 5:\r\n                times_valid = np.array(times)[valid_idx]\r\n                log_rho_valid = log_rho[valid_idx]\r\n                coeffs = np.polyfit(times_valid, log_rho_valid, 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                \r\n                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, \r\n                           label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.set_xlabel('Time', fontsize=11)\r\n                ax3.set_ylabel('Max Density (log scale)', fontsize=11)\r\n                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')\r\n                ax3.legend(fontsize=10)\r\n                ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):\r\n        \"\"\"\r\n        Track how prediction errors grow and accumulate over time.\r\n        Helps identify when the solution starts diverging from physical behavior.\r\n        \r\n        Now computes ACTUAL PDE residuals and gradient norms including phi.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        \r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        error_data = {\r\n            'times': time_points,\r\n            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},\r\n            'field_ranges': {'rho': [], 'vx': [], 'vy': []},\r\n            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},\r\n            'max_residuals': []\r\n        }\r\n        \r\n        if dimension == 3:\r\n            error_data['pde_residuals']['momentum_z'] = []\r\n            error_data['field_ranges']['vz'] = []\r\n            error_data['gradient_norms']['vz'] = []\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n            else:  # dimension == 3\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                Z_flat = Z.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n            \r\n            # Compute ACTUAL PDE residuals using the proper residue function\r\n            if dimension == 2:\r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n                \r\n            else:  # 3D\r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n            \r\n            # Get predictions for field statistics\r\n            pred = model(colloc)\r\n            rho = pred[:, 0:1]\r\n            vx = pred[:, 1:2]\r\n            vy = pred[:, 2:3]\r\n            phi = pred[:, -1:]  # Last column is phi\r\n            \r\n            # Track field statistics\r\n            error_data['field_ranges']['rho'].append(torch.std(rho).item())\r\n            error_data['field_ranges']['vx'].append(torch.std(vx).item())\r\n            error_data['field_ranges']['vy'].append(torch.std(vy).item())\r\n            if dimension == 3:\r\n                vz = pred[:, 3:4]\r\n                error_data['field_ranges']['vz'].append(torch.std(vz).item())\r\n            \r\n            # Track gradient norms for ALL fields including phi\r\n            with torch.enable_grad():\r\n                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                \r\n                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())\r\n                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())\r\n                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())\r\n                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())\r\n                \r\n                if dimension == 3:\r\n                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())\r\n            \r\n            # Max residual magnitude (now including Poisson!)\r\n            max_res = max(\r\n                error_data['pde_residuals']['continuity'][-1],\r\n                error_data['pde_residuals']['momentum_x'][-1],\r\n                error_data['pde_residuals']['momentum_y'][-1],\r\n                error_data['pde_residuals']['poisson'][-1]\r\n            )\r\n            if dimension == 3:\r\n                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])\r\n            \r\n            error_data['max_residuals'].append(max_res)\r\n        \r\n        return error_data\r\n    \r\n    def plot_temporal_error_accumulation(self, error_data):\r\n        \"\"\"\r\n        Visualize how errors accumulate and grow over time.\r\n        Shows which equations fail first and error growth rates.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = error_data['times']\r\n        \r\n        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)\r\n        # IMPORTANT: Now showing actual Poisson residuals!\r\n        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)\r\n        if 'momentum_z' in error_data['pde_residuals']:\r\n            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)\r\n        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Field Range Evolution (variance growth)\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)\r\n        if 'vz' in error_data['field_ranges']:\r\n            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Field Standard Deviation', fontsize=11)\r\n        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=9)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: Maximum Residual Growth\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)\r\n        if len(times) &gt; 5:\r\n            # Fit exponential growth\r\n            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)\r\n            valid_idx = np.isfinite(log_res)\r\n            if valid_idx.sum() &gt; 3:\r\n                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.legend(fontsize=10)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Max Residual (log)', fontsize=11)\r\n        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)\r\n        if 'vz' in error_data['gradient_norms']:\r\n            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)\r\n        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax4.legend(fontsize=9, loc='best')\r\n        ax4.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 5: Cumulative Error Metric\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        cumulative_error = np.cumsum(error_data['max_residuals'])\r\n        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Cumulative Error', fontsize=11)\r\n        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3)\r\n        \r\n        # Plot 6: Error Growth Rate\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        if len(times) &gt; 1:\r\n            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)\r\n            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n            ax6.set_xlabel('Time', fontsize=11)\r\n            ax6.set_ylabel('Error Growth Rate', fontsize=11)\r\n            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')\r\n            ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_spectral_bias_analysis(self, spectra_data):\r\n        \"\"\"\r\n        Enhanced spectral analysis focused on detecting spectral bias.\r\n        Shows frequency damping and mode evolution over time.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: Radially averaged power spectra at different times\r\n        ax1 = fig.add_subplot(gs[0, :2])\r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.sum() &gt; 0:\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,\r\n                          label=f't={spec[\"time\"]:.2f}', marker='o', markersize=3)\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power P(k)', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=8, ncol=2)\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Spectral Damping Rate\r\n        ax2 = fig.add_subplot(gs[0, 2])\r\n        if len(spectra_data) &gt;= 2:\r\n            # Compare first and last spectra\r\n            spec_0 = spectra_data[0]\r\n            spec_f = spectra_data[-1]\r\n            \r\n            # Compute power ratio (damping)\r\n            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))\r\n            k_bins = np.linspace(0, k_max, 20)\r\n            damping_ratio = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask_0 = (spec_0['k'] &gt;= k_bins[i]) &amp; (spec_0['k'] &lt; k_bins[i+1])\r\n                mask_f = (spec_f['k'] &gt;= k_bins[i]) &amp; (spec_f['k'] &lt; k_bins[i+1])\r\n                if mask_0.sum() &gt; 0 and mask_f.sum() &gt; 0:\r\n                    p0 = np.mean(spec_0['power'][mask_0])\r\n                    pf = np.mean(spec_f['power'][mask_f])\r\n                    if p0 &gt; 0:\r\n                        damping_ratio.append(pf / p0)\r\n                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)\r\n                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')\r\n                ax2.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)\r\n                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')\r\n                ax2.legend(fontsize=10)\r\n                ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: High-k Mode Tracking\r\n        ax3 = fig.add_subplot(gs[1, 0])\r\n        times = [spec['time'] for spec in spectra_data]\r\n        \r\n        # Track power in different k ranges\r\n        low_k_power = []\r\n        mid_k_power = []\r\n        high_k_power = []\r\n        \r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            k_max = np.max(k_flat)\r\n            low_k_power.append(np.mean(p_flat[k_flat &lt; k_max/3]))\r\n            mid_k_power.append(np.mean(p_flat[(k_flat &gt;= k_max/3) &amp; (k_flat &lt; 2*k_max/3)]))\r\n            high_k_power.append(np.mean(p_flat[k_flat &gt;= 2*k_max/3]))\r\n        \r\n        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)\r\n        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)\r\n        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Average Power (log)', fontsize=11)\r\n        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Spectral Index Evolution\r\n        ax4 = fig.add_subplot(gs[1, 1])\r\n        spectral_indices = []\r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            # Fit power law in log space\r\n            valid = (k_flat &gt; 0) &amp; (p_flat &gt; 0)\r\n            if valid.sum() &gt; 10:\r\n                log_k = np.log(k_flat[valid])\r\n                log_p = np.log(p_flat[valid])\r\n                coeffs = np.polyfit(log_k, log_p, 1)\r\n                spectral_indices.append(coeffs[0])  # Slope = spectral index\r\n            else:\r\n                spectral_indices.append(np.nan)\r\n        \r\n        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)\r\n        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)\r\n        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Density field snapshots\r\n        ax5 = fig.add_subplot(gs[1, 2])\r\n        # Show first, middle, last density fields\r\n        n_show = min(3, len(spectra_data))\r\n        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]\r\n        \r\n        for i, idx in enumerate(indices[:n_show]):\r\n            rho = spectra_data[idx]['rho']\r\n            # Compute structure function or variance\r\n            rho_var = np.var(rho)\r\n            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)\r\n            ax5.text(i, rho_var, f't={spectra_data[idx][\"time\"]:.2f}', \r\n                    ha='center', va='bottom', fontsize=9)\r\n        \r\n        ax5.set_xlabel('Time Snapshot', fontsize=11)\r\n        ax5.set_ylabel('Density Variance', fontsize=11)\r\n        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')\r\n        ax5.set_xticks(range(n_show))\r\n        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])\r\n        ax5.grid(True, alpha=0.3, axis='y')\r\n        \r\n        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):\r\n        \"\"\"\r\n        Check for signs of solution instability or unphysical behavior.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_check)\r\n        \r\n        stability_data = {\r\n            'times': time_points,\r\n            'density_bounds': {'min': [], 'max': [], 'mean': []},\r\n            'velocity_norms': [],\r\n            'negative_density_fraction': [],\r\n            'extreme_values': [],\r\n            'solution_smoothness': []\r\n        }\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n            else:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n            \r\n            with torch.no_grad():\r\n                pred = model(colloc)\r\n                rho = pred[:, 0].cpu().numpy()\r\n                vx = pred[:, 1].cpu().numpy()\r\n                vy = pred[:, 2].cpu().numpy()\r\n                \r\n                # Density bounds\r\n                stability_data['density_bounds']['min'].append(np.min(rho))\r\n                stability_data['density_bounds']['max'].append(np.max(rho))\r\n                stability_data['density_bounds']['mean'].append(np.mean(rho))\r\n                \r\n                # Velocity norms\r\n                v_norm = np.sqrt(vx**2 + vy**2)\r\n                if dimension == 3:\r\n                    vz = pred[:, 3].cpu().numpy()\r\n                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)\r\n                stability_data['velocity_norms'].append(np.max(v_norm))\r\n                \r\n                # Negative density fraction\r\n                neg_frac = np.sum(rho &lt; 0) / len(rho)\r\n                stability_data['negative_density_fraction'].append(neg_frac)\r\n                \r\n                # Extreme value detection (more than 10x mean)\r\n                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) &gt; 10 * np.std(rho)) / len(rho)\r\n                stability_data['extreme_values'].append(extreme_frac)\r\n                \r\n                # Solution smoothness (Laplacian magnitude as proxy)\r\n                if dimension == 2:\r\n                    rho_2d = rho.reshape(n_spatial, n_spatial)\r\n                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + \r\n                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))\r\n                    stability_data['solution_smoothness'].append(np.mean(laplacian))\r\n                else:\r\n                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D\r\n        \r\n        return stability_data\r\n    \r\n    def plot_solution_stability(self, stability_data):\r\n        \"\"\"\r\n        Visualize solution stability metrics to detect unphysical behavior.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = stability_data['times']\r\n        \r\n        # Plot 1: Density Bounds\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)\r\n        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Plot 2: Velocity Norms\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)\r\n        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 3: Negative Density Fraction\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)\r\n        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Negative Density (%)', fontsize=11)\r\n        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        # Plot 4: Extreme Values\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)\r\n        ax4.set_title('Outlier Detection (&gt;10σ)', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Solution Smoothness\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)\r\n        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 6: Stability Summary\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        # Compute overall stability score (0-1, higher is worse)\r\n        neg_dens_score = np.array(stability_data['negative_density_fraction'])\r\n        extreme_score = np.array(stability_data['extreme_values'])\r\n        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score\r\n        \r\n        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)\r\n        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')\r\n        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')\r\n        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')\r\n        ax6.set_xlabel('Time', fontsize=11)\r\n        ax6.set_ylabel('Instability Score (%)', fontsize=11)\r\n        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')\r\n        ax6.legend(fontsize=9)\r\n        ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_comprehensive_diagnostics(self, model, dimension, tmax):\r\n        \"\"\"\r\n        Run all post-training diagnostics for long-term evolution analysis.\r\n        \r\n        Generates 6 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail\r\n        3. Conservation law violations - Mass/momentum drift over time\r\n        4. Temporal error accumulation - How errors compound over time\r\n        5. Spectral bias analysis - Frequency damping and mode evolution\r\n        6. Solution stability - Detecting unphysical behavior\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            dimension: Spatial dimension (2 or 3)\r\n            tmax: Maximum time to analyze\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Long-Term Evolution Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: PDE Residual Evolution\r\n        print(\"\\n[1/5] Computing PDE residual evolution...\")\r\n        residuals = self.compute_residual_heatmap(model, dimension, tmax)\r\n        self.plot_residual_heatmaps(residuals)\r\n        print(\"      [OK] PDE residual evolution saved\")\r\n        \r\n        # Plot 3: Conservation Laws\r\n        print(\"\\n[2/5] Analyzing conservation law violations...\")\r\n        conservation = self.check_conservation_laws(model, dimension, tmax)\r\n        self.plot_conservation_laws(conservation)\r\n        print(\"      [OK] Conservation violations plot saved\")\r\n        \r\n        # Plot 4: Temporal Error Accumulation\r\n        print(\"\\n[3/5] Computing temporal error accumulation...\")\r\n        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)\r\n        self.plot_temporal_error_accumulation(error_data)\r\n        print(\"      [OK] Error accumulation plot saved\")\r\n        \r\n        # Plot 5: Spectral Bias Analysis\r\n        print(\"\\n[4/5] Analyzing spectral bias and mode damping...\")\r\n        spectra = self.compute_spectral_content(model, dimension, tmax)\r\n        self.plot_spectral_bias_analysis(spectra)\r\n        print(\"      [OK] Spectral bias analysis saved\")\r\n        \r\n        # Plot 6: Solution Stability\r\n        print(\"\\n[5/5] Checking solution stability...\")\r\n        stats = self.compute_temporal_statistics(model, dimension, tmax)\r\n        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)\r\n        self.plot_solution_stability(stability_data)\r\n        print(\"      [OK] Solution stability diagnostics saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nLong-Term Evolution Diagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")\r\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns\")\r\n        print(\"  3. conservation_laws.png - Conservation law drift over time\")\r\n        print(\"  4. temporal_error_accumulation.png - Error growth and compounding\")\r\n        print(\"  5. spectral_bias_analysis.png - Frequency damping and spectral issues\")\r\n        print(\"  6. solution_stability.png - Physical validity and stability metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n    \r\n    \r\n    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):\r\n        \"\"\"\r\n        Smart dispatcher: runs appropriate diagnostics based on case type.\r\n        \r\n        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics\r\n        - 3D power spectrum: IC fitting diagnostics  \r\n        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)\r\n            true_ic_data: Dict with IC data (required for 3D power spectrum)\r\n                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        if self.is_3d_power_spectrum:\r\n            # 3D power spectrum: Focus on IC fitting\r\n            if true_ic_data is None:\r\n                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")\r\n            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)\r\n        else:\r\n            # All other cases: Focus on temporal evolution\r\n            if tmax is None:\r\n                raise ValueError(\"tmax required for temporal evolution diagnostics\")\r\n            self.run_comprehensive_diagnostics(model, self.dimension, tmax)\r\n    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================\r\n    \r\n    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):\r\n        \"\"\"\r\n        Compare predicted vs true ICs spatially for both 2D and 3D cases.\r\n        Critical for diagnosing WHERE the network fails to fit the IC structure.\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC collocation points and true values\r\n                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}\r\n                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            n_grid: Grid resolution per dimension (for visualization grid)\r\n        \r\n        Returns:\r\n            Dict with predicted and true fields, plus errors\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing IC spatial comparison on training IC points...\")\r\n        \r\n        # Use the actual IC collocation points from training\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions at IC points\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n                phi_pred = pred[:, 4].cpu().numpy()\r\n            else:\r\n                phi_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # For visualization, create a regular grid\r\n        x = torch.linspace(0, 1, n_grid, device=device)\r\n        y = torch.linspace(0, 1, n_grid, device=device)\r\n        X, Y = torch.meshgrid(x, y, indexing='ij')\r\n        T = torch.zeros_like(X)\r\n        \r\n        if self.dimension == 3:\r\n            # For 3D, take z=0.5 slice\r\n            Z = torch.full_like(X, 0.5)\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n        else:\r\n            # For 2D\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n        \r\n        # Get predictions on visualization grid\r\n        with torch.no_grad():\r\n            pred_viz = model(colloc_viz)\r\n            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()\r\n            else:\r\n                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n        \r\n        # For \"true\" visualization, we need to interpolate from IC points to regular grid\r\n        # Extract spatial coordinates from IC collocation points (detach first)\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Find points near z=0.5 for visualization\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        if mask_z.sum() &gt; 100:  # Need enough points for interpolation\r\n            from scipy.interpolate import griddata\r\n            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n            X_viz = X.cpu().numpy()\r\n            Y_viz = Y.cpu().numpy()\r\n            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])\r\n            \r\n            # Interpolate with nearest neighbor fallback for NaN values\r\n            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            \r\n            # For uniform density (all values the same), fill NaNs with the constant value\r\n            # For varying fields, use nearest neighbor interpolation for NaN regions\r\n            if np.isnan(rho_true_viz).any():\r\n                if np.std(rho_true[mask_z]) &lt; 1e-10:\r\n                    # Uniform field - fill with mean\r\n                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))\r\n                else:\r\n                    # Varying field - use nearest neighbor for gaps\r\n                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)\r\n            \r\n            if np.isnan(vx_true_viz).any():\r\n                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)\r\n            \r\n            if np.isnan(vy_true_viz).any():\r\n                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)\r\n            \r\n            if self.dimension == 3:\r\n                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n                if np.isnan(vz_true_viz).any():\r\n                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)\r\n        else:\r\n            # Not enough points for interpolation, use predicted as reference\r\n            print(\"  [WARN] Not enough IC points for ground truth visualization\")\r\n            rho_true_viz = rho_pred_viz\r\n            vx_true_viz = vx_pred_viz\r\n            vy_true_viz = vy_pred_viz\r\n            if self.dimension == 3:\r\n                vz_true_viz = vz_pred_viz\r\n        \r\n        result = {\r\n            'x': x.cpu().numpy(),\r\n            'y': y.cpu().numpy(),\r\n            'rho_pred': rho_pred_viz,\r\n            'rho_true': rho_true_viz,\r\n            'vx_pred': vx_pred_viz,\r\n            'vx_true': vx_true_viz,\r\n            'vy_pred': vy_pred_viz,\r\n            'vy_true': vy_true_viz,\r\n            'phi_pred': phi_pred_viz,\r\n            # Also store scatter point data for metrics\r\n            'rho_pred_scatter': rho_pred,\r\n            'rho_true_scatter': rho_true,\r\n            'vx_pred_scatter': vx_pred,\r\n            'vx_true_scatter': vx_true,\r\n            'vy_pred_scatter': vy_pred,\r\n            'vy_true_scatter': vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            result.update({\r\n                'vz_pred': vz_pred_viz,\r\n                'vz_true': vz_true_viz,\r\n                'vz_pred_scatter': vz_pred,\r\n                'vz_true_scatter': vz_true,\r\n            })\r\n        \r\n        return result\r\n    \r\n    def plot_ic_power_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot power spectrum comparison for all velocity components.\r\n        Shows which scales (k-modes) are missing or damped.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_ic_component_convergence(self):\r\n        \"\"\"\r\n        Plot IC component loss convergence over training.\r\n        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.\r\n        \"\"\"\r\n        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:\r\n            print(\"IC component tracking not available.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        \r\n        # Replace zeros and very small values with a minimum threshold for log plotting\r\n        def safe_log_data(data):\r\n            \"\"\"Replace zeros/negative values with small epsilon for log plotting.\"\"\"\r\n            data_array = np.array(data)\r\n            data_array = np.where(data_array &lt;= 0, 1e-12, data_array)\r\n            return data_array\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: All IC components on same plot\r\n        ax1 = fig.add_subplot(gs[0, :])\r\n        \r\n        # Check if rho is zero/constant (uniform field case)\r\n        rho_is_uniform = np.max(self.history['ic_rho']) &lt; 1e-10\r\n        \r\n        if not rho_is_uniform:\r\n            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', \r\n                        marker='o', markersize=3, markevery=max(1, len(iters)//20))\r\n        else:\r\n            # Plot a dashed line at the bottom to indicate uniform/zero loss\r\n            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)\r\n        \r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', \r\n                    marker='s', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', \r\n                    marker='^', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', \r\n                    marker='d', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', \r\n                    marker='v', markersize=3, markevery=max(1, len(iters)//20))\r\n        \r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10, ncol=5)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Individual component plots\r\n        components = [\r\n            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),\r\n            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),\r\n            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),\r\n        ]\r\n        \r\n        for hist_key, title, grid_pos, is_uniform in components:\r\n            ax = fig.add_subplot(grid_pos)\r\n            if is_uniform:\r\n                # Show a flat line at bottom with annotation\r\n                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)\r\n                ax.text(0.5, 0.5, 'Uniform IC\\n(no spatial variation)', \r\n                       transform=ax.transAxes, ha='center', va='center',\r\n                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\r\n                ax.set_ylim([1e-13, 1e-10])\r\n            else:\r\n                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')\r\n            \r\n            ax.set_xlabel('Iteration', fontsize=10)\r\n            ax.set_ylabel('Loss (log scale)', fontsize=10)\r\n            ax.set_title(title, fontsize=11, fontweight='bold')\r\n            ax.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_ic_correlation_metrics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Compute spatial correlation between predicted and true ICs.\r\n        High correlation = good spatial structure capture.\r\n        \"\"\"\r\n        print(f\"  Computing IC correlation metrics on training IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy().flatten()\r\n            vx_pred = pred[:, 1].cpu().numpy().flatten()\r\n            vy_pred = pred[:, 2].cpu().numpy().flatten()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy().flatten()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()\r\n        \r\n        # Compute correlations - handle uniform fields (zero variance)\r\n        def safe_corrcoef(pred, true):\r\n            \"\"\"Compute correlation, handling constant fields gracefully.\"\"\"\r\n            # Check if either field is constant\r\n            if np.std(pred) &lt; 1e-10 or np.std(true) &lt; 1e-10:\r\n                # For uniform fields, correlation is undefined\r\n                # If both are uniform and equal, perfect match (1.0)\r\n                # If different constants, poor match (use normalized error)\r\n                if np.std(pred) &lt; 1e-10 and np.std(true) &lt; 1e-10:\r\n                    # Both uniform - check if they match\r\n                    mean_diff = abs(np.mean(pred) - np.mean(true))\r\n                    return 1.0 if mean_diff &lt; 1e-6 else 0.0\r\n                else:\r\n                    # One uniform, one not - cannot capture structure\r\n                    return 0.0\r\n            else:\r\n                return np.corrcoef(pred, true)[0, 1]\r\n        \r\n        correlations = {\r\n            'rho': safe_corrcoef(rho_pred, rho_true),\r\n            'vx': safe_corrcoef(vx_pred, vx_true),\r\n            'vy': safe_corrcoef(vy_pred, vy_true),\r\n        }\r\n        \r\n        # Compute RMS errors\r\n        rms_errors = {\r\n            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),\r\n            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),\r\n            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)\r\n            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))\r\n        \r\n        return correlations, rms_errors\r\n    \r\n    def plot_ic_metrics_summary(self, correlations, rms_errors):\r\n        \"\"\"\r\n        Summary plot: correlation and RMS error for each IC component.\r\n        Quick visual diagnostic of IC fitting quality.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        components = ['rho', 'vx', 'vy', 'vz']\r\n        labels = ['ρ', 'vx', 'vy', 'vz']\r\n        \r\n        # Correlation plot\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        corr_vals = [correlations[c] for c in components]\r\n        \r\n        # Handle NaN/inf correlations from uniform fields\r\n        corr_vals_safe = []\r\n        for val in corr_vals:\r\n            if np.isnan(val) or np.isinf(val):\r\n                corr_vals_safe.append(0.0)  # Show as 0 for visualization\r\n            else:\r\n                corr_vals_safe.append(val)\r\n        \r\n        colors = ['green' if c &gt; 0.9 else 'orange' if c &gt; 0.7 else 'red' for c in corr_vals_safe]\r\n        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')\r\n        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')\r\n        ax1.set_ylabel('Correlation Coefficient', fontsize=11)\r\n        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax1.set_ylim([0, 1.05])\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):\r\n            height = bar.get_height()\r\n            if np.isnan(orig_val) or np.isinf(orig_val):\r\n                label_text = 'N/A\\n(uniform)'\r\n                fontsize = 8\r\n            else:\r\n                label_text = f'{val:.3f}'\r\n                fontsize = 10\r\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\r\n                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')\r\n        \r\n        # RMS Error plot\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        rms_vals = [rms_errors[c] for c in components]\r\n        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax2.set_ylabel('RMS Error', fontsize=11)\r\n        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val in zip(bars2, rms_vals):\r\n            height = bar.get_height()\r\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\r\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\r\n        \r\n        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Run diagnostics for 3D power spectrum case.\r\n        Focus on IC fitting quality, not temporal evolution.\r\n        \r\n        Generates 5 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum comparison (scale-by-scale)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running 3D Power Spectrum IC Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[1/4] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power Spectrum Comparison\r\n        print(\"\\n[2/4] Computing IC power spectrum comparison...\")\r\n        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n        self.plot_ic_power_spectrum_comparison(ps_data)\r\n        print(\"      [OK] IC power spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[3/4] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[4/4] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit\")\r\n        print(\"  4. ic_component_convergence.png - Which IC component fails\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        for comp in ['rho', 'vx', 'vy', 'vz']:\r\n            print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================\r\n    \r\n    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):\r\n        \"\"\"\r\n        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).\r\n        \r\n        The 5 plots are:\r\n        1. Training diagnostics (loss convergence and balance)\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            final_iteration: Final iteration number\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Unified Training Diagnostics (5 plots)\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 1: Training diagnostics\r\n        print(\"\\n[1/5] Plotting training diagnostics...\")\r\n        self.plot_diagnostics(final_iteration)\r\n        print(\"      [OK] Training diagnostics saved\")\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[2/5] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power/Field Spectrum Comparison\r\n        print(\"\\n[3/5] Computing spectrum comparison...\")\r\n        if self.perturbation_type == 'power_spectrum':\r\n            # For power spectrum: compare power spectra\r\n            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n            self.plot_ic_power_spectrum_comparison(ps_data)\r\n        else:\r\n            # For sinusoidal: compute field spectra from predictions\r\n            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)\r\n            self.plot_field_spectrum_comparison(ps_data)\r\n        print(\"      [OK] Spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[4/5] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[5/5] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Spectrum comparison\")\r\n        print(\"  4. ic_component_convergence.png - IC component convergence\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']\r\n        for comp in comps:\r\n            if comp in correlations:\r\n                print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):\r\n        \"\"\"\r\n        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.\r\n        Similar to power spectrum comparison but computed from actual fields.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing field spectrum comparison on IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # Need to interpolate to regular grid for FFT\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Use points near z=0.5 for 2D slice\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n            if mask_z.sum() &lt; 100:\r\n                mask_z = np.ones(len(z_ic), dtype=bool)\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        # Create regular grid for FFT\r\n        x_grid = np.linspace(0, 1, n_grid)\r\n        y_grid = np.linspace(0, 1, n_grid)\r\n        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')\r\n        \r\n        # Interpolate from scattered IC points to regular grid\r\n        from scipy.interpolate import griddata\r\n        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])\r\n        \r\n        # Interpolate density\r\n        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Interpolate velocities\r\n        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        if self.dimension == 3:\r\n            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Compute power spectra\r\n        def compute_power_spectrum(field):\r\n            fft = np.fft.fft2(field)\r\n            power = np.abs(np.fft.fftshift(fft))**2\r\n            \r\n            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            kx_shift = np.fft.fftshift(kx)\r\n            ky_shift = np.fft.fftshift(ky)\r\n            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n            K = np.sqrt(KX**2 + KY**2)\r\n            \r\n            return power, K\r\n        \r\n        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)\r\n        power_rho_true, _ = compute_power_spectrum(rho_true_grid)\r\n        \r\n        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)\r\n        power_vx_true, _ = compute_power_spectrum(vx_true_grid)\r\n        \r\n        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)\r\n        power_vy_true, _ = compute_power_spectrum(vy_true_grid)\r\n        \r\n        result = {\r\n            'K': K,\r\n            'power_rho_pred': power_rho_pred,\r\n            'power_rho_true': power_rho_true,\r\n            'power_vx_pred': power_vx_pred,\r\n            'power_vx_true': power_vx_true,\r\n            'power_vy_pred': power_vy_pred,\r\n            'power_vy_true': power_vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)\r\n            power_vz_true, _ = compute_power_spectrum(vz_true_grid)\r\n            result['power_vz_pred'] = power_vz_pred\r\n            result['power_vz_true'] = power_vz_true\r\n        \r\n        return result\r\n    \r\n    def plot_field_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot field spectrum comparison for sinusoidal cases.\r\n        Shows density and velocity spectra.\r\n        \"\"\"\r\n        if self.dimension == 3:\r\n            fig = plt.figure(figsize=(18, 5))\r\n            gs = GridSpec(1, 4, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        else:\r\n            fig = plt.figure(figsize=(14, 5))\r\n            gs = GridSpec(1, 3, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "      def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n          \"\"\"\r\n-         Compute PDE residuals on a regular grid over space-time.\r\n-         Critical for identifying WHERE and WHEN physics breaks down.\r\n-         \r\n-         Note: For power spectrum cases, uses higher resolution to match IC generation grid.\r\n+         Compute PDE residuals on a regular grid over space-time using CHUNKING to avoid OOM.\r\n          \"\"\"\r\n          from core.losses import pde_residue\r",
        "oldStart": 164,
        "newStart": 164,
        "oldLines": 8,
        "newLines": 5,
        "linesRemoved": 4,
        "linesAdded": 1
      },
      {
        "diffString": "          device = next(model.parameters()).device\r\n          \r\n-         # Use higher resolution for power spectrum to match IC generation\r\n+         # Determine grid size\r\n          if n_spatial is None:\r\n              if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r",
        "oldStart": 174,
        "newStart": 171,
        "oldLines": 5,
        "newLines": 5,
        "linesRemoved": 1,
        "linesAdded": 1
      },
      {
        "diffString": "                  n_spatial = 80\r\n          \r\n-         print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...\")\r\n+         print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid (Chunked)...\")\r\n          \r\n          if dimension == 2:\r",
        "oldStart": 181,
        "newStart": 178,
        "oldLines": 5,
        "newLines": 5,
        "linesRemoved": 1,
        "linesAdded": 1
      },
      {
        "diffString": "              t = torch.linspace(0, tmax, n_temporal, device=device)\r\n              \r\n-             # Create meshgrid\r\n              X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n-             colloc = [\r\n-                 X.reshape(-1, 1).requires_grad_(True),\r\n-                 Y.reshape(-1, 1).requires_grad_(True),\r\n-                 T.reshape(-1, 1).requires_grad_(True)\r\n-             ]\r\n-             \r\n-             rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n-             \r\n-             # Reshape back to grid\r\n-             rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n-             vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n-             vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n-             phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()\r\n+             \r\n+             # Flatten all points: (N_total, 1)\r\n+             X_flat = X.reshape(-1, 1)\r\n+             Y_flat = Y.reshape(-1, 1)\r\n+             T_flat = T.reshape(-1, 1)\r\n+             \r\n+             total_points = X_flat.shape[0]\r\n+             \r\n+             # Initialize result arrays (on CPU to save GPU memory)\r\n+             rho_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             vx_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             vy_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             phi_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             \r\n+             # Process in chunks\r\n+             chunk_size = 40000\r\n+             model.eval()\r\n+             \r\n+             for i in range(0, total_points, chunk_size):\r\n+                 end_idx = min(i + chunk_size, total_points)\r\n+                 \r\n+                 x_chunk = X_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 y_chunk = Y_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 t_chunk = T_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 \r\n+                 colloc_chunk = [x_chunk, y_chunk, t_chunk]\r\n+                 \r\n+                 rho_r, vx_r, vy_r, phi_r = pde_residue(colloc_chunk, model, dimension=2)\r\n+                 \r\n+                 rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()\r\n+                 vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()\r\n+                 vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()\r\n+                 phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()\r\n+                 \r\n+                 del x_chunk, y_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, phi_r\r\n+                 if device.type == \"cuda\":\r\n+                     torch.cuda.empty_cache()\r\n              \r\n              return {\r\n                  'x': x.cpu().numpy(),\r\n                  'y': y.cpu().numpy(),\r\n                  't': t.cpu().numpy(),\r\n-                 'rho_residual': rho_r,\r\n-                 'vx_residual': vx_r,\r\n-                 'vy_residual': vy_r,\r\n-                 'phi_residual': phi_r\r\n+                 'rho_residual': rho_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n+                 'vx_residual': vx_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n+                 'vy_residual': vy_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n+                 'phi_residual': phi_res_all.reshape(n_spatial, n_spatial, n_temporal)\r\n              }\r\n          \r",
        "oldStart": 188,
        "newStart": 185,
        "oldLines": 28,
        "newLines": 51,
        "linesRemoved": 18,
        "linesAdded": 41
      },
      {
        "diffString": "              x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n              y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n-             z = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n              t = torch.linspace(0, tmax, n_temporal, device=device)\r\n              \r\n-             # For 3D, we'll slice at z=0.5 to create 2D heatmaps\r\n-             z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)\r\n+             z_slice_val = 0.5\r\n+             Z_slice = torch.full((n_spatial_3d, n_spatial_3d, n_temporal), z_slice_val, device=device)\r\n              X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n-             colloc = [\r\n-                 X.reshape(-1, 1).requires_grad_(True),\r\n-                 Y.reshape(-1, 1).requires_grad_(True),\r\n-                 z_slice.reshape(-1, 1).requires_grad_(True),\r\n-                 T.reshape(-1, 1).requires_grad_(True)\r\n-             ]\r\n-             \r\n-             rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n-             \r\n-             rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n-             vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n-             vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n-             phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()\r\n+             \r\n+             X_flat = X.reshape(-1, 1)\r\n+             Y_flat = Y.reshape(-1, 1)\r\n+             Z_flat = Z_slice.reshape(-1, 1)\r\n+             T_flat = T.reshape(-1, 1)\r\n+             \r\n+             total_points = X_flat.shape[0]\r\n+             \r\n+             rho_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             vx_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             vy_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             vz_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             phi_res_all = np.zeros(total_points, dtype=np.float32)\r\n+             \r\n+             chunk_size = 20000\r\n+             model.eval()\r\n+             \r\n+             for i in range(0, total_points, chunk_size):\r\n+                 end_idx = min(i + chunk_size, total_points)\r\n+                 \r\n+                 x_chunk = X_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 y_chunk = Y_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 z_chunk = Z_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 t_chunk = T_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n+                 \r\n+                 colloc_chunk = [x_chunk, y_chunk, z_chunk, t_chunk]\r\n+                 \r\n+                 rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc_chunk, model, dimension=3)\r\n+                 \r\n+                 rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()\r\n+                 vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()\r\n+                 vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()\r\n+                 vz_res_all[i:end_idx] = vz_r.detach().cpu().numpy().flatten()\r\n+                 phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()\r\n+                 \r\n+                 del x_chunk, y_chunk, z_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, vz_r, phi_r\r\n+                 if device.type == \"cuda\":\r\n+                     torch.cuda.empty_cache()\r\n              \r\n              return {\r\n                  'x': x.cpu().numpy(),\r\n                  'y': y.cpu().numpy(),\r\n                  't': t.cpu().numpy(),\r\n-                 'rho_residual': rho_r,\r\n-                 'vx_residual': vx_r,\r\n-                 'vy_residual': vy_r,\r\n-                 'phi_residual': phi_r\r\n+                 'rho_residual': rho_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n+                 'vx_residual': vx_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n+                 'vy_residual': vy_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n+                 'vz_residual': vz_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n+                 'phi_residual': phi_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal)\r\n              }\r\n      \r",
        "oldStart": 219,
        "newStart": 239,
        "oldLines": 32,
        "newLines": 57,
        "linesRemoved": 20,
        "linesAdded": 45
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM training/training_diagnostics.py",
  "contentsAfterEdit": "import os\r\nimport numpy as np\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\n\r\nclass TrainingDiagnostics:\r\n    \"\"\"\r\n    Comprehensive diagnostics for PINN performance analysis.\r\n    \r\n    TWO DIAGNOSTIC MODES:\r\n    \r\n    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t&gt;0)\r\n       Focus: Why does PINN fail at long times?\r\n       Generates 6 diagnostic plots:\r\n         - Training diagnostics: Loss convergence and balance\r\n         - PDE residual evolution: When/where/which equations fail\r\n         - Conservation violations: Mass and momentum drift\r\n         - Temporal error accumulation: How errors compound over time\r\n         - Spectral bias analysis: Frequency damping and mode evolution\r\n         - Solution stability: Detecting unphysical behavior\r\n    \r\n    2. IC FITTING MODE (3D power spectrum only)\r\n       Focus: Why can't PINN fit complex initial conditions?\r\n       Generates 5 diagnostic plots:\r\n         - Training diagnostics: Loss convergence with IC component breakdown\r\n         - IC spatial comparison: Predicted vs true IC fields\r\n         - IC power spectrum: Scale-by-scale fit quality\r\n         - IC component convergence: Which component fails to converge\r\n         - IC metrics summary: Correlation and RMS error statistics\r\n    \r\n    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True\r\n    \"\"\"\r\n\r\n    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):\r\n        self.save_dir = save_dir\r\n        self.dimension = dimension\r\n        self.perturbation_type = str(perturbation_type).lower()\r\n        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')\r\n        \r\n        os.makedirs(save_dir, exist_ok=True)\r\n\r\n        # Storage for tracking metrics over iterations\r\n        self.history = {\r\n            'iteration': [],\r\n            'total_loss': [],\r\n            'pde_loss': [],\r\n            'ic_loss': [],\r\n            'mean_rho': [],\r\n            'max_rho': [],\r\n        }\r\n        \r\n        # Additional tracking for 3D power spectrum IC diagnostics\r\n        if self.is_3d_power_spectrum:\r\n            self.history.update({\r\n                'ic_rho': [],\r\n                'ic_vx': [],\r\n                'ic_vy': [],\r\n                'ic_vz': [],\r\n                'ic_phi': [],\r\n            })\r\n\r\n    def _prepare_inputs(self, geomtime_col):\r\n        \"\"\"Normalize collocation inputs to the model's expected format.\"\"\"\r\n        if isinstance(geomtime_col, (list, tuple)):\r\n            return geomtime_col\r\n        # Single tensor [N, D] -&gt; list of [N,1]\r\n        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]\r\n\r\n    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):\r\n        \"\"\"\r\n        Call this every N iterations during training.\r\n        \r\n        Args:\r\n            iteration: Current iteration number\r\n            model: PINN model\r\n            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses\r\n            geomtime_col: Collocation points\r\n            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum\r\n        \"\"\"\r\n        self.history['iteration'].append(iteration)\r\n        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))\r\n        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))\r\n        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))\r\n\r\n        # Compute diagnostics from current model state\r\n        with torch.no_grad():\r\n            inputs = self._prepare_inputs(geomtime_col)\r\n            pred = model(inputs)\r\n            # Use first channel as density/log-density proxy\r\n            rho = pred[:, 0].detach().cpu().numpy()\r\n\r\n            # Density statistics\r\n            self.history['mean_rho'].append(np.nanmean(rho))\r\n            self.history['max_rho'].append(np.nanmax(rho))\r\n        \r\n        # Track component IC losses for 3D power spectrum\r\n        if self.is_3d_power_spectrum and ic_component_losses is not None:\r\n            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))\r\n            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))\r\n            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))\r\n            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))\r\n            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))\r\n\r\n    def plot_diagnostics(self, iteration=None):\r\n        \"\"\"\r\n        Generate streamlined training diagnostic plot.\r\n        Combined loss components and balance in one figure.\r\n        \"\"\"\r\n        if len(self.history['iteration']) == 0:\r\n            print(\"No diagnostic data to plot.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        suffix = f\"_iter_{iteration}\" if iteration is not None else \"_final\"\r\n\r\n        # Combined training diagnostics plot\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        # Subplot 1: Loss evolution\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')\r\n        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')\r\n        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')\r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 2: Loss balance ratio\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ic_arr = np.array(self.history['ic_loss'])\r\n        pde_arr = np.array(self.history['pde_loss'])\r\n        ratio = pde_arr / (ic_arr + 1e-10)\r\n        ax2.plot(iters, ratio, 'purple', linewidth=2)\r\n        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')\r\n        ax2.set_xlabel('Iteration', fontsize=11)\r\n        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)\r\n        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Subplot 3: Density evolution\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')\r\n        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')\r\n        ax3.set_xlabel('Iteration', fontsize=11)\r\n        ax3.set_ylabel('Density', fontsize=11)\r\n        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n        \r\n        print(f\"Training diagnostic plot saved to {self.save_dir}\")\r\n\r\n    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================\r\n    \r\n    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):\r\n        \"\"\"\r\n        Compute PDE residuals on a regular grid over space-time using CHUNKING to avoid OOM.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Determine grid size\r\n        if n_spatial is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_spatial = 80\r\n        \r\n        print(f\"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid (Chunked)...\")\r\n        \r\n        if dimension == 2:\r\n            x = torch.linspace(0, 1, n_spatial, device=device)\r\n            y = torch.linspace(0, 1, n_spatial, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            \r\n            # Flatten all points: (N_total, 1)\r\n            X_flat = X.reshape(-1, 1)\r\n            Y_flat = Y.reshape(-1, 1)\r\n            T_flat = T.reshape(-1, 1)\r\n            \r\n            total_points = X_flat.shape[0]\r\n            \r\n            # Initialize result arrays (on CPU to save GPU memory)\r\n            rho_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vx_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vy_res_all = np.zeros(total_points, dtype=np.float32)\r\n            phi_res_all = np.zeros(total_points, dtype=np.float32)\r\n            \r\n            # Process in chunks\r\n            chunk_size = 40000\r\n            model.eval()\r\n            \r\n            for i in range(0, total_points, chunk_size):\r\n                end_idx = min(i + chunk_size, total_points)\r\n                \r\n                x_chunk = X_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                y_chunk = Y_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                t_chunk = T_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                \r\n                colloc_chunk = [x_chunk, y_chunk, t_chunk]\r\n                \r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc_chunk, model, dimension=2)\r\n                \r\n                rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()\r\n                vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()\r\n                vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()\r\n                phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()\r\n                \r\n                del x_chunk, y_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, phi_r\r\n                if device.type == \"cuda\":\r\n                    torch.cuda.empty_cache()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n                'vx_residual': vx_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n                'vy_residual': vy_res_all.reshape(n_spatial, n_spatial, n_temporal),\r\n                'phi_residual': phi_res_all.reshape(n_spatial, n_spatial, n_temporal)\r\n            }\r\n        \r\n        elif dimension == 3:\r\n            # For 3D, use coarser grid to save memory\r\n            n_spatial_3d = max(32, n_spatial // 2)\r\n            x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n            t = torch.linspace(0, tmax, n_temporal, device=device)\r\n            \r\n            z_slice_val = 0.5\r\n            Z_slice = torch.full((n_spatial_3d, n_spatial_3d, n_temporal), z_slice_val, device=device)\r\n            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')\r\n            \r\n            X_flat = X.reshape(-1, 1)\r\n            Y_flat = Y.reshape(-1, 1)\r\n            Z_flat = Z_slice.reshape(-1, 1)\r\n            T_flat = T.reshape(-1, 1)\r\n            \r\n            total_points = X_flat.shape[0]\r\n            \r\n            rho_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vx_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vy_res_all = np.zeros(total_points, dtype=np.float32)\r\n            vz_res_all = np.zeros(total_points, dtype=np.float32)\r\n            phi_res_all = np.zeros(total_points, dtype=np.float32)\r\n            \r\n            chunk_size = 20000\r\n            model.eval()\r\n            \r\n            for i in range(0, total_points, chunk_size):\r\n                end_idx = min(i + chunk_size, total_points)\r\n                \r\n                x_chunk = X_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                y_chunk = Y_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                z_chunk = Z_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                t_chunk = T_flat[i:end_idx].detach().clone().requires_grad_(True)\r\n                \r\n                colloc_chunk = [x_chunk, y_chunk, z_chunk, t_chunk]\r\n                \r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc_chunk, model, dimension=3)\r\n                \r\n                rho_res_all[i:end_idx] = rho_r.detach().cpu().numpy().flatten()\r\n                vx_res_all[i:end_idx] = vx_r.detach().cpu().numpy().flatten()\r\n                vy_res_all[i:end_idx] = vy_r.detach().cpu().numpy().flatten()\r\n                vz_res_all[i:end_idx] = vz_r.detach().cpu().numpy().flatten()\r\n                phi_res_all[i:end_idx] = phi_r.detach().cpu().numpy().flatten()\r\n                \r\n                del x_chunk, y_chunk, z_chunk, t_chunk, colloc_chunk, rho_r, vx_r, vy_r, vz_r, phi_r\r\n                if device.type == \"cuda\":\r\n                    torch.cuda.empty_cache()\r\n            \r\n            return {\r\n                'x': x.cpu().numpy(),\r\n                'y': y.cpu().numpy(),\r\n                't': t.cpu().numpy(),\r\n                'rho_residual': rho_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'vx_residual': vx_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'vy_residual': vy_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'vz_residual': vz_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal),\r\n                'phi_residual': phi_res_all.reshape(n_spatial_3d, n_spatial_3d, n_temporal)\r\n            }\r\n    \r\n    def plot_residual_heatmaps(self, residual_data, slice_idx=None):\r\n        \"\"\"\r\n        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.\r\n        Shows WHEN and WHERE each equation fails.\r\n        \"\"\"\r\n        if slice_idx is None:\r\n            slice_idx = residual_data['rho_residual'].shape[1] // 2\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        residuals = [\r\n            ('Continuity Equation', residual_data['rho_residual']),\r\n            ('Momentum X Equation', residual_data['vx_residual']),\r\n            ('Momentum Y Equation', residual_data['vy_residual']),\r\n            ('Poisson Equation', residual_data['phi_residual'])\r\n        ]\r\n        \r\n        for idx, (name, resid) in enumerate(residuals):\r\n            ax = fig.add_subplot(gs[idx // 2, idx % 2])\r\n            \r\n            # Slice at fixed y\r\n            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow\r\n            \r\n            # Use log scale for better visualization\r\n            resid_log = np.log10(np.abs(resid_slice) + 1e-16)\r\n            actual_min = np.min(resid_log)\r\n            data_max = np.max(resid_log)\r\n            data_min = max(data_max - 4, -12)\r\n            \r\n            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',\r\n                          extent=[residual_data['x'][0], residual_data['x'][-1],\r\n                                 residual_data['t'][0], residual_data['t'][-1]],\r\n                          vmin=data_min, vmax=data_max)\r\n            ax.set_xlabel('x', fontsize=11)\r\n            ax.set_ylabel('Time', fontsize=11)\r\n            ax.set_title(\r\n                f'{name} Residual\\nMin: 10^{actual_min:.1f}, Max: 10^{data_max:.1f}',\r\n                fontsize=12,\r\n                fontweight='bold'\r\n            )\r\n            cbar = plt.colorbar(im, ax=ax)\r\n            cbar.set_label('log₁₀|residual|', fontsize=10)\r\n        \r\n        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data[\"y\"][slice_idx]:.2f})', \r\n                    fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):\r\n        \"\"\"\r\n        Check if mass and momentum are conserved over time.\r\n        Critical for physical consistency - should be flat lines!\r\n        \r\n        Note: Uses higher resolution for power spectrum to ensure accurate integration.\r\n        \"\"\"\r\n        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D\r\n        \r\n        device = next(model.parameters()).device\r\n        \r\n        # Use higher resolution for power spectrum\r\n        if n_grid is None:\r\n            if str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\r\n                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)\r\n            else:\r\n                n_grid = 100\r\n        \r\n        print(f\"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...\")\r\n        \r\n        conservation_data = {\r\n            'times': [],\r\n            'total_mass': [],\r\n            'total_momentum_x': [],\r\n            'total_momentum_y': []\r\n        }\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # Integrate using trapezoidal rule\r\n                dx = 1.0 / (n_grid - 1)\r\n                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, use coarser grid\r\n                n_grid_3d = max(40, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                z = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                dx = 1.0 / (n_grid_3d - 1)\r\n                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)\r\n                \r\n                conservation_data['times'].append(t_val)\r\n                conservation_data['total_mass'].append(total_mass)\r\n                conservation_data['total_momentum_x'].append(total_px)\r\n                conservation_data['total_momentum_y'].append(total_py)\r\n        \r\n        return conservation_data\r\n    \r\n    def plot_conservation_laws(self, conservation_data):\r\n        \"\"\"Plot conserved quantities over time - should be flat lines!\"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = conservation_data['times']\r\n        \r\n        # Mass conservation\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        mass = np.array(conservation_data['total_mass'])\r\n        mass_initial = mass[0]\r\n        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift\r\n        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)\r\n        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Mass drift (%)', fontsize=11)\r\n        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Momentum X conservation\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        px = np.array(conservation_data['total_momentum_x'])\r\n        px_initial = px[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(px_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum X absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            px_drift = ((px - px_initial) / abs(px_initial)) * 100\r\n            ylabel = 'Momentum X drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax2.plot(times, px_drift, 'g-', linewidth=2.5)\r\n        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel(ylabel, fontsize=11)\r\n        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Momentum Y conservation\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        py = np.array(conservation_data['total_momentum_y'])\r\n        py_initial = py[0]\r\n        \r\n        # Handle near-zero initial momentum properly\r\n        if abs(py_initial) &lt; 1e-10:\r\n            # Plot absolute change for near-zero initial momentum\r\n            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility\r\n            ylabel = 'Momentum Y absolute change (×10⁻³)'\r\n            title_suffix = ' (near-zero initial)'\r\n        else:\r\n            # Plot percentage change for non-zero initial momentum\r\n            py_drift = ((py - py_initial) / abs(py_initial)) * 100\r\n            ylabel = 'Momentum Y drift (%)'\r\n            title_suffix = ''\r\n        \r\n        ax3.plot(times, py_drift, 'orange', linewidth=2.5)\r\n        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel(ylabel, fontsize=11)\r\n        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):\r\n        \"\"\"\r\n        Compute FFT of density field at different times.\r\n        Reveals spectral bias and high-frequency mode damping.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing spectral content at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        spectra = []\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_grid, device=device)\r\n                y = torch.linspace(0, 1, n_grid, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n                \r\n                # 2D FFT\r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                # Create wavenumber grid\r\n                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5\r\n                n_grid_3d = max(64, n_grid // 2)\r\n                x = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                y = torch.linspace(0, 1, n_grid_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n                \r\n                with torch.no_grad():\r\n                    pred = model(colloc)\r\n                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()\r\n                \r\n                fft = np.fft.fft2(rho)\r\n                power = np.abs(np.fft.fftshift(fft))**2\r\n                \r\n                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)\r\n                kx_shift = np.fft.fftshift(kx)\r\n                ky_shift = np.fft.fftshift(ky)\r\n                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n                K = np.sqrt(KX**2 + KY**2)\r\n                \r\n                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})\r\n        \r\n        return spectra\r\n    \r\n    def plot_spectral_evolution(self, spectra_data):\r\n        \"\"\"\r\n        Plot power spectrum evolution over time.\r\n        Shows if high-frequency modes are being damped (spectral bias).\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        # Left: Radially averaged power spectra\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        \r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], \r\n                          linewidth=2, markersize=4, label=f\"t={spec['time']:.2f}\")\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Right: 2D power spectrum at final time\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        final_spec = spectra_data[-1]\r\n        power_log = np.log10(final_spec['power'] + 1e-12)\r\n        \r\n        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',\r\n                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),\r\n                              -np.max(final_spec['k']), np.max(final_spec['k'])])\r\n        ax2.set_xlabel('k_x', fontsize=11)\r\n        ax2.set_ylabel('k_y', fontsize=11)\r\n        ax2.set_title(f'2D Power Spectrum at t={final_spec[\"time\"]:.2f}', \r\n                     fontsize=12, fontweight='bold')\r\n        cbar = plt.colorbar(im, ax=ax2)\r\n        cbar.set_label('log₁₀(Power)', fontsize=10)\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):\r\n        \"\"\"\r\n        Compute field statistics over time to track error accumulation.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing temporal statistics at {n_times} time points...\")\r\n        \r\n        time_points = np.linspace(0, tmax, n_times)\r\n        stats = {\r\n            'times': [],\r\n            'rho_mean': [],\r\n            'rho_std': [],\r\n            'rho_max': [],\r\n            'grad_rho_mean': [],\r\n            'grad_rho_max': []\r\n        }\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients (retain_graph=True for first call since we need multiple gradients)\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n            \r\n            elif dimension == 3:\r\n                # For 3D, take a 2D slice at z=0.5 for computational efficiency\r\n                n_spatial_3d = max(50, n_spatial // 2)\r\n                x = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                y = torch.linspace(0, 1, n_spatial_3d, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1)\r\n                Y_flat = Y.reshape(-1, 1)\r\n                Z_flat = Z.reshape(-1, 1)\r\n                T_flat = T.reshape(-1, 1)\r\n                \r\n                X_flat.requires_grad_(True)\r\n                Y_flat.requires_grad_(True)\r\n                Z_flat.requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n                \r\n                pred = model(colloc)\r\n                rho = pred[:, 0:1]\r\n                \r\n                # Compute gradients\r\n                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]\r\n                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]\r\n                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)\r\n                \r\n                rho_np = rho.detach().cpu().numpy().flatten()\r\n                grad_np = grad_mag.detach().cpu().numpy().flatten()\r\n                \r\n                stats['times'].append(t_val)\r\n                stats['rho_mean'].append(np.mean(rho_np))\r\n                stats['rho_std'].append(np.std(rho_np))\r\n                stats['rho_max'].append(np.max(rho_np))\r\n                stats['grad_rho_mean'].append(np.mean(grad_np))\r\n                stats['grad_rho_max'].append(np.max(grad_np))\r\n        \r\n        return stats\r\n    \r\n    def plot_temporal_statistics(self, stats_data):\r\n        \"\"\"\r\n        Plot field statistics over time.\r\n        Shows if solution becomes unphysical or gradients explode.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        times = stats_data['times']\r\n        \r\n        # Density statistics\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax1.fill_between(times,\r\n                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),\r\n                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),\r\n                         alpha=0.3, color='b', label='±1 std')\r\n        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Gradient statistics\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')\r\n        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)\r\n        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=10)\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Growth rate analysis\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        rho_max = np.array(stats_data['rho_max'])\r\n        if len(rho_max) &gt; 5:\r\n            # Fit exponential growth\r\n            log_rho = np.log(rho_max + 1e-10)\r\n            valid_idx = np.isfinite(log_rho)\r\n            if valid_idx.sum() &gt; 5:\r\n                times_valid = np.array(times)[valid_idx]\r\n                log_rho_valid = log_rho[valid_idx]\r\n                coeffs = np.polyfit(times_valid, log_rho_valid, 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                \r\n                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, \r\n                           label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.set_xlabel('Time', fontsize=11)\r\n                ax3.set_ylabel('Max Density (log scale)', fontsize=11)\r\n                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')\r\n                ax3.legend(fontsize=10)\r\n                ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):\r\n        \"\"\"\r\n        Track how prediction errors grow and accumulate over time.\r\n        Helps identify when the solution starts diverging from physical behavior.\r\n        \r\n        Now computes ACTUAL PDE residuals and gradient norms including phi.\r\n        \"\"\"\r\n        from core.losses import pde_residue\r\n        \r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_times)\r\n        \r\n        error_data = {\r\n            'times': time_points,\r\n            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},\r\n            'field_ranges': {'rho': [], 'vx': [], 'vy': []},\r\n            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},\r\n            'max_residuals': []\r\n        }\r\n        \r\n        if dimension == 3:\r\n            error_data['pde_residuals']['momentum_z'] = []\r\n            error_data['field_ranges']['vz'] = []\r\n            error_data['gradient_norms']['vz'] = []\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, T_flat]\r\n            else:  # dimension == 3\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                \r\n                X_flat = X.reshape(-1, 1).requires_grad_(True)\r\n                Y_flat = Y.reshape(-1, 1).requires_grad_(True)\r\n                Z_flat = Z.reshape(-1, 1).requires_grad_(True)\r\n                T_flat = T.reshape(-1, 1).requires_grad_(True)\r\n                \r\n                colloc = [X_flat, Y_flat, Z_flat, T_flat]\r\n            \r\n            # Compute ACTUAL PDE residuals using the proper residue function\r\n            if dimension == 2:\r\n                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n                \r\n            else:  # 3D\r\n                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)\r\n                \r\n                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())\r\n                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())\r\n                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())\r\n                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())\r\n                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())\r\n            \r\n            # Get predictions for field statistics\r\n            pred = model(colloc)\r\n            rho = pred[:, 0:1]\r\n            vx = pred[:, 1:2]\r\n            vy = pred[:, 2:3]\r\n            phi = pred[:, -1:]  # Last column is phi\r\n            \r\n            # Track field statistics\r\n            error_data['field_ranges']['rho'].append(torch.std(rho).item())\r\n            error_data['field_ranges']['vx'].append(torch.std(vx).item())\r\n            error_data['field_ranges']['vy'].append(torch.std(vy).item())\r\n            if dimension == 3:\r\n                vz = pred[:, 3:4]\r\n                error_data['field_ranges']['vz'].append(torch.std(vz).item())\r\n            \r\n            # Track gradient norms for ALL fields including phi\r\n            with torch.enable_grad():\r\n                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                \r\n                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())\r\n                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())\r\n                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())\r\n                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())\r\n                \r\n                if dimension == 3:\r\n                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]\r\n                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())\r\n            \r\n            # Max residual magnitude (now including Poisson!)\r\n            max_res = max(\r\n                error_data['pde_residuals']['continuity'][-1],\r\n                error_data['pde_residuals']['momentum_x'][-1],\r\n                error_data['pde_residuals']['momentum_y'][-1],\r\n                error_data['pde_residuals']['poisson'][-1]\r\n            )\r\n            if dimension == 3:\r\n                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])\r\n            \r\n            error_data['max_residuals'].append(max_res)\r\n        \r\n        return error_data\r\n    \r\n    def plot_temporal_error_accumulation(self, error_data):\r\n        \"\"\"\r\n        Visualize how errors accumulate and grow over time.\r\n        Shows which equations fail first and error growth rates.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = error_data['times']\r\n        \r\n        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)\r\n        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)\r\n        # IMPORTANT: Now showing actual Poisson residuals!\r\n        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)\r\n        if 'momentum_z' in error_data['pde_residuals']:\r\n            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)\r\n        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=9, loc='best')\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Field Range Evolution (variance growth)\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)\r\n        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)\r\n        if 'vz' in error_data['field_ranges']:\r\n            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Field Standard Deviation', fontsize=11)\r\n        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')\r\n        ax2.legend(fontsize=9)\r\n        ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: Maximum Residual Growth\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)\r\n        if len(times) &gt; 5:\r\n            # Fit exponential growth\r\n            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)\r\n            valid_idx = np.isfinite(log_res)\r\n            if valid_idx.sum() &gt; 3:\r\n                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)\r\n                growth_rate = coeffs[0]\r\n                fit = np.exp(np.poly1d(coeffs)(times))\r\n                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')\r\n                ax3.legend(fontsize=10)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Max Residual (log)', fontsize=11)\r\n        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)\r\n        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)\r\n        if 'vz' in error_data['gradient_norms']:\r\n            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)\r\n        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax4.legend(fontsize=9, loc='best')\r\n        ax4.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 5: Cumulative Error Metric\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        cumulative_error = np.cumsum(error_data['max_residuals'])\r\n        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Cumulative Error', fontsize=11)\r\n        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3)\r\n        \r\n        # Plot 6: Error Growth Rate\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        if len(times) &gt; 1:\r\n            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)\r\n            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n            ax6.set_xlabel('Time', fontsize=11)\r\n            ax6.set_ylabel('Error Growth Rate', fontsize=11)\r\n            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')\r\n            ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_spectral_bias_analysis(self, spectra_data):\r\n        \"\"\"\r\n        Enhanced spectral analysis focused on detecting spectral bias.\r\n        Shows frequency damping and mode evolution over time.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: Radially averaged power spectra at different times\r\n        ax1 = fig.add_subplot(gs[0, :2])\r\n        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))\r\n        \r\n        for idx, spec in enumerate(spectra_data):\r\n            # Radial binning\r\n            k_max = np.max(spec['k'])\r\n            k_bins = np.linspace(0, k_max, 25)\r\n            power_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (spec['k'] &gt;= k_bins[i]) &amp; (spec['k'] &lt; k_bins[i+1])\r\n                if mask.sum() &gt; 0:\r\n                    power_avg.append(np.mean(spec['power'][mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,\r\n                          label=f't={spec[\"time\"]:.2f}', marker='o', markersize=3)\r\n        \r\n        ax1.set_xlabel('Wavenumber k', fontsize=11)\r\n        ax1.set_ylabel('Power P(k)', fontsize=11)\r\n        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=8, ncol=2)\r\n        ax1.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 2: Spectral Damping Rate\r\n        ax2 = fig.add_subplot(gs[0, 2])\r\n        if len(spectra_data) &gt;= 2:\r\n            # Compare first and last spectra\r\n            spec_0 = spectra_data[0]\r\n            spec_f = spectra_data[-1]\r\n            \r\n            # Compute power ratio (damping)\r\n            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))\r\n            k_bins = np.linspace(0, k_max, 20)\r\n            damping_ratio = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask_0 = (spec_0['k'] &gt;= k_bins[i]) &amp; (spec_0['k'] &lt; k_bins[i+1])\r\n                mask_f = (spec_f['k'] &gt;= k_bins[i]) &amp; (spec_f['k'] &lt; k_bins[i+1])\r\n                if mask_0.sum() &gt; 0 and mask_f.sum() &gt; 0:\r\n                    p0 = np.mean(spec_0['power'][mask_0])\r\n                    pf = np.mean(spec_f['power'][mask_f])\r\n                    if p0 &gt; 0:\r\n                        damping_ratio.append(pf / p0)\r\n                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)\r\n                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')\r\n                ax2.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)\r\n                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')\r\n                ax2.legend(fontsize=10)\r\n                ax2.grid(True, alpha=0.3)\r\n        \r\n        # Plot 3: High-k Mode Tracking\r\n        ax3 = fig.add_subplot(gs[1, 0])\r\n        times = [spec['time'] for spec in spectra_data]\r\n        \r\n        # Track power in different k ranges\r\n        low_k_power = []\r\n        mid_k_power = []\r\n        high_k_power = []\r\n        \r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            k_max = np.max(k_flat)\r\n            low_k_power.append(np.mean(p_flat[k_flat &lt; k_max/3]))\r\n            mid_k_power.append(np.mean(p_flat[(k_flat &gt;= k_max/3) &amp; (k_flat &lt; 2*k_max/3)]))\r\n            high_k_power.append(np.mean(p_flat[k_flat &gt;= 2*k_max/3]))\r\n        \r\n        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)\r\n        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)\r\n        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Average Power (log)', fontsize=11)\r\n        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')\r\n        ax3.legend(fontsize=10)\r\n        ax3.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 4: Spectral Index Evolution\r\n        ax4 = fig.add_subplot(gs[1, 1])\r\n        spectral_indices = []\r\n        for spec in spectra_data:\r\n            k_flat = spec['k'].flatten()\r\n            p_flat = spec['power'].flatten()\r\n            \r\n            # Fit power law in log space\r\n            valid = (k_flat &gt; 0) &amp; (p_flat &gt; 0)\r\n            if valid.sum() &gt; 10:\r\n                log_k = np.log(k_flat[valid])\r\n                log_p = np.log(p_flat[valid])\r\n                coeffs = np.polyfit(log_k, log_p, 1)\r\n                spectral_indices.append(coeffs[0])  # Slope = spectral index\r\n            else:\r\n                spectral_indices.append(np.nan)\r\n        \r\n        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)\r\n        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)\r\n        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Density field snapshots\r\n        ax5 = fig.add_subplot(gs[1, 2])\r\n        # Show first, middle, last density fields\r\n        n_show = min(3, len(spectra_data))\r\n        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]\r\n        \r\n        for i, idx in enumerate(indices[:n_show]):\r\n            rho = spectra_data[idx]['rho']\r\n            # Compute structure function or variance\r\n            rho_var = np.var(rho)\r\n            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)\r\n            ax5.text(i, rho_var, f't={spectra_data[idx][\"time\"]:.2f}', \r\n                    ha='center', va='bottom', fontsize=9)\r\n        \r\n        ax5.set_xlabel('Time Snapshot', fontsize=11)\r\n        ax5.set_ylabel('Density Variance', fontsize=11)\r\n        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')\r\n        ax5.set_xticks(range(n_show))\r\n        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])\r\n        ax5.grid(True, alpha=0.3, axis='y')\r\n        \r\n        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):\r\n        \"\"\"\r\n        Check for signs of solution instability or unphysical behavior.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        time_points = np.linspace(0, tmax, n_check)\r\n        \r\n        stability_data = {\r\n            'times': time_points,\r\n            'density_bounds': {'min': [], 'max': [], 'mean': []},\r\n            'velocity_norms': [],\r\n            'negative_density_fraction': [],\r\n            'extreme_values': [],\r\n            'solution_smoothness': []\r\n        }\r\n        \r\n        n_spatial = 80 if dimension == 2 else 60\r\n        \r\n        for t_val in time_points:\r\n            if dimension == 2:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n            else:\r\n                x = torch.linspace(0, 1, n_spatial, device=device)\r\n                y = torch.linspace(0, 1, n_spatial, device=device)\r\n                X, Y = torch.meshgrid(x, y, indexing='ij')\r\n                Z = torch.full_like(X, 0.5)\r\n                T = torch.full_like(X, t_val)\r\n                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n            \r\n            with torch.no_grad():\r\n                pred = model(colloc)\r\n                rho = pred[:, 0].cpu().numpy()\r\n                vx = pred[:, 1].cpu().numpy()\r\n                vy = pred[:, 2].cpu().numpy()\r\n                \r\n                # Density bounds\r\n                stability_data['density_bounds']['min'].append(np.min(rho))\r\n                stability_data['density_bounds']['max'].append(np.max(rho))\r\n                stability_data['density_bounds']['mean'].append(np.mean(rho))\r\n                \r\n                # Velocity norms\r\n                v_norm = np.sqrt(vx**2 + vy**2)\r\n                if dimension == 3:\r\n                    vz = pred[:, 3].cpu().numpy()\r\n                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)\r\n                stability_data['velocity_norms'].append(np.max(v_norm))\r\n                \r\n                # Negative density fraction\r\n                neg_frac = np.sum(rho &lt; 0) / len(rho)\r\n                stability_data['negative_density_fraction'].append(neg_frac)\r\n                \r\n                # Extreme value detection (more than 10x mean)\r\n                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) &gt; 10 * np.std(rho)) / len(rho)\r\n                stability_data['extreme_values'].append(extreme_frac)\r\n                \r\n                # Solution smoothness (Laplacian magnitude as proxy)\r\n                if dimension == 2:\r\n                    rho_2d = rho.reshape(n_spatial, n_spatial)\r\n                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + \r\n                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))\r\n                    stability_data['solution_smoothness'].append(np.mean(laplacian))\r\n                else:\r\n                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D\r\n        \r\n        return stability_data\r\n    \r\n    def plot_solution_stability(self, stability_data):\r\n        \"\"\"\r\n        Visualize solution stability metrics to detect unphysical behavior.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(18, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        times = stability_data['times']\r\n        \r\n        # Plot 1: Density Bounds\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)\r\n        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)\r\n        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')\r\n        ax1.set_xlabel('Time', fontsize=11)\r\n        ax1.set_ylabel('Density', fontsize=11)\r\n        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Plot 2: Velocity Norms\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)\r\n        ax2.set_xlabel('Time', fontsize=11)\r\n        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)\r\n        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 3: Negative Density Fraction\r\n        ax3 = fig.add_subplot(gs[0, 2])\r\n        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)\r\n        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)\r\n        ax3.set_xlabel('Time', fontsize=11)\r\n        ax3.set_ylabel('Negative Density (%)', fontsize=11)\r\n        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')\r\n        ax3.grid(True, alpha=0.3)\r\n        \r\n        # Plot 4: Extreme Values\r\n        ax4 = fig.add_subplot(gs[1, 0])\r\n        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)\r\n        ax4.set_xlabel('Time', fontsize=11)\r\n        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)\r\n        ax4.set_title('Outlier Detection (&gt;10σ)', fontsize=12, fontweight='bold')\r\n        ax4.grid(True, alpha=0.3)\r\n        \r\n        # Plot 5: Solution Smoothness\r\n        ax5 = fig.add_subplot(gs[1, 1])\r\n        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)\r\n        ax5.set_xlabel('Time', fontsize=11)\r\n        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)\r\n        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')\r\n        ax5.grid(True, alpha=0.3, which='both')\r\n        \r\n        # Plot 6: Stability Summary\r\n        ax6 = fig.add_subplot(gs[1, 2])\r\n        # Compute overall stability score (0-1, higher is worse)\r\n        neg_dens_score = np.array(stability_data['negative_density_fraction'])\r\n        extreme_score = np.array(stability_data['extreme_values'])\r\n        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score\r\n        \r\n        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)\r\n        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')\r\n        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')\r\n        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')\r\n        ax6.set_xlabel('Time', fontsize=11)\r\n        ax6.set_ylabel('Instability Score (%)', fontsize=11)\r\n        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')\r\n        ax6.legend(fontsize=9)\r\n        ax6.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_comprehensive_diagnostics(self, model, dimension, tmax):\r\n        \"\"\"\r\n        Run all post-training diagnostics for long-term evolution analysis.\r\n        \r\n        Generates 6 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail\r\n        3. Conservation law violations - Mass/momentum drift over time\r\n        4. Temporal error accumulation - How errors compound over time\r\n        5. Spectral bias analysis - Frequency damping and mode evolution\r\n        6. Solution stability - Detecting unphysical behavior\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            dimension: Spatial dimension (2 or 3)\r\n            tmax: Maximum time to analyze\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Long-Term Evolution Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: PDE Residual Evolution\r\n        print(\"\\n[1/5] Computing PDE residual evolution...\")\r\n        residuals = self.compute_residual_heatmap(model, dimension, tmax)\r\n        self.plot_residual_heatmaps(residuals)\r\n        print(\"      [OK] PDE residual evolution saved\")\r\n        \r\n        # Plot 3: Conservation Laws\r\n        print(\"\\n[2/5] Analyzing conservation law violations...\")\r\n        conservation = self.check_conservation_laws(model, dimension, tmax)\r\n        self.plot_conservation_laws(conservation)\r\n        print(\"      [OK] Conservation violations plot saved\")\r\n        \r\n        # Plot 4: Temporal Error Accumulation\r\n        print(\"\\n[3/5] Computing temporal error accumulation...\")\r\n        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)\r\n        self.plot_temporal_error_accumulation(error_data)\r\n        print(\"      [OK] Error accumulation plot saved\")\r\n        \r\n        # Plot 5: Spectral Bias Analysis\r\n        print(\"\\n[4/5] Analyzing spectral bias and mode damping...\")\r\n        spectra = self.compute_spectral_content(model, dimension, tmax)\r\n        self.plot_spectral_bias_analysis(spectra)\r\n        print(\"      [OK] Spectral bias analysis saved\")\r\n        \r\n        # Plot 6: Solution Stability\r\n        print(\"\\n[5/5] Checking solution stability...\")\r\n        stats = self.compute_temporal_statistics(model, dimension, tmax)\r\n        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)\r\n        self.plot_solution_stability(stability_data)\r\n        print(\"      [OK] Solution stability diagnostics saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nLong-Term Evolution Diagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")\r\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns\")\r\n        print(\"  3. conservation_laws.png - Conservation law drift over time\")\r\n        print(\"  4. temporal_error_accumulation.png - Error growth and compounding\")\r\n        print(\"  5. spectral_bias_analysis.png - Frequency damping and spectral issues\")\r\n        print(\"  6. solution_stability.png - Physical validity and stability metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n    \r\n    \r\n    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):\r\n        \"\"\"\r\n        Smart dispatcher: runs appropriate diagnostics based on case type.\r\n        \r\n        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics\r\n        - 3D power spectrum: IC fitting diagnostics  \r\n        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)\r\n            true_ic_data: Dict with IC data (required for 3D power spectrum)\r\n                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        if self.is_3d_power_spectrum:\r\n            # 3D power spectrum: Focus on IC fitting\r\n            if true_ic_data is None:\r\n                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")\r\n            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)\r\n        else:\r\n            # All other cases: Focus on temporal evolution\r\n            if tmax is None:\r\n                raise ValueError(\"tmax required for temporal evolution diagnostics\")\r\n            self.run_comprehensive_diagnostics(model, self.dimension, tmax)\r\n    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================\r\n    \r\n    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):\r\n        \"\"\"\r\n        Compare predicted vs true ICs spatially for both 2D and 3D cases.\r\n        Critical for diagnosing WHERE the network fails to fit the IC structure.\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC collocation points and true values\r\n                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}\r\n                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            n_grid: Grid resolution per dimension (for visualization grid)\r\n        \r\n        Returns:\r\n            Dict with predicted and true fields, plus errors\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing IC spatial comparison on training IC points...\")\r\n        \r\n        # Use the actual IC collocation points from training\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions at IC points\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n                phi_pred = pred[:, 4].cpu().numpy()\r\n            else:\r\n                phi_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # For visualization, create a regular grid\r\n        x = torch.linspace(0, 1, n_grid, device=device)\r\n        y = torch.linspace(0, 1, n_grid, device=device)\r\n        X, Y = torch.meshgrid(x, y, indexing='ij')\r\n        T = torch.zeros_like(X)\r\n        \r\n        if self.dimension == 3:\r\n            # For 3D, take z=0.5 slice\r\n            Z = torch.full_like(X, 0.5)\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]\r\n        else:\r\n            # For 2D\r\n            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]\r\n        \r\n        # Get predictions on visualization grid\r\n        with torch.no_grad():\r\n            pred_viz = model(colloc_viz)\r\n            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()\r\n            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()\r\n            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()\r\n            else:\r\n                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()\r\n        \r\n        # For \"true\" visualization, we need to interpolate from IC points to regular grid\r\n        # Extract spatial coordinates from IC collocation points (detach first)\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Find points near z=0.5 for visualization\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        if mask_z.sum() &gt; 100:  # Need enough points for interpolation\r\n            from scipy.interpolate import griddata\r\n            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n            X_viz = X.cpu().numpy()\r\n            Y_viz = Y.cpu().numpy()\r\n            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])\r\n            \r\n            # Interpolate with nearest neighbor fallback for NaN values\r\n            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n            \r\n            # For uniform density (all values the same), fill NaNs with the constant value\r\n            # For varying fields, use nearest neighbor interpolation for NaN regions\r\n            if np.isnan(rho_true_viz).any():\r\n                if np.std(rho_true[mask_z]) &lt; 1e-10:\r\n                    # Uniform field - fill with mean\r\n                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))\r\n                else:\r\n                    # Varying field - use nearest neighbor for gaps\r\n                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)\r\n            \r\n            if np.isnan(vx_true_viz).any():\r\n                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)\r\n            \r\n            if np.isnan(vy_true_viz).any():\r\n                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)\r\n            \r\n            if self.dimension == 3:\r\n                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)\r\n                if np.isnan(vz_true_viz).any():\r\n                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)\r\n                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)\r\n        else:\r\n            # Not enough points for interpolation, use predicted as reference\r\n            print(\"  [WARN] Not enough IC points for ground truth visualization\")\r\n            rho_true_viz = rho_pred_viz\r\n            vx_true_viz = vx_pred_viz\r\n            vy_true_viz = vy_pred_viz\r\n            if self.dimension == 3:\r\n                vz_true_viz = vz_pred_viz\r\n        \r\n        result = {\r\n            'x': x.cpu().numpy(),\r\n            'y': y.cpu().numpy(),\r\n            'rho_pred': rho_pred_viz,\r\n            'rho_true': rho_true_viz,\r\n            'vx_pred': vx_pred_viz,\r\n            'vx_true': vx_true_viz,\r\n            'vy_pred': vy_pred_viz,\r\n            'vy_true': vy_true_viz,\r\n            'phi_pred': phi_pred_viz,\r\n            # Also store scatter point data for metrics\r\n            'rho_pred_scatter': rho_pred,\r\n            'rho_true_scatter': rho_true,\r\n            'vx_pred_scatter': vx_pred,\r\n            'vx_true_scatter': vx_true,\r\n            'vy_pred_scatter': vy_pred,\r\n            'vy_true_scatter': vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            result.update({\r\n                'vz_pred': vz_pred_viz,\r\n                'vz_true': vz_true_viz,\r\n                'vz_pred_scatter': vz_pred,\r\n                'vz_true_scatter': vz_true,\r\n            })\r\n        \r\n        return result\r\n    \r\n    def plot_ic_power_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot power spectrum comparison for all velocity components.\r\n        Shows which scales (k-modes) are missing or damped.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(16, 5))\r\n        gs = GridSpec(1, 3, figure=fig)\r\n        \r\n        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def plot_ic_component_convergence(self):\r\n        \"\"\"\r\n        Plot IC component loss convergence over training.\r\n        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.\r\n        \"\"\"\r\n        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:\r\n            print(\"IC component tracking not available.\")\r\n            return\r\n        \r\n        iters = self.history['iteration']\r\n        \r\n        # Replace zeros and very small values with a minimum threshold for log plotting\r\n        def safe_log_data(data):\r\n            \"\"\"Replace zeros/negative values with small epsilon for log plotting.\"\"\"\r\n            data_array = np.array(data)\r\n            data_array = np.where(data_array &lt;= 0, 1e-12, data_array)\r\n            return data_array\r\n        \r\n        fig = plt.figure(figsize=(16, 10))\r\n        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\r\n        \r\n        # Plot 1: All IC components on same plot\r\n        ax1 = fig.add_subplot(gs[0, :])\r\n        \r\n        # Check if rho is zero/constant (uniform field case)\r\n        rho_is_uniform = np.max(self.history['ic_rho']) &lt; 1e-10\r\n        \r\n        if not rho_is_uniform:\r\n            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', \r\n                        marker='o', markersize=3, markevery=max(1, len(iters)//20))\r\n        else:\r\n            # Plot a dashed line at the bottom to indicate uniform/zero loss\r\n            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)\r\n        \r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', \r\n                    marker='s', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', \r\n                    marker='^', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', \r\n                    marker='d', markersize=3, markevery=max(1, len(iters)//20))\r\n        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', \r\n                    marker='v', markersize=3, markevery=max(1, len(iters)//20))\r\n        \r\n        ax1.set_xlabel('Iteration', fontsize=11)\r\n        ax1.set_ylabel('Loss (log scale)', fontsize=11)\r\n        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')\r\n        ax1.legend(fontsize=10, ncol=5)\r\n        ax1.grid(True, alpha=0.3)\r\n        \r\n        # Individual component plots\r\n        components = [\r\n            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),\r\n            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),\r\n            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),\r\n        ]\r\n        \r\n        for hist_key, title, grid_pos, is_uniform in components:\r\n            ax = fig.add_subplot(grid_pos)\r\n            if is_uniform:\r\n                # Show a flat line at bottom with annotation\r\n                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)\r\n                ax.text(0.5, 0.5, 'Uniform IC\\n(no spatial variation)', \r\n                       transform=ax.transAxes, ha='center', va='center',\r\n                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\r\n                ax.set_ylim([1e-13, 1e-10])\r\n            else:\r\n                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')\r\n            \r\n            ax.set_xlabel('Iteration', fontsize=10)\r\n            ax.set_ylabel('Loss (log scale)', fontsize=10)\r\n            ax.set_title(title, fontsize=11, fontweight='bold')\r\n            ax.grid(True, alpha=0.3)\r\n        \r\n        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)\r\n        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def compute_ic_correlation_metrics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Compute spatial correlation between predicted and true ICs.\r\n        High correlation = good spatial structure capture.\r\n        \"\"\"\r\n        print(f\"  Computing IC correlation metrics on training IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy().flatten()\r\n            vx_pred = pred[:, 1].cpu().numpy().flatten()\r\n            vy_pred = pred[:, 2].cpu().numpy().flatten()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy().flatten()\r\n        \r\n        # Get true ICs (detach in case they have gradients)\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()\r\n        if self.dimension == 3 and true_ic_data['vz'] is not None:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()\r\n        \r\n        # Compute correlations - handle uniform fields (zero variance)\r\n        def safe_corrcoef(pred, true):\r\n            \"\"\"Compute correlation, handling constant fields gracefully.\"\"\"\r\n            # Check if either field is constant\r\n            if np.std(pred) &lt; 1e-10 or np.std(true) &lt; 1e-10:\r\n                # For uniform fields, correlation is undefined\r\n                # If both are uniform and equal, perfect match (1.0)\r\n                # If different constants, poor match (use normalized error)\r\n                if np.std(pred) &lt; 1e-10 and np.std(true) &lt; 1e-10:\r\n                    # Both uniform - check if they match\r\n                    mean_diff = abs(np.mean(pred) - np.mean(true))\r\n                    return 1.0 if mean_diff &lt; 1e-6 else 0.0\r\n                else:\r\n                    # One uniform, one not - cannot capture structure\r\n                    return 0.0\r\n            else:\r\n                return np.corrcoef(pred, true)[0, 1]\r\n        \r\n        correlations = {\r\n            'rho': safe_corrcoef(rho_pred, rho_true),\r\n            'vx': safe_corrcoef(vx_pred, vx_true),\r\n            'vy': safe_corrcoef(vy_pred, vy_true),\r\n        }\r\n        \r\n        # Compute RMS errors\r\n        rms_errors = {\r\n            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),\r\n            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),\r\n            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)\r\n            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))\r\n        \r\n        return correlations, rms_errors\r\n    \r\n    def plot_ic_metrics_summary(self, correlations, rms_errors):\r\n        \"\"\"\r\n        Summary plot: correlation and RMS error for each IC component.\r\n        Quick visual diagnostic of IC fitting quality.\r\n        \"\"\"\r\n        fig = plt.figure(figsize=(14, 5))\r\n        gs = GridSpec(1, 2, figure=fig)\r\n        \r\n        components = ['rho', 'vx', 'vy', 'vz']\r\n        labels = ['ρ', 'vx', 'vy', 'vz']\r\n        \r\n        # Correlation plot\r\n        ax1 = fig.add_subplot(gs[0, 0])\r\n        corr_vals = [correlations[c] for c in components]\r\n        \r\n        # Handle NaN/inf correlations from uniform fields\r\n        corr_vals_safe = []\r\n        for val in corr_vals:\r\n            if np.isnan(val) or np.isinf(val):\r\n                corr_vals_safe.append(0.0)  # Show as 0 for visualization\r\n            else:\r\n                corr_vals_safe.append(val)\r\n        \r\n        colors = ['green' if c &gt; 0.9 else 'orange' if c &gt; 0.7 else 'red' for c in corr_vals_safe]\r\n        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')\r\n        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')\r\n        ax1.set_ylabel('Correlation Coefficient', fontsize=11)\r\n        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax1.set_ylim([0, 1.05])\r\n        ax1.legend(fontsize=10)\r\n        ax1.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):\r\n            height = bar.get_height()\r\n            if np.isnan(orig_val) or np.isinf(orig_val):\r\n                label_text = 'N/A\\n(uniform)'\r\n                fontsize = 8\r\n            else:\r\n                label_text = f'{val:.3f}'\r\n                fontsize = 10\r\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\r\n                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')\r\n        \r\n        # RMS Error plot\r\n        ax2 = fig.add_subplot(gs[0, 1])\r\n        rms_vals = [rms_errors[c] for c in components]\r\n        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\r\n        ax2.set_ylabel('RMS Error', fontsize=11)\r\n        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')\r\n        ax2.grid(True, alpha=0.3, axis='y')\r\n        \r\n        # Add value labels on bars\r\n        for bar, val in zip(bars2, rms_vals):\r\n            height = bar.get_height()\r\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\r\n                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\r\n        \r\n        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')\r\n        plt.close()\r\n    \r\n    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):\r\n        \"\"\"\r\n        Run diagnostics for 3D power spectrum case.\r\n        Focus on IC fitting quality, not temporal evolution.\r\n        \r\n        Generates 5 critical diagnostic plots:\r\n        1. Training diagnostics (loss, balance, density) - already generated\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum comparison (scale-by-scale)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running 3D Power Spectrum IC Diagnostics\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[1/4] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power Spectrum Comparison\r\n        print(\"\\n[2/4] Computing IC power spectrum comparison...\")\r\n        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n        self.plot_ic_power_spectrum_comparison(ps_data)\r\n        print(\"      [OK] IC power spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[3/4] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[4/4] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit\")\r\n        print(\"  4. ic_component_convergence.png - Which IC component fails\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        for comp in ['rho', 'vx', 'vy', 'vz']:\r\n            print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================\r\n    \r\n    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):\r\n        \"\"\"\r\n        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).\r\n        \r\n        The 5 plots are:\r\n        1. Training diagnostics (loss convergence and balance)\r\n        2. IC spatial comparison (predicted vs true fields)\r\n        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)\r\n        4. IC component convergence (which component fails)\r\n        5. IC metrics summary (correlation and RMS error)\r\n        \r\n        Args:\r\n            model: Trained PINN model\r\n            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}\r\n            final_iteration: Final iteration number\r\n        \"\"\"\r\n        print(\"\\n\" + \"=\"*70)\r\n        print(\"  Running Unified Training Diagnostics (5 plots)\")\r\n        print(\"=\"*70)\r\n        \r\n        # Plot 1: Training diagnostics\r\n        print(\"\\n[1/5] Plotting training diagnostics...\")\r\n        self.plot_diagnostics(final_iteration)\r\n        print(\"      [OK] Training diagnostics saved\")\r\n        \r\n        # Plot 2: IC Spatial Comparison\r\n        print(\"\\n[2/5] Computing IC spatial comparison...\")\r\n        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)\r\n        self.plot_ic_spatial_comparison(ic_data)\r\n        print(\"      [OK] IC spatial comparison saved\")\r\n        \r\n        # Plot 3: IC Power/Field Spectrum Comparison\r\n        print(\"\\n[3/5] Computing spectrum comparison...\")\r\n        if self.perturbation_type == 'power_spectrum':\r\n            # For power spectrum: compare power spectra\r\n            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)\r\n            self.plot_ic_power_spectrum_comparison(ps_data)\r\n        else:\r\n            # For sinusoidal: compute field spectra from predictions\r\n            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)\r\n            self.plot_field_spectrum_comparison(ps_data)\r\n        print(\"      [OK] Spectrum comparison saved\")\r\n        \r\n        # Plot 4: IC Component Convergence\r\n        print(\"\\n[4/5] Plotting IC component convergence...\")\r\n        self.plot_ic_component_convergence()\r\n        print(\"      [OK] IC component convergence saved\")\r\n        \r\n        # Plot 5: IC Metrics Summary\r\n        print(\"\\n[5/5] Computing IC metrics summary...\")\r\n        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)\r\n        self.plot_ic_metrics_summary(correlations, rms_errors)\r\n        print(\"      [OK] IC metrics summary saved\")\r\n        \r\n        print(\"\\n\" + \"=\"*70)\r\n        print(f\"  All diagnostics saved to: {self.save_dir}\")\r\n        print(\"=\"*70)\r\n        print(\"\\nDiagnostic Summary:\")\r\n        print(\"  1. training_diagnostics.png - Training convergence analysis\")\r\n        print(\"  2. ic_spatial_comparison.png - Predicted vs true IC fields\")\r\n        print(\"  3. ic_power_spectrum.png - Spectrum comparison\")\r\n        print(\"  4. ic_component_convergence.png - IC component convergence\")\r\n        print(\"  5. ic_metrics_summary.png - Correlation and RMS error metrics\")\r\n        print(\"=\"*70 + \"\\n\")\r\n        \r\n        # Print numerical summary\r\n        print(\"\\nIC Fitting Quality Summary:\")\r\n        print(\"-\" * 50)\r\n        print(f\"{'Component':&lt;15} {'Correlation':&lt;15} {'RMS Error':&lt;15}\")\r\n        print(\"-\" * 50)\r\n        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']\r\n        for comp in comps:\r\n            if comp in correlations:\r\n                print(f\"{comp:&lt;15} {correlations[comp]:&gt;14.4f} {rms_errors[comp]:&gt;14.6f}\")\r\n        print(\"-\" * 50)\r\n        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])\r\n        print(f\"{'Average':&lt;15} {avg_corr:&gt;14.4f}\")\r\n        print(\"-\" * 50 + \"\\n\")\r\n    \r\n    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):\r\n        \"\"\"\r\n        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.\r\n        Similar to power spectrum comparison but computed from actual fields.\r\n        \"\"\"\r\n        device = next(model.parameters()).device\r\n        \r\n        print(f\"  Computing field spectrum comparison on IC points...\")\r\n        \r\n        # Use IC collocation points\r\n        colloc_IC = true_ic_data['colloc_IC']\r\n        \r\n        # Get predictions\r\n        with torch.no_grad():\r\n            pred = model(colloc_IC)\r\n            rho_pred = pred[:, 0].cpu().numpy()\r\n            vx_pred = pred[:, 1].cpu().numpy()\r\n            vy_pred = pred[:, 2].cpu().numpy()\r\n            if self.dimension == 3:\r\n                vz_pred = pred[:, 3].cpu().numpy()\r\n        \r\n        # Get true ICs\r\n        rho_true = true_ic_data['rho'].detach().cpu().numpy()\r\n        vx_true = true_ic_data['vx'].detach().cpu().numpy()\r\n        vy_true = true_ic_data['vy'].detach().cpu().numpy()\r\n        if self.dimension == 3:\r\n            vz_true = true_ic_data['vz'].detach().cpu().numpy()\r\n        \r\n        # Need to interpolate to regular grid for FFT\r\n        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()\r\n        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()\r\n        \r\n        if self.dimension == 3:\r\n            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()\r\n            # Use points near z=0.5 for 2D slice\r\n            z_tolerance = 0.1\r\n            mask_z = np.abs(z_ic - 0.5) &lt; z_tolerance\r\n            if mask_z.sum() &lt; 100:\r\n                mask_z = np.ones(len(z_ic), dtype=bool)\r\n        else:\r\n            mask_z = np.ones(len(x_ic), dtype=bool)\r\n        \r\n        # Create regular grid for FFT\r\n        x_grid = np.linspace(0, 1, n_grid)\r\n        y_grid = np.linspace(0, 1, n_grid)\r\n        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')\r\n        \r\n        # Interpolate from scattered IC points to regular grid\r\n        from scipy.interpolate import griddata\r\n        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])\r\n        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])\r\n        \r\n        # Interpolate density\r\n        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Interpolate velocities\r\n        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        if self.dimension == 3:\r\n            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)\r\n        \r\n        # Compute power spectra\r\n        def compute_power_spectrum(field):\r\n            fft = np.fft.fft2(field)\r\n            power = np.abs(np.fft.fftshift(fft))**2\r\n            \r\n            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)\r\n            kx_shift = np.fft.fftshift(kx)\r\n            ky_shift = np.fft.fftshift(ky)\r\n            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')\r\n            K = np.sqrt(KX**2 + KY**2)\r\n            \r\n            return power, K\r\n        \r\n        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)\r\n        power_rho_true, _ = compute_power_spectrum(rho_true_grid)\r\n        \r\n        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)\r\n        power_vx_true, _ = compute_power_spectrum(vx_true_grid)\r\n        \r\n        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)\r\n        power_vy_true, _ = compute_power_spectrum(vy_true_grid)\r\n        \r\n        result = {\r\n            'K': K,\r\n            'power_rho_pred': power_rho_pred,\r\n            'power_rho_true': power_rho_true,\r\n            'power_vx_pred': power_vx_pred,\r\n            'power_vx_true': power_vx_true,\r\n            'power_vy_pred': power_vy_pred,\r\n            'power_vy_true': power_vy_true,\r\n        }\r\n        \r\n        if self.dimension == 3:\r\n            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)\r\n            power_vz_true, _ = compute_power_spectrum(vz_true_grid)\r\n            result['power_vz_pred'] = power_vz_pred\r\n            result['power_vz_true'] = power_vz_true\r\n        \r\n        return result\r\n    \r\n    def plot_field_spectrum_comparison(self, ps_data):\r\n        \"\"\"\r\n        Plot field spectrum comparison for sinusoidal cases.\r\n        Shows density and velocity spectra.\r\n        \"\"\"\r\n        if self.dimension == 3:\r\n            fig = plt.figure(figsize=(18, 5))\r\n            gs = GridSpec(1, 4, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]\r\n        else:\r\n            fig = plt.figure(figsize=(14, 5))\r\n            gs = GridSpec(1, 3, figure=fig)\r\n            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]\r\n        \r\n        for idx, (comp, label) in enumerate(components):\r\n            ax = fig.add_subplot(gs[0, idx])\r\n            \r\n            # Radial binning\r\n            K = ps_data['K']\r\n            power_pred = ps_data[f'power_{comp}_pred']\r\n            power_true = ps_data[f'power_{comp}_true']\r\n            \r\n            k_max = np.max(K)\r\n            k_bins = np.linspace(0, k_max, 30)\r\n            power_pred_avg = []\r\n            power_true_avg = []\r\n            k_centers = []\r\n            \r\n            for i in range(len(k_bins)-1):\r\n                mask = (K &gt;= k_bins[i]) &amp; (K &lt; k_bins[i+1])\r\n                if mask.any():\r\n                    power_pred_avg.append(np.mean(power_pred[mask]))\r\n                    power_true_avg.append(np.mean(power_true[mask]))\r\n                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)\r\n            \r\n            if len(k_centers) &gt; 0:\r\n                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)\r\n                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)\r\n                \r\n                ax.set_xlabel('Wavenumber k', fontsize=11)\r\n                ax.set_ylabel('Power', fontsize=11)\r\n                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')\r\n                ax.legend(fontsize=10)\r\n                ax.grid(True, alpha=0.3, which='both')\r\n        \r\n        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)\r\n        plt.tight_layout()\r\n        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')\r\n        plt.close()"
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 1 file</summary>
      
Lint paths:

- `training/training_diagnostics.py`

Lint results


**No lint errors found**
</details>
</tool-use>

---

Implemented chunked `compute_residual_heatmap` to avoid OOM while keeping gradients enabled. I also corrected the config import to the existing `PERTURBATION_TYPE` and added safe `torch.cuda.empty_cache()` only when on CUDA.

If you want different chunk sizes or want them configurable via `config.py`, I can wire that up.

---

_**User (2026-01-24 16:39Z)**_

the exact function was copy pasted from a different place so it might not be fully ok for my codebase. check if it works for my codebase

---

_**Agent (model claude-4.5-opus-high-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
The user wants me to verify that the chunked `compute_residual_heatmap` function works correctly with their codebase. I should read the current state of the function and check:

1. Import statements - does `PERTURBATION_TYPE` exist in config.py?
2. Does `pde_residue` function signature match what's expected?
3. Are there any other compatibility issues?

Let me read the relevant files to check.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: training/training_diagnostics.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: config.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="search" data-tool-name="grep">
<details>
<summary>Tool use: **grep** • Grep for "def pde_residue" in "core/losses.py" • 2 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `core/losses.py` | `def pde_residue(colloc, net, dimension = 1):` | L27 |
| `core/losses.py` | `def pde_residue_standard(colloc, net, dimension = 1):` | L41 |


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Let me check the full pde_residue function to see what it returns for 2D and 3D dimensions.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: core/losses.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Let me check the return statement for 3D to make sure we're capturing all 5 values correctly.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: core/losses.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
I've checked the codebase:

1. **Config imports** - The code now correctly uses `PERTURBATION_TYPE`, `N_GRID`, and `N_GRID_3D` which all exist in `config.py` ✓

2. **pde_residue function signature** - The function takes `(colloc, net, dimension)` and returns:
   - 2D: `rho_r, vx_r, vy_r, phi_r` (4 values) ✓
   - 3D: `rho_r, vx_r, vy_r, vz_r, phi_r` (5 values) ✓
   
   The current code correctly unpacks these.

3. **One issue I notice**: In the 3D case, the original code in diagnostics tracked `vz_residual` but the current function stores it. Let me check if the `plot_residual_heatmaps` function uses `vz_residual` in 3D case - if not, that's fine since it's already computed. But there might be an issue with the 3D return not including a 'z' key for the z coordinate.

Actually, looking at the 3D case more carefully:
- The function returns `'x': x.cpu().numpy(), 'y': y.cpu().numpy(), 't': t.cpu().numpy()` but for 3D it should probably also return `'z'` information since we're slicing at z=0.5.
- The `plot_residual_heatmaps` function only uses x, y, t coordinates for the extent, so it should work.

Let me check the plot_residual_heatmaps function to ensure it can handle both 2D and 3D return structures.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: training/training_diagnostics.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
I can see one issue: In 3D case, the function returns `vz_residual` but `plot_residual_heatmaps` only plots 4 residuals (continuity, momentum X, momentum Y, Poisson) and doesn't include momentum Z. This is by design since we're limited to a 2x2 grid, but I should mention this to the user.

Actually, let me verify all the compatibility issues:

1. ✅ **Config imports**: `PERTURBATION_TYPE`, `N_GRID`, `N_GRID_3D` all exist in config.py
2. ✅ **pde_residue signature**: Matches - takes `(colloc, net, dimension)` and returns correct tuples
3. ✅ **2D return**: 4 residuals (rho, vx, vy, phi) - correctly matched
4. ✅ **3D return**: 5 residuals (rho, vx, vy, vz, phi) - correctly unpacked
5. ⚠️ **3D plot limitation**: `vz_residual` is computed but not displayed in the 2x2 heatmap grid

The code should work correctly. Let me just note the 3D limitation to the user.</details></think>

---

I've verified the code against your codebase. Here's the compatibility check:

### ✅ **All Good:**

| Item | Status | Notes |
|------|--------|-------|
| Config imports | ✅ | `PERTURBATION_TYPE`, `N_GRID`, `N_GRID_3D` all exist in `config.py` |
| `pde_residue` signature | ✅ | Takes `(colloc, net, dimension)` as expected |
| 2D residual unpacking | ✅ | Returns `(rho_r, vx_r, vy_r, phi_r)` - 4 values |
| 3D residual unpacking | ✅ | Returns `(rho_r, vx_r, vy_r, vz_r, phi_r)` - 5 values |
| Chunking logic | ✅ | Properly detaches, clones, enables gradients |
| CUDA cache clearing | ✅ | Only runs on CUDA devices |

### ⚠️ **Minor Note (Not a Bug):**

For **3D cases**, the `vz_residual` (Momentum Z equation) is computed and stored but **not plotted** in `plot_residual_heatmaps` since it uses a 2×2 grid showing only:
- Continuity, Momentum X, Momentum Y, Poisson

If you want to see the Z-momentum residual for 3D, I can expand the plot to a 2×3 grid. Otherwise the current implementation is fully functional.

**The chunked function is compatible with your codebase and ready to use.**

---

_**User (2026-01-24 16:39Z)**_

Currently the eval mode begins after training plots are made. This kind of output is printed:
======================================================================
All diagnostic plots generated successfully!
Check ./diagnostics/ folder for:
  1. training_diagnostics.png - Training convergence
  2. residual_heatmaps.png - Spatiotemporal PDE violations
  3. conservation_laws.png - Physical consistency
  4. spectral_evolution.png - Frequency content
  5. temporal_statistics.png - Field evolution
======================================================================

Starting comprehensive GPU memory cleanup...
  Moved network 0 to cpu (eval mode)
  Deleted optimizer 0 state
  Deleted optimizer 1 state
  Deleted collocation data dict
GPU memory cleanup completed
Model moved back to GPU for visualization
Computing FD cache for 5x3 comparison table (5 time points)...
Computing FD data cache for 5 time points (optimized: single run to t=2.00)...

Can the training plots be made after shifting to eval mode?

---

_**Agent (model claude-4.5-sonnet-thinking, mode Ask)**_

<think><details><summary>Thought Process</summary>
The user is asking if the training plots can be made after shifting to eval mode. Let me understand the current flow:

1. Training plots are generated
2. Then GPU memory cleanup happens (moving to CPU, eval mode)
3. Then model is moved back to GPU for visualization

The user wants to know if the diagnostic plots can be made after the model is already in eval mode.

Let me check what the training diagnostics actually need. Looking at the `compute_residual_heatmap` function I just implemented, I can see:
- Line 204: `model.eval()` is called
- Line 261: `model.eval()` is called for 3D

So the diagnostic functions already put the model in eval mode! This means they can definitely be run after the model is already in eval mode.

The key question is: does anything in the diagnostic functions require the model to be in training mode? Let me think about what each diagnostic does:

1. `plot_diagnostics` - just plots loss history, no model evaluation needed
2. `compute_residual_heatmap` - explicitly calls `model.eval()` 
3. `check_conservation_laws` - uses `torch.no_grad()`, so eval mode is fine
4. `compute_spectral_content` - uses `torch.no_grad()`, so eval mode is fine
5. `compute_temporal_statistics` - needs gradients but eval mode is fine for autograd

So yes, all diagnostics can run in eval mode. In fact, they should run in eval mode for consistency (no dropout, batch norm in eval state, etc.).

The user should move the diagnostic generation to after the eval mode shift to avoid the back-and-forth of:
1. Generate diagnostics (internally uses eval)
2. Cleanup and move to CPU eval
3. Move back to GPU

Instead:
1. Cleanup and move to CPU eval
2. Move to GPU
3. Generate diagnostics (already in eval mode)</details></think>

---

Yes, absolutely! In fact, **the training plots should be made in eval mode** for better memory efficiency and consistency. 

Looking at your current `compute_residual_heatmap` function, it already explicitly calls `model.eval()` on lines 204 and 261. This means:

1. **The diagnostics already expect eval mode** - they're designed to work with it
2. **Making plots before cleanup is wasteful** - you're keeping training artifacts in memory during diagnostic generation
3. **Moving to eval mode is beneficial** - disables dropout/batch norm inconsistencies

### Recommended Flow:

**Current (inefficient):**
```
Training → Generate plots (internally switches to eval) → Cleanup & eval mode → Move to GPU → Visualization
```

**Better:**
```
Training → Cleanup & eval mode → Move to GPU → Generate plots (already in eval) → Visualization
```

### Why This Works:

All diagnostic functions are eval-mode safe:
- `compute_residual_heatmap`: Explicitly calls `model.eval()`
- `check_conservation_laws`: Uses `torch.no_grad()`  
- `compute_spectral_content`: Uses `torch.no_grad()`
- `compute_temporal_statistics`: Gradients work fine in eval mode
- `plot_diagnostics`: Only plots loss history (no model calls)

### What You Gain:

✅ No redundant eval mode switches  
✅ Lower memory during plot generation  
✅ Consistent model state (eval throughout diagnostics)  
✅ Cleaner code flow  

**You can safely move the diagnostic generation to after your cleanup block with no changes needed to the diagnostic code.**

---

_**User (2026-01-24 16:39Z)**_

Well then change the flow to Training → Cleanup & eval mode → Move to GPU → Generate plots (already in eval) → Visualization

---

_**Agent (model gpt-5.2-codex, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Searching for flow code locations**</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "diagnostic|diagnostics|plot_diagnostics|run_comprehensive_diagnostics|cleanup|memory cleanup|Moved network|GPU memory",
  "path": "",
  "outputMode": "content",
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "diagnostic|diagnostics|plot_diagnostics|run_comprehensive_diagnostics|cleanup|memory cleanup|Moved network|GPU memory",
    "outputMode": "content",
    "workspaceResults": {
      "c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations": {
        "content": {
          "matches": [
            {
              "file": ".\\config.py",
              "matches": [
                {
                  "lineNumber": 72,
                  "content": "# Training diagnostics"
                },
                {
                  "lineNumber": 73,
                  "content": "ENABLE_TRAINING_DIAGNOSTICS = True  # Enable automatic training diagnostics plots and logging"
                }
              ]
            },
            {
              "file": ".\\training\\training_diagnostics.py",
              "matches": [
                {
                  "lineNumber": 10,
                  "content": "    Comprehensive diagnostics for PINN performance analysis."
                },
                {
                  "lineNumber": 16,
                  "content": "       Generates 6 diagnostic plots:"
                },
                {
                  "lineNumber": 17,
                  "content": "         - Training diagnostics: Loss convergence and balance"
                },
                {
                  "lineNumber": 26,
                  "content": "       Generates 5 diagnostic plots:"
                },
                {
                  "lineNumber": 27,
                  "content": "         - Training diagnostics: Loss convergence with IC component breakdown"
                },
                {
                  "lineNumber": 33,
                  "content": "    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True"
                },
                {
                  "lineNumber": 36,
                  "content": "    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):"
                },
                {
                  "lineNumber": 54,
                  "content": "        # Additional tracking for 3D power spectrum IC diagnostics"
                },
                {
                  "lineNumber": 87,
                  "content": "        # Compute diagnostics from current model state"
                },
                {
                  "lineNumber": 106,
                  "content": "    def plot_diagnostics(self, iteration=None):"
                },
                {
                  "lineNumber": 108,
                  "content": "        Generate streamlined training diagnostic plot."
                },
                {
                  "lineNumber": 112,
                  "content": "            print(\"No diagnostic data to plot.\")"
                },
                {
                  "lineNumber": 118,
                  "content": "        # Combined training diagnostics plot"
                },
                {
                  "lineNumber": 157,
                  "content": "        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')"
                },
                {
                  "lineNumber": 160,
                  "content": "        print(f\"Training diagnostic plot saved to {self.save_dir}\")"
                },
                {
                  "lineNumber": 196,
                  "content": "            # Initialize result arrays (on CPU to save GPU memory)"
                },
                {
                  "lineNumber": 1278,
                  "content": "    def run_comprehensive_diagnostics(self, model, dimension, tmax):"
                },
                {
                  "lineNumber": 1280,
                  "content": "        Run all post-training diagnostics for long-term evolution analysis."
                },
                {
                  "lineNumber": 1282,
                  "content": "        Generates 6 critical diagnostic plots:"
                },
                {
                  "lineNumber": 1283,
                  "content": "        1. Training diagnostics (loss, balance, density) - already generated"
                },
                {
                  "lineNumber": 1328,
                  "content": "        print(\"      [OK] Solution stability diagnostics saved\")"
                },
                {
                  "lineNumber": 1331,
                  "content": "        print(f\"  All diagnostics saved to: {self.save_dir}\")"
                },
                {
                  "lineNumber": 1334,
                  "content": "        print(\"  1. training_diagnostics.png - Training convergence and loss balance\")"
                },
                {
                  "lineNumber": 1343,
                  "content": "    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):"
                },
                {
                  "lineNumber": 1345,
                  "content": "        Smart dispatcher: runs appropriate diagnostics based on case type."
                },
                {
                  "lineNumber": 1347,
                  "content": "        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics"
                },
                {
                  "lineNumber": 1348,
                  "content": "        - 3D power spectrum: IC fitting diagnostics  "
                },
                {
                  "lineNumber": 1349,
                  "content": "        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)"
                },
                {
                  "lineNumber": 1360,
                  "content": "                raise ValueError(\"true_ic_data required for 3D power spectrum diagnostics\")"
                },
                {
                  "lineNumber": 1361,
                  "content": "            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)"
                },
                {
                  "lineNumber": 1365,
                  "content": "                raise ValueError(\"tmax required for temporal evolution diagnostics\")"
                },
                {
                  "lineNumber": 1366,
                  "content": "            self.run_comprehensive_diagnostics(model, self.dimension, tmax)"
                },
                {
                  "lineNumber": 1712,
                  "content": "        Quick visual diagnostic of IC fitting quality."
                },
                {
                  "lineNumber": 1773,
                  "content": "    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):"
                },
                {
                  "lineNumber": 1775,
                  "content": "        Run diagnostics for 3D power spectrum case."
                },
                {
                  "lineNumber": 1778,
                  "content": "        Generates 5 critical diagnostic plots:"
                },
                {
                  "lineNumber": 1779,
                  "content": "        1. Training diagnostics (loss, balance, density) - already generated"
                },
                {
                  "lineNumber": 1817,
                  "content": "        print(f\"  All diagnostics saved to: {self.save_dir}\")"
                },
                {
                  "lineNumber": 1820,
                  "content": "        print(\"  1. training_diagnostics.png - Training convergence analysis\")"
                },
                {
                  "lineNumber": 1841,
                  "content": "    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):"
                },
                {
                  "lineNumber": 1843,
                  "content": "        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation)."
                },
                {
                  "lineNumber": 1846,
                  "content": "        1. Training diagnostics (loss convergence and balance)"
                },
                {
                  "lineNumber": 1861,
                  "content": "        # Plot 1: Training diagnostics"
                },
                {
                  "lineNumber": 1862,
                  "content": "        print(\"\\n[1/5] Plotting training diagnostics...\")"
                },
                {
                  "lineNumber": 1863,
                  "content": "        self.plot_diagnostics(final_iteration)"
                },
                {
                  "lineNumber": 1864,
                  "content": "        print(\"      [OK] Training diagnostics saved\")"
                },
                {
                  "lineNumber": 1896,
                  "content": "        print(f\"  All diagnostics saved to: {self.save_dir}\")"
                },
                {
                  "lineNumber": 1899,
                  "content": "        print(\"  1. training_diagnostics.png - Training convergence analysis\")"
                }
              ]
            },
            {
              "file": ".\\train.py",
              "matches": [
                {
                  "lineNumber": 58,
                  "content": "def _comprehensive_gpu_cleanup(nets, optimizers=None, collocation_data=None, "
                },
                {
                  "lineNumber": 62,
                  "content": "    Comprehensive GPU memory cleanup after PINN training."
                },
                {
                  "lineNumber": 64,
                  "content": "    This function properly frees GPU memory by:"
                },
                {
                  "lineNumber": 88,
                  "content": "    print(\"Starting comprehensive GPU memory cleanup...\")"
                },
                {
                  "lineNumber": 106,
                  "content": "                    print(f\"  Moved network {i} to {target_device} (eval mode)\")"
                },
                {
                  "lineNumber": 177,
                  "content": "    print(\"GPU memory cleanup completed\")"
                },
                {
                  "lineNumber": 215,
                  "content": "            except Exception as exc:  # pragma: no cover - cleanup best effort"
                },
                {
                  "lineNumber": 225,
                  "content": "                except Exception as exc:  # pragma: no cover - cleanup best effort"
                },
                {
                  "lineNumber": 242,
                  "content": "    # Prevent the rest of the training script from running in cleanup-only mode"
                },
                {
                  "lineNumber": 249,
                  "content": "# Clear GPU memory if using CUDA"
                },
                {
                  "lineNumber": 339,
                  "content": "    training_diagnostics = train_multistage("
                },
                {
                  "lineNumber": 362,
                  "content": "    training_diagnostics = train("
                },
                {
                  "lineNumber": 389,
                  "content": "# Run comprehensive diagnostics for high-tmax failure analysis"
                },
                {
                  "lineNumber": 390,
                  "content": "# This generates 4 additional critical plots beyond the training diagnostics"
                },
                {
                  "lineNumber": 391,
                  "content": "if ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:"
                },
                {
                  "lineNumber": 393,
                  "content": "    print(\"Running post-training comprehensive diagnostics...\")"
                },
                {
                  "lineNumber": 397,
                  "content": "        # Run comprehensive diagnostics - generates 4 additional plots:"
                },
                {
                  "lineNumber": 402,
                  "content": "        training_diagnostics.run_comprehensive_diagnostics("
                },
                {
                  "lineNumber": 409,
                  "content": "        print(\"All diagnostic plots generated successfully!\")"
                },
                {
                  "lineNumber": 410,
                  "content": "        print(\"Check ./diagnostics/ folder for:\")"
                },
                {
                  "lineNumber": 411,
                  "content": "        print(\"  1. training_diagnostics.png - Training convergence\")"
                },
                {
                  "lineNumber": 418,
                  "content": "        print(f\"[WARNING] Post-training diagnostics failed: {e}\")"
                },
                {
                  "lineNumber": 419,
                  "content": "        print(\"Training completed successfully, but diagnostic plots may be incomplete.\")"
                },
                {
                  "lineNumber": 423,
                  "content": "# Comprehensive GPU memory cleanup after training"
                },
                {
                  "lineNumber": 426,
                  "content": "    # Collect all training-related data for cleanup"
                },
                {
                  "lineNumber": 427,
                  "content": "    cleanup_optimizers = [optimizer, optimizerL] if 'optimizer' in locals() and 'optimizerL' in locals() else None"
                },
                {
                  "lineNumber": 431,
                  "content": "        cleanup_collocation = {"
                },
                {
                  "lineNumber": 437,
                  "content": "        cleanup_collocation = None"
                },
                {
                  "lineNumber": 439,
                  "content": "    # Perform comprehensive cleanup"
                },
                {
                  "lineNumber": 440,
                  "content": "    # This frees GPU memory by deleting optimizer state and collocation data"
                },
                {
                  "lineNumber": 441,
                  "content": "    # Models are temporarily moved to CPU during cleanup, then moved back to GPU for visualization"
                },
                {
                  "lineNumber": 442,
                  "content": "    _comprehensive_gpu_cleanup("
                },
                {
                  "lineNumber": 444,
                  "content": "        optimizers=cleanup_optimizers,"
                },
                {
                  "lineNumber": 445,
                  "content": "        collocation_data=cleanup_collocation,"
                },
                {
                  "lineNumber": 448,
                  "content": "        target_device='cpu'  # Temporarily move models to CPU during cleanup"
                },
                {
                  "lineNumber": 476,
                  "content": "print(\"Performing final Python cache cleanup...\")"
                }
              ]
            },
            {
              "file": ".\\training\\trainer.py",
              "matches": [
                {
                  "lineNumber": 9,
                  "content": "from training.training_diagnostics import TrainingDiagnostics"
                },
                {
                  "lineNumber": 16,
                  "content": "    Compute individual IC component losses for 3D power spectrum diagnostics."
                },
                {
                  "lineNumber": 80,
                  "content": "    # Import config values for diagnostics"
                },
                {
                  "lineNumber": 83,
                  "content": "    # Create diagnostics with correct dimension and perturbation type"
                },
                {
                  "lineNumber": 84,
                  "content": "    diagnostics = TrainingDiagnostics("
                },
                {
                  "lineNumber": 85,
                  "content": "        save_dir='./diagnostics/',"
                },
                {
                  "lineNumber": 97,
                  "content": "            if i % 50 == 0 and diagnostics is not None:"
                },
                {
                  "lineNumber": 101,
                  "content": "                    if diagnostics.is_3d_power_spectrum:"
                },
                {
                  "lineNumber": 106,
                  "content": "                    diagnostics.log_iteration("
                },
                {
                  "lineNumber": 118,
                  "content": "                    # Keep training robust if diagnostics fail"
                },
                {
                  "lineNumber": 146,
                  "content": "            if i % 50 == 0 and loss_breakdown is not None and diagnostics is not None:"
                },
                {
                  "lineNumber": 150,
                  "content": "                    if diagnostics.is_3d_power_spectrum:"
                },
                {
                  "lineNumber": 155,
                  "content": "                    diagnostics.log_iteration("
                },
                {
                  "lineNumber": 175,
                  "content": "    # Generate diagnostic plots at the end of training"
                },
                {
                  "lineNumber": 176,
                  "content": "    if diagnostics is not None:"
                },
                {
                  "lineNumber": 180,
                  "content": "            # Generate unified diagnostics (5 plots for all cases)"
                },
                {
                  "lineNumber": 181,
                  "content": "            print(\"\\n[INFO] Generating unified training diagnostics...\")"
                },
                {
                  "lineNumber": 194,
                  "content": "            diagnostics.run_unified_diagnostics("
                },
                {
                  "lineNumber": 200,
                  "content": "            print(f\"\\n[INFO] All diagnostics saved to ./diagnostics/\")"
                },
                {
                  "lineNumber": 203,
                  "content": "            print(f\"[WARN] Final diagnostics plotting failed: {_diag_err}\")"
                },
                {
                  "lineNumber": 206,
                  "content": "    # Return diagnostics object for potential reuse in post-training analysis"
                },
                {
                  "lineNumber": 207,
                  "content": "    return diagnostics"
                },
                {
                  "lineNumber": 478,
                  "content": "    diagnostics = TrainingDiagnostics("
                },
                {
                  "lineNumber": 479,
                  "content": "        save_dir='./diagnostics/',"
                },
                {
                  "lineNumber": 495,
                  "content": "    def _log_diagnostics(iteration, total_loss, pde_loss, ic_loss):"
                },
                {
                  "lineNumber": 496,
                  "content": "        if diagnostics is None:"
                },
                {
                  "lineNumber": 500,
                  "content": "            if diagnostics.is_3d_power_spectrum:"
                },
                {
                  "lineNumber": 504,
                  "content": "            diagnostics.log_iteration("
                },
                {
                  "lineNumber": 545,
                  "content": "            _log_diagnostics(i, avg_loss.item(), avg_loss.item(), 0.0)"
                },
                {
                  "lineNumber": 573,
                  "content": "            _log_diagnostics(stage1_iters + i, value, value, 0.0)"
                },
                {
                  "lineNumber": 614,
                  "content": "            _log_diagnostics(stage1_iters + stage1_lbfgs_iters + i, avg_loss.item(), avg_loss.item(), ic_loss.item())"
                },
                {
                  "lineNumber": 657,
                  "content": "            _log_diagnostics(stage1_iters + stage1_lbfgs_iters + stage2_iters + i, value, value, ic_holder[0])"
                },
                {
                  "lineNumber": 707,
                  "content": "            _log_diagnostics("
                },
                {
                  "lineNumber": 763,
                  "content": "            _log_diagnostics("
                },
                {
                  "lineNumber": 770,
                  "content": "    # Final diagnostics plotting"
                },
                {
                  "lineNumber": 771,
                  "content": "    if diagnostics is not None:"
                },
                {
                  "lineNumber": 787,
                  "content": "            diagnostics.run_unified_diagnostics("
                },
                {
                  "lineNumber": 792,
                  "content": "            print(f\"\\n[INFO] All diagnostics saved to ./diagnostics/\")"
                },
                {
                  "lineNumber": 795,
                  "content": "            print(f\"[WARN] Final diagnostics plotting failed: {_diag_err}\")"
                },
                {
                  "lineNumber": 798,
                  "content": "    return diagnostics"
                }
              ]
            },
            {
              "file": ".\\training\\physics.py",
              "matches": [
                {
                  "lineNumber": 8,
                  "content": "from training.training_diagnostics import TrainingDiagnostics"
                },
                {
                  "lineNumber": 413,
                  "content": "    # Aggressive memory cleanup after backward pass"
                }
              ]
            },
            {
              "file": ".\\package_for_kaggle.py",
              "matches": [
                {
                  "lineNumber": 35,
                  "content": "    (\"training/training_diagnostics.py\", \"training.training_diagnostics\"),"
                }
              ]
            },
            {
              "file": ".\\utilities\\__init__.py",
              "matches": [
                {
                  "lineNumber": 1,
                  "content": "# Utility scripts for analysis and diagnostics"
                }
              ]
            },
            {
              "file": ".\\utilities\\DIAGNOSTICS_GUIDE.md",
              "matches": [
                {
                  "lineNumber": 3,
                  "content": "This guide explains the 5 optimized diagnostic plots and what they reveal about PINN performance degradation at high tmax."
                },
                {
                  "lineNumber": 17,
                  "content": "## Plot 1: Training Diagnostics (&#96;training_diagnostics.png&#96;)"
                },
                {
                  "lineNumber": 132,
                  "content": "diagnostics = TrainingDiagnostics(save_dir='./diagnostics/')"
                },
                {
                  "lineNumber": 138,
                  "content": "        diagnostics.log_iteration(iteration, model, loss_dict, geomtime_col)"
                },
                {
                  "lineNumber": 140,
                  "content": "diagnostics.plot_diagnostics()  # Generates Plot 1"
                },
                {
                  "lineNumber": 146,
                  "content": "diagnostics.run_comprehensive_diagnostics("
                },
                {
                  "lineNumber": 214,
                  "content": "Based on diagnostic results, consider these techniques:"
                }
              ]
            },
            {
              "file": ".\\visualization\\Plotting.py",
              "matches": [
                {
                  "lineNumber": 22,
                  "content": "    This helps prevent OOM errors when FD solver needs GPU memory."
                },
                {
                  "lineNumber": 111,
                  "content": "    # Create diagnostic plot"
                },
                {
                  "lineNumber": 1308,
                  "content": "                # Optional: Save diagnostic plot first time"
                }
              ]
            },
            {
              "file": ".\\utilities\\diagnose_comparison.py",
              "matches": [
                {
                  "lineNumber": 31,
                  "content": "from utilities.fd_diagnostics import diagnose_fd_solution, plot_diagnostic_masks"
                },
                {
                  "lineNumber": 46,
                  "content": "    parser.add_argument('--save-plot', action='store_true', help='Save diagnostic plot')"
                },
                {
                  "lineNumber": 129,
                  "content": "    # Run diagnostics"
                },
                {
                  "lineNumber": 130,
                  "content": "    diagnostics = diagnose_fd_solution("
                },
                {
                  "lineNumber": 138,
                  "content": "    # Create diagnostic plot"
                },
                {
                  "lineNumber": 146,
                  "content": "        save_path = os.path.join(output_dir, f\"fd_diagnostics_t{args.time:.4f}.png\")"
                },
                {
                  "lineNumber": 148,
                  "content": "    plot_diagnostic_masks(xs, ys, diagnostics, rho_fd_interp, epsilon, "
                },
                {
                  "lineNumber": 156,
                  "content": "    quality = diagnostics['quality_score']"
                },
                {
                  "lineNumber": 160,
                  "content": "        print(f\"  Mean error in valid regions: {diagnostics['mean_epsilon_valid']:.2f}%\")"
                },
                {
                  "lineNumber": 164,
                  "content": "        print(f\"  Mean error (all): {diagnostics['mean_epsilon']:.2f}%\")"
                },
                {
                  "lineNumber": 165,
                  "content": "        print(f\"  Mean error (valid only): {diagnostics['mean_epsilon_valid']:.2f}%\")"
                }
              ]
            },
            {
              "file": ".\\utilities\\fd_diagnostics.py",
              "matches": [
                {
                  "lineNumber": 22,
                  "content": "        verbose: Print diagnostic messages"
                },
                {
                  "lineNumber": 25,
                  "content": "        Dictionary with diagnostic information and quality masks"
                },
                {
                  "lineNumber": 27,
                  "content": "    diagnostics = {}"
                },
                {
                  "lineNumber": 31,
                  "content": "    diagnostics['has_nan_inf'] = np.any(nan_mask)"
                },
                {
                  "lineNumber": 32,
                  "content": "    diagnostics['nan_inf_fraction'] = np.mean(nan_mask)"
                },
                {
                  "lineNumber": 33,
                  "content": "    diagnostics['nan_inf_mask'] = nan_mask"
                },
                {
                  "lineNumber": 37,
                  "content": "    diagnostics['has_negative'] = np.any(negative_mask)"
                },
                {
                  "lineNumber": 38,
                  "content": "    diagnostics['negative_fraction'] = np.mean(negative_mask)"
                },
                {
                  "lineNumber": 39,
                  "content": "    diagnostics['negative_mask'] = negative_mask"
                },
                {
                  "lineNumber": 44,
                  "content": "    diagnostics['extreme_low_fraction'] = np.mean(extreme_low_mask)"
                },
                {
                  "lineNumber": 45,
                  "content": "    diagnostics['extreme_low_mask'] = extreme_low_mask"
                },
                {
                  "lineNumber": 46,
                  "content": "    diagnostics['min_density_ratio'] = np.min(density_ratio)"
                },
                {
                  "lineNumber": 47,
                  "content": "    diagnostics['max_density_ratio'] = np.max(density_ratio)"
                },
                {
                  "lineNumber": 69,
                  "content": "    diagnostics['shear_threshold'] = shear_threshold"
                },
                {
                  "lineNumber": 70,
                  "content": "    diagnostics['high_shear_fraction'] = np.mean(high_shear_mask)"
                },
                {
                  "lineNumber": 71,
                  "content": "    diagnostics['max_shear'] = np.max(normalized_shear)"
                },
                {
                  "lineNumber": 72,
                  "content": "    diagnostics['high_shear_mask'] = high_shear_mask"
                },
                {
                  "lineNumber": 79,
                  "content": "    diagnostics['div_anomaly_fraction'] = np.mean(div_anomaly_mask)"
                },
                {
                  "lineNumber": 80,
                  "content": "    diagnostics['div_anomaly_mask'] = div_anomaly_mask"
                },
                {
                  "lineNumber": 98,
                  "content": "    diagnostics['shear_layer_fraction'] = np.mean(shear_layer_mask)"
                },
                {
                  "lineNumber": 99,
                  "content": "    diagnostics['shear_layer_mask'] = shear_layer_mask"
                },
                {
                  "lineNumber": 106,
                  "content": "    diagnostics['supersonic_fraction'] = np.mean(supersonic_mask)"
                },
                {
                  "lineNumber": 107,
                  "content": "    diagnostics['max_mach'] = np.max(mach)"
                },
                {
                  "lineNumber": 108,
                  "content": "    diagnostics['supersonic_mask'] = supersonic_mask"
                },
                {
                  "lineNumber": 112,
                  "content": "    diagnostics['mean_epsilon'] = np.mean(epsilon)"
                },
                {
                  "lineNumber": 113,
                  "content": "    diagnostics['median_epsilon'] = np.median(epsilon)"
                },
                {
                  "lineNumber": 114,
                  "content": "    diagnostics['max_epsilon'] = np.max(epsilon)"
                },
                {
                  "lineNumber": 115,
                  "content": "    diagnostics['p95_epsilon'] = np.percentile(epsilon, 95)"
                },
                {
                  "lineNumber": 126,
                  "content": "    diagnostics['high_error_instability_fraction'] = np.mean(high_error_moderate_density)"
                },
                {
                  "lineNumber": 127,
                  "content": "    diagnostics['high_error_instability_mask'] = high_error_moderate_density"
                },
                {
                  "lineNumber": 128,
                  "content": "    diagnostics['epsilon_threshold'] = epsilon_threshold"
                },
                {
                  "lineNumber": 135,
                  "content": "    diagnostics['invalid_mask'] = invalid_mask"
                },
                {
                  "lineNumber": 136,
                  "content": "    diagnostics['invalid_fraction'] = np.mean(invalid_mask)"
                },
                {
                  "lineNumber": 141,
                  "content": "        diagnostics['mean_epsilon_valid'] = np.mean(valid_epsilon)"
                },
                {
                  "lineNumber": 142,
                  "content": "        diagnostics['median_epsilon_valid'] = np.median(valid_epsilon)"
                },
                {
                  "lineNumber": 143,
                  "content": "        diagnostics['max_epsilon_valid'] = np.max(valid_epsilon)"
                },
                {
                  "lineNumber": 144,
                  "content": "        diagnostics['p95_epsilon_valid'] = np.percentile(valid_epsilon, 95)"
                },
                {
                  "lineNumber": 146,
                  "content": "        diagnostics['mean_epsilon_valid'] = np.nan"
                },
                {
                  "lineNumber": 147,
                  "content": "        diagnostics['median_epsilon_valid'] = np.nan"
                },
                {
                  "lineNumber": 148,
                  "content": "        diagnostics['max_epsilon_valid'] = np.nan"
                },
                {
                  "lineNumber": 149,
                  "content": "        diagnostics['p95_epsilon_valid'] = np.nan"
                },
                {
                  "lineNumber": 152,
                  "content": "    quality_score = 1.0 - diagnostics['invalid_fraction']"
                },
                {
                  "lineNumber": 153,
                  "content": "    diagnostics['quality_score'] = quality_score"
                },
                {
                  "lineNumber": 160,
                  "content": "        print(f\"Invalid regions: {diagnostics['invalid_fraction']:.1%}\")"
                },
                {
                  "lineNumber": 163,
                  "content": "        print(f\"  - NaN/Inf values: {diagnostics['nan_inf_fraction']:.1%}\")"
                },
                {
                  "lineNumber": 164,
                  "content": "        print(f\"  - Negative density: {diagnostics['negative_fraction']:.1%}\")"
                },
                {
                  "lineNumber": 165,
                  "content": "        print(f\"  - Extreme low density (&lt;0.4x): {diagnostics['extreme_low_fraction']:.1%} (min ratio: {diagnostics['min_density_ratio']:.2f})\")"
                },
                {
                  "lineNumber": 166,
                  "content": "        print(f\"  - High velocity shear (instability): {diagnostics['high_shear_fraction']:.1%} (max shear: {diagnostics['max_shear']:.2f}, threshold: {diagnostics.get('shear_threshold', 0):.3f})\")"
                },
                {
                  "lineNumber": 167,
                  "content": "        print(f\"  - Velocity divergence anomalies: {diagnostics['div_anomaly_fraction']:.1%}\")"
                },
                {
                  "lineNumber": 168,
                  "content": "        print(f\"  - Shear layers (velocity+density gradients): {diagnostics['shear_layer_fraction']:.1%}\")"
                },
                {
                  "lineNumber": 169,
                  "content": "        print(f\"  - High error in moderate density (instability): {diagnostics['high_error_instability_fraction']:.1%} (threshold: {diagnostics.get('epsilon_threshold', 0):.1f}%)\")"
                },
                {
                  "lineNumber": 170,
                  "content": "        print(f\"  - Supersonic flow (outside cores): {diagnostics['supersonic_fraction']:.1%} (Max Mach: {diagnostics['max_mach']:.2f})\")"
                },
                {
                  "lineNumber": 171,
                  "content": "        print(f\"  - Density range: {diagnostics['min_density_ratio']:.2f} - {diagnostics['max_density_ratio']:.2f}x background\")"
                },
                {
                  "lineNumber": 174,
                  "content": "        print(f\"  - Mean error: {diagnostics['mean_epsilon']:.2f}%\")"
                },
                {
                  "lineNumber": 175,
                  "content": "        print(f\"  - Median error: {diagnostics['median_epsilon']:.2f}%\")"
                },
                {
                  "lineNumber": 176,
                  "content": "        print(f\"  - 95th percentile: {diagnostics['p95_epsilon']:.2f}%\")"
                },
                {
                  "lineNumber": 177,
                  "content": "        print(f\"  - Max error: {diagnostics['max_epsilon']:.2f}%\")"
                },
                {
                  "lineNumber": 179,
                  "content": "        if not np.isnan(diagnostics['mean_epsilon_valid']):"
                },
                {
                  "lineNumber": 181,
                  "content": "            print(f\"  - Mean error: {diagnostics['mean_epsilon_valid']:.2f}%\")"
                },
                {
                  "lineNumber": 182,
                  "content": "            print(f\"  - Median error: {diagnostics['median_epsilon_valid']:.2f}%\")"
                },
                {
                  "lineNumber": 183,
                  "content": "            print(f\"  - 95th percentile: {diagnostics['p95_epsilon_valid']:.2f}%\")"
                },
                {
                  "lineNumber": 184,
                  "content": "            print(f\"  - Max error: {diagnostics['max_epsilon_valid']:.2f}%\")"
                },
                {
                  "lineNumber": 191,
                  "content": "            if diagnostics['high_shear_fraction'] &gt; 0.05:"
                },
                {
                  "lineNumber": 195,
                  "content": "            if diagnostics['supersonic_fraction'] &gt; 0.05:"
                },
                {
                  "lineNumber": 198,
                  "content": "            if diagnostics['invalid_fraction'] &gt; 0.10:"
                },
                {
                  "lineNumber": 209,
                  "content": "    return diagnostics"
                },
                {
                  "lineNumber": 212,
                  "content": "def plot_diagnostic_masks(x: np.ndarray, y: np.ndarray, diagnostics: Dict,"
                },
                {
                  "lineNumber": 217,
                  "content": "    Create visualization of diagnostic masks to identify problematic regions."
                },
                {
                  "lineNumber": 221,
                  "content": "        diagnostics: Output from diagnose_fd_solution"
                },
                {
                  "lineNumber": 238,
                  "content": "    combined_mask = (diagnostics['negative_mask'].astype(int) + "
                },
                {
                  "lineNumber": 239,
                  "content": "                    diagnostics['extreme_low_mask'].astype(int) * 2 +"
                },
                {
                  "lineNumber": 240,
                  "content": "                    diagnostics['high_shear_mask'].astype(int) * 3 +"
                },
                {
                  "lineNumber": 241,
                  "content": "                    diagnostics['shear_layer_mask'].astype(int) * 4 +"
                },
                {
                  "lineNumber": 242,
                  "content": "                    diagnostics.get('high_error_instability_mask', np.zeros_like(rho_fd)).astype(int) * 5 +"
                },
                {
                  "lineNumber": 243,
                  "content": "                    diagnostics['supersonic_mask'].astype(int) * 6)"
                },
                {
                  "lineNumber": 261,
                  "content": "        im = ax.pcolormesh(XX, YY, diagnostics['high_shear_mask'].astype(float), "
                },
                {
                  "lineNumber": 273,
                  "content": "    im = ax.pcolormesh(XX, YY, diagnostics['invalid_mask'].astype(float),"
                },
                {
                  "lineNumber": 275,
                  "content": "    ax.set_title(f\"Invalid Regions ({diagnostics['invalid_fraction']:.1%})\")"
                },
                {
                  "lineNumber": 281,
                  "content": "    epsilon_masked[diagnostics['invalid_mask']] = 0"
                },
                {
                  "lineNumber": 285,
                  "content": "    ax.text(0.02, 0.98, f\"Mean: {diagnostics['mean_epsilon_valid']:.2f}%\", "
                }
              ]
            }
          ],
          "totalLines": 231,
          "totalMatchedLines": 231
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Examining training flow**

I'm reviewing the relevant section in train.py to understand the current training flow and identify where modifications are needed.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: train.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Planning diagnostics reorder after cleanup**</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: train.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Reorganizing diagnostics after cleanup and GPU move**

I'm moving the diagnostics block after the cleanup steps that move the network back to GPU and set it to eval mode, ensuring diagnostics run on GPU when available. For CPU, diagnostics will still run after cleanup but before visualization, with net.eval() called to ensure proper evaluation mode.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "train.py"
}
```
Additional data:

```json
{
  "codeblockId": "432798aa-fa6b-4b48-be66-192aff4c64d2",
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/train.py",
      "query": "",
      "fragment": "",
      "_formatted": null,
      "_fsPath": null
    },
    "codeblockId": "432798aa-fa6b-4b48-be66-192aff4c64d2",
    "codeBlockIdx": 0,
    "content": "import os\nimport sys\nimport shutil\nimport numpy as np\nimport time\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\n\n# ==================== PyTorch Performance Optimizations ====================\n# Enable cuDNN autotuner - finds optimal convolution algorithms\n# This helps when input sizes are consistent (which they are in PINN training)\ntorch.backends.cudnn.benchmark = True\n\n# Enable TF32 on Ampere GPUs (A100, RTX 3090, RTX 4090, etc.) for 20-30% speedup\n# TF32 provides faster matrix multiplication with minimal accuracy loss\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    # Prefer explicit device/dtype settings rather than deprecated tensor-type override\n    torch.set_default_dtype(torch.float32)\n    if hasattr(torch, \"set_default_device\"):\n        torch.set_default_device(\"cuda\")\n    print(\"PyTorch performance optimizations enabled (cuDNN benchmark, TF32)\")\n\nfrom core.data_generator import input_taker, req_consts_calc\nfrom training.trainer import train, train_multistage\nfrom core.initial_conditions import initialize_shared_velocity_fields\nfrom config import BATCH_SIZE, NUM_BATCHES, N_0, N_r, DIMENSION\nfrom config import a, wave, cs, xmin, ymin, zmin, tmin, tmax as TMAX_CFG, iteration_adam_2D, iteration_lbgfs_2D, harmonics, PERTURBATION_TYPE, rho_o, SLICE_Y\nfrom config import num_neurons, num_layers, num_of_waves\nfrom config import RANDOM_SEED, STARTUP_DT\nfrom config import ENABLE_TRAINING_DIAGNOSTICS\nfrom config import USE_MULTI_STAGE_TRAINING, STAGE1_ADAM_ITERS, STAGE2_ADAM_ITERS, STAGE3_ADAM_ITERS\nfrom config import STAGE1_LBFGS_ITERS, STAGE2_LBFGS_ITERS, STAGE3_LBFGS_ITERS\nfrom core.losses import ASTPN\nfrom core.model_architecture import PINN\nfrom visualization.Plotting import create_2d_animation\nfrom visualization.Plotting import create_1d_cross_sections_sinusoidal\nfrom visualization.Plotting import create_density_growth_plot\nfrom config import PLOT_DENSITY_GROWTH, GROWTH_PLOT_TMAX, GROWTH_PLOT_DT\nfrom config import FD_N_2D\n\n\ndef _save_trained_models(net):\n    \"\"\"Persist trained model immediately after training completes.\"\"\"\n    try:\n        from config import SNAPSHOT_DIR\n        model_dir = os.path.join(SNAPSHOT_DIR, \"GRINN\")\n        os.makedirs(model_dir, exist_ok=True)\n        model_path = os.path.join(model_dir, \"model.pth\")\n        torch.save(net.state_dict(), model_path)\n        print(f\"Saved model to {model_path}\")\n    except Exception as e:\n        print(f\"Warning: failed to save model: {e}\")\n\n\ndef _comprehensive_gpu_cleanup(nets, optimizers=None, collocation_data=None, \n                                cached_ic_values=None, keep_models_for_vis=True, \n                                target_device=None):\n    \"\"\"\n    Comprehensive GPU memory cleanup after PINN training.\n    \n    This function properly frees GPU memory by:\n    1. Moving models to CPU (if keep_models_for_vis=True) or deleting them\n    2. Deleting optimizer state (Adam momentum buffers, LBFGS history)\n    3. Deleting collocation points and related tensors\n    4. Deleting cached IC values\n    5. Clearing PyTorch's cache allocator\n    \n    Args:\n        nets: List of neural networks (or single network in a list)\n        optimizers: List of optimizers to clean up (optional)\n        collocation_data: Dict or list of collocation tensors to delete (optional)\n        cached_ic_values: List of cached IC value dicts to delete (optional)\n        keep_models_for_vis: If True, move models to CPU instead of deleting (default: True)\n        target_device: Target device for models if keep_models_for_vis=True (default: 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if not torch.cuda.is_available():\n        return\n    \n    if target_device is None:\n        target_device = 'cpu'\n    \n    print(\"Starting comprehensive GPU memory cleanup...\")\n    \n    # 1. Set models to eval mode (reduces memory usage) and optionally move to CPU\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                # Set to eval mode to disable gradient tracking and reduce memory\n                net.eval()\n                # Clear any cached gradients\n                for param in net.parameters():\n                    if param.grad is not None:\n                        param.grad = None\n                \n                if keep_models_for_vis:\n                    # Move to target device (usually CPU) for visualization\n                    # Models can be moved back to GPU later if needed\n                    net = net.to(target_device)\n                    nets[i] = net\n                    print(f\"  Moved network {i} to {target_device} (eval mode)\")\n                else:\n                    # Delete model entirely\n                    del net\n                    nets[i] = None\n                    print(f\"  Deleted network {i}\")\n    \n    # 2. Delete optimizer state (can be large, especially LBFGS history)\n    if optimizers is not None:\n        if isinstance(optimizers, (list, tuple)):\n            for i, opt in enumerate(optimizers):\n                if opt is not None:\n                    # Clear optimizer state (this frees momentum buffers, LBFGS history, etc.)\n                    opt.state.clear()\n                    opt.param_groups.clear()\n                    del opt\n                    print(f\"  Deleted optimizer {i} state\")\n        else:\n            if optimizers is not None:\n                optimizers.state.clear()\n                optimizers.param_groups.clear()\n                del optimizers\n                print(\"  Deleted optimizer state\")\n    \n    # 3. Delete collocation points (these can be very large)\n    if collocation_data is not None:\n        if isinstance(collocation_data, dict):\n            for key, value in collocation_data.items():\n                if value is not None:\n                    if isinstance(value, (list, tuple)):\n                        for item in value:\n                            if isinstance(item, torch.Tensor) and item.is_cuda:\n                                del item\n                    elif isinstance(value, torch.Tensor) and value.is_cuda:\n                        del value\n            collocation_data.clear()\n            print(\"  Deleted collocation data dict\")\n        elif isinstance(collocation_data, (list, tuple)):\n            for item in collocation_data:\n                if item is not None:\n                    if isinstance(item, (list, tuple)):\n                        for subitem in item:\n                            if isinstance(subitem, torch.Tensor) and subitem.is_cuda:\n                                del subitem\n                    elif isinstance(item, torch.Tensor) and item.is_cuda:\n                        del item\n            print(\"  Deleted collocation data list\")\n        elif isinstance(collocation_data, torch.Tensor) and collocation_data.is_cuda:\n            del collocation_data\n            print(\"  Deleted collocation tensor\")\n    \n    # 4. Delete cached IC values\n    if cached_ic_values is not None:\n        if isinstance(cached_ic_values, list):\n            for i, cached_dict in enumerate(cached_ic_values):\n                if cached_dict is not None and isinstance(cached_dict, dict):\n                    for key, value in cached_dict.items():\n                        if isinstance(value, torch.Tensor) and value.is_cuda:\n                            del value\n                    cached_dict.clear()\n            cached_ic_values.clear()\n            print(\"  Deleted cached IC values\")\n    \n    # 5. Force garbage collection\n    import gc\n    gc.collect()\n    \n    # 6. Clear PyTorch's cache allocator and synchronize\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    print(\"GPU memory cleanup completed\")\n    \n    # Note: If visualization needs models on GPU, they can be moved back with:\n    # for net in nets:\n    #     if net is not None:\n    #         net = net.to(device)\n\n\ndef _move_models_to_device(nets, target_device):\n    \"\"\"\n    Move models to target device (e.g., back to GPU for visualization).\n    \n    Args:\n        nets: List of neural networks\n        target_device: Target device string (e.g., 'cuda:0' or 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                net = net.to(target_device)\n                nets[i] = net\n\n\ndef clean_pycache(root_dir: str) -&gt; Tuple[int, int]:\n    \"\"\"Remove all __pycache__ directories and .pyc files under &#96;&#96;root_dir&#96;&#96;.\"\"\"\n    removed_dirs = 0\n    removed_files = 0\n\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        if \"__pycache__\" in dirnames:\n            pycache_path = os.path.join(dirpath, \"__pycache__\")\n            try:\n                shutil.rmtree(pycache_path)\n                print(f\"Removed: {pycache_path}\")\n                removed_dirs += 1\n            except Exception as exc:  # pragma: no cover - cleanup best effort\n                print(f\"Error removing {pycache_path}: {exc}\")\n\n        for filename in filenames:\n            if filename.endswith(\".pyc\"):\n                pyc_path = os.path.join(dirpath, filename)\n                try:\n                    os.remove(pyc_path)\n                    print(f\"Removed: {pyc_path}\")\n                    removed_files += 1\n                except Exception as exc:  # pragma: no cover - cleanup best effort\n                    print(f\"Error removing {pyc_path}: {exc}\")\n\n    print(\"=\" * 60)\n    print(\"Cleanup complete!\")\n    print(f\"  Removed {removed_dirs} __pycache__ directories\")\n    print(f\"  Removed {removed_files} .pyc files\")\n    print(\"=\" * 60)\n\n    return removed_dirs, removed_files\n\n\nif \"--clean-pycache\" in sys.argv or \"--clean-cache\" in sys.argv:\n    flag = \"--clean-pycache\" if \"--clean-pycache\" in sys.argv else \"--clean-cache\"\n    script_root = os.path.dirname(os.path.abspath(__file__))\n    print(f\"Cleaning Python cache files from: {script_root}\")\n    clean_pycache(script_root)\n    # Prevent the rest of the training script from running in cleanup-only mode\n    sys.exit(0)\n\nhas_gpu = torch.cuda.is_available()\nhas_mps = torch.backends.mps.is_built()\ndevice = \"mps\" if torch.backends.mps.is_built() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Clear GPU memory if using CUDA\nif device.startswith('cuda'):\n    torch.cuda.empty_cache()\n\n\nlam, rho_1, num_of_waves, tmax, _, _, _ = input_taker(wave, a, num_of_waves, TMAX_CFG, N_0, 0, N_r)\n\njeans, alpha = req_consts_calc(lam, rho_1)\n# Set initial velocity amplitude per perturbation type\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    k = 2*np.pi/lam\n    v_1 = (rho_1 / (rho_o if rho_o != 0 else 1.0)) * (alpha / k)\nelse:\n    v_1 = a * cs\n\nxmax = xmin + lam * num_of_waves\nymax = ymin + lam * num_of_waves\nzmax = zmin + lam * num_of_waves\n#zmax = 1.0\n\n# Initialize shared velocity fields for consistent PINN/FD initial conditions\nvx_np, vy_np, vz_np = None, None, None  # Default values for sinusoidal case\nif str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\n    if DIMENSION == 3:\n        vx_np, vy_np, vz_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=3)\n    else:\n        vx_np, vy_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=2)\n    \n    # Set shared velocity fields for plotting (only for 2D visualization)\n    if DIMENSION == 2:\n        from visualization.Plotting import set_shared_velocity_fields\n        set_shared_velocity_fields(vx_np, vy_np)\n\n# ==================== STANDARD PINN MODE ====================\nprint(\"Running in standard PINN mode (single network)...\")\n\nnet = PINN(n_harmonics=harmonics)\nnet = net.to(device)\nmse_cost_function = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\noptimizerL = torch.optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n\nif DIMENSION == 1:\n    astpn_rmin = [xmin, tmin]\n    astpn_rmax = [xmax, tmax]\nelif DIMENSION == 2:\n    astpn_rmin = [xmin, ymin, tmin]\n    astpn_rmax = [xmax, ymax, tmax]\nelif DIMENSION == 3:\n    astpn_rmin = [xmin, ymin, zmin, tmin]\n    astpn_rmax = [xmax, ymax, zmax, tmax]\nelse:\n    raise ValueError(f\"Unsupported DIMENSION={DIMENSION}\")\ncollocation_model = ASTPN(rmin=astpn_rmin, rmax=astpn_rmax, N_0=N_0, N_b=0, N_r=N_r, dimension=DIMENSION)\n\n# Set domain on the network so periodic embeddings enforce hard BCs\nspatial_rmin = [xmin]\nspatial_rmax = [xmax]\nif DIMENSION &gt;= 2:\n    spatial_rmin.append(ymin)\n    spatial_rmax.append(ymax)\nif DIMENSION &gt;= 3:\n    spatial_rmin.append(zmin)\n    spatial_rmax.append(zmax)\nnet.set_domain(rmin=spatial_rmin, rmax=spatial_rmax, dimension=DIMENSION)\n\n# IC collocation stays at t=0 throughout\ncollocation_IC = collocation_model.geo_time_coord(option=\"IC\")\n\n\n# Generate extra collocation points at t=0 for Poisson enforcement (Option 3)\nfrom core.data_generator import generate_poisson_ic_points\nfrom config import N_POISSON_IC\ncollocation_poisson_ic = generate_poisson_ic_points(\n    rmin=collocation_model.rmin,\n    rmax=collocation_model.rmax,\n    n_points=N_POISSON_IC,\n    dimension=DIMENSION,\n    device=device\n)\nprint(f\"Generated {N_POISSON_IC} extra collocation points at t=0 for Poisson enforcement\")\n\n\nstart_time = time.time()\n\n# Standard training\ncollocation_domain = collocation_model.geo_time_coord(option=\"Domain\")\n\nif USE_MULTI_STAGE_TRAINING:\n    print(\"Using multi-stage training...\")\n    training_diagnostics = train_multistage(\n        net=net,\n        model=collocation_model,\n        collocation_domain=collocation_domain,\n        collocation_IC=collocation_IC,\n        collocation_poisson_ic=collocation_poisson_ic,\n        optimizer=optimizer,\n        optimizerL=optimizerL,\n        mse_cost_function=mse_cost_function,\n        rho_1=rho_1,\n        lam=lam,\n        jeans=jeans,\n        v_1=v_1,\n        device=device,\n        stage1_iters=STAGE1_ADAM_ITERS,\n        stage2_iters=STAGE2_ADAM_ITERS,\n        stage3_iters=STAGE3_ADAM_ITERS,\n        stage1_lbfgs_iters=STAGE1_LBFGS_ITERS,\n        stage2_lbfgs_iters=STAGE2_LBFGS_ITERS,\n        stage3_lbfgs_iters=STAGE3_LBFGS_ITERS\n    )\nelse:\n    print(\"Using standard training...\")\n    training_diagnostics = train(\n        net=net,\n        model=collocation_model,\n        collocation_domain=collocation_domain,\n        collocation_IC=collocation_IC,\n        collocation_poisson_ic=collocation_poisson_ic,\n        optimizer=optimizer,\n        optimizerL=optimizerL,\n        closure=None,\n        mse_cost_function=mse_cost_function,\n        iteration_adam=iteration_adam_2D,\n        iterationL=iteration_lbgfs_2D,\n        rho_1=rho_1,\n        lam=lam,\n        jeans=jeans,\n        v_1=v_1,\n        device=device\n    )\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"[Timing] PINN training completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n\n# Save model immediately after training completes\n_save_trained_models(net)\n\n# Comprehensive GPU memory cleanup after training\n# This properly frees model parameters, optimizer state, collocation points, etc.\nif device.startswith('cuda'):\n    # Collect all training-related data for cleanup\n    cleanup_optimizers = [optimizer, optimizerL] if 'optimizer' in locals() and 'optimizerL' in locals() else None\n    \n    # Try to get collocation data references\n    try:\n        cleanup_collocation = {\n            'domain': collocation_domain if 'collocation_domain' in locals() else None,\n            'IC': collocation_IC if 'collocation_IC' in locals() else None,\n            'poisson_IC': collocation_poisson_ic if 'collocation_poisson_ic' in locals() else None\n        }\n    except:\n        cleanup_collocation = None\n    \n    # Perform comprehensive cleanup\n    # This frees GPU memory by deleting optimizer state and collocation data\n    # Models are temporarily moved to CPU during cleanup, then moved back to GPU for visualization\n    _comprehensive_gpu_cleanup(\n        nets=[net],\n        optimizers=cleanup_optimizers,\n        collocation_data=cleanup_collocation,\n        cached_ic_values=None,\n        keep_models_for_vis=True,  # Keep models for visualization\n        target_device='cpu'  # Temporarily move models to CPU during cleanup\n    )\n    \n    # Move model back to GPU for visualization (visualization code expects model on GPU)\n    # The FD solver will clear cache before it runs, so this is safe\n    if device.startswith('cuda'):\n        _move_models_to_device([net], device)\n        print(\"Model moved back to GPU for visualization\")\n\n# ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================\n# Run comprehensive diagnostics for high-tmax failure analysis\n# This generates 4 additional critical plots beyond the training diagnostics\nif ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:\n    print(\"\\n\" + \"=\"*70)\n    print(\"Running post-training comprehensive diagnostics...\")\n    print(\"=\"*70)\n    \n    try:\n        net.eval()\n        # Run comprehensive diagnostics - generates 4 additional plots:\n        # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks\n        # 2. Conservation Laws - Mass/momentum conservation\n        # 3. Spectral Evolution - Frequency content analysis\n        # 4. Temporal Statistics - Error accumulation tracking\n        training_diagnostics.run_comprehensive_diagnostics(\n            model=net,\n            dimension=DIMENSION,\n            tmax=tmax\n        )\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"All diagnostic plots generated successfully!\")\n        print(\"Check ./diagnostics/ folder for:\")\n        print(\"  1. training_diagnostics.png - Training convergence\")\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violations\")\n        print(\"  3. conservation_laws.png - Physical consistency\")\n        print(\"  4. spectral_evolution.png - Frequency content\")\n        print(\"  5. temporal_statistics.png - Field evolution\")\n        print(\"=\"*70 + \"\\n\")\n    except Exception as e:\n        print(f\"[WARNING] Post-training diagnostics failed: {e}\")\n        print(\"Training completed successfully, but diagnostic plots may be incomplete.\")\n        import traceback\n        traceback.print_exc()\n\n# ==================== VISUALIZATION ====================\ninitial_params = (xmin, xmax, ymin, ymax, rho_1, alpha, lam, \"temp\", tmax)\n\nanim_density = create_2d_animation(net, initial_params, which=\"density\", fps=10, verbose=False)\nanim_velocity = create_2d_animation(net, initial_params, which=\"velocity\", fps=10, verbose=False)\n\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    create_1d_cross_sections_sinusoidal(net, initial_params, time_points=None, y_fixed=SLICE_Y, N_fd=600, nu_fd=0.5)\n\nif PLOT_DENSITY_GROWTH:\n    try:\n        tmax_growth = float(GROWTH_PLOT_TMAX)\n    except Exception:\n        tmax_growth = float(TMAX_CFG)\n    dt_growth = float(GROWTH_PLOT_DT)\n    create_density_growth_plot(net, initial_params, tmax=tmax_growth, dt=dt_growth)\n\n# ==================== FINAL CACHE CLEANUP ====================\nscript_root = os.path.dirname(os.path.abspath(__file__))\nprint(\"Performing final Python cache cleanup...\")\nclean_pycache(script_root)\n\n",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "  _save_trained_models(net)\n  \n- # ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================\n- # Run comprehensive diagnostics for high-tmax failure analysis\n- # This generates 4 additional critical plots beyond the training diagnostics\n- if ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:\n-     print(\"\\n\" + \"=\"*70)\n-     print(\"Running post-training comprehensive diagnostics...\")\n-     print(\"=\"*70)\n-     \n-     try:\n-         # Run comprehensive diagnostics - generates 4 additional plots:\n-         # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks\n-         # 2. Conservation Laws - Mass/momentum conservation\n-         # 3. Spectral Evolution - Frequency content analysis\n-         # 4. Temporal Statistics - Error accumulation tracking\n-         training_diagnostics.run_comprehensive_diagnostics(\n-             model=net,\n-             dimension=DIMENSION,\n-             tmax=tmax\n-         )\n-         \n-         print(\"\\n\" + \"=\"*70)\n-         print(\"All diagnostic plots generated successfully!\")\n-         print(\"Check ./diagnostics/ folder for:\")\n-         print(\"  1. training_diagnostics.png - Training convergence\")\n-         print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violations\")\n-         print(\"  3. conservation_laws.png - Physical consistency\")\n-         print(\"  4. spectral_evolution.png - Frequency content\")\n-         print(\"  5. temporal_statistics.png - Field evolution\")\n-         print(\"=\"*70 + \"\\n\")\n-     except Exception as e:\n-         print(f\"[WARNING] Post-training diagnostics failed: {e}\")\n-         print(\"Training completed successfully, but diagnostic plots may be incomplete.\")\n-         import traceback\n-         traceback.print_exc()\n- \n  # Comprehensive GPU memory cleanup after training\n  # This properly frees model parameters, optimizer state, collocation points, etc.",
        "oldStart": 386,
        "newStart": 386,
        "oldLines": 39,
        "newLines": 4,
        "linesRemoved": 35
      },
      {
        "diffString": "          print(\"Model moved back to GPU for visualization\")\n  \n+ # ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================\n+ # Run comprehensive diagnostics for high-tmax failure analysis\n+ # This generates 4 additional critical plots beyond the training diagnostics\n+ if ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:\n+     print(\"\\n\" + \"=\"*70)\n+     print(\"Running post-training comprehensive diagnostics...\")\n+     print(\"=\"*70)\n+     \n+     try:\n+         net.eval()\n+         # Run comprehensive diagnostics - generates 4 additional plots:\n+         # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks\n+         # 2. Conservation Laws - Mass/momentum conservation\n+         # 3. Spectral Evolution - Frequency content analysis\n+         # 4. Temporal Statistics - Error accumulation tracking\n+         training_diagnostics.run_comprehensive_diagnostics(\n+             model=net,\n+             dimension=DIMENSION,\n+             tmax=tmax\n+         )\n+         \n+         print(\"\\n\" + \"=\"*70)\n+         print(\"All diagnostic plots generated successfully!\")\n+         print(\"Check ./diagnostics/ folder for:\")\n+         print(\"  1. training_diagnostics.png - Training convergence\")\n+         print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violations\")\n+         print(\"  3. conservation_laws.png - Physical consistency\")\n+         print(\"  4. spectral_evolution.png - Frequency content\")\n+         print(\"  5. temporal_statistics.png - Field evolution\")\n+         print(\"=\"*70 + \"\\n\")\n+     except Exception as e:\n+         print(f\"[WARNING] Post-training diagnostics failed: {e}\")\n+         print(\"Training completed successfully, but diagnostic plots may be incomplete.\")\n+         import traceback\n+         traceback.print_exc()\n+ \n  # ==================== VISUALIZATION ====================\n  initial_params = (xmin, xmax, ymin, ymax, rho_1, alpha, lam, \"temp\", tmax)",
        "oldStart": 455,
        "newStart": 420,
        "oldLines": 4,
        "newLines": 40,
        "linesAdded": 36
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM train.py",
  "contentsAfterEdit": "import os\nimport sys\nimport shutil\nimport numpy as np\nimport time\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\n\n# ==================== PyTorch Performance Optimizations ====================\n# Enable cuDNN autotuner - finds optimal convolution algorithms\n# This helps when input sizes are consistent (which they are in PINN training)\ntorch.backends.cudnn.benchmark = True\n\n# Enable TF32 on Ampere GPUs (A100, RTX 3090, RTX 4090, etc.) for 20-30% speedup\n# TF32 provides faster matrix multiplication with minimal accuracy loss\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    # Prefer explicit device/dtype settings rather than deprecated tensor-type override\n    torch.set_default_dtype(torch.float32)\n    if hasattr(torch, \"set_default_device\"):\n        torch.set_default_device(\"cuda\")\n    print(\"PyTorch performance optimizations enabled (cuDNN benchmark, TF32)\")\n\nfrom core.data_generator import input_taker, req_consts_calc\nfrom training.trainer import train, train_multistage\nfrom core.initial_conditions import initialize_shared_velocity_fields\nfrom config import BATCH_SIZE, NUM_BATCHES, N_0, N_r, DIMENSION\nfrom config import a, wave, cs, xmin, ymin, zmin, tmin, tmax as TMAX_CFG, iteration_adam_2D, iteration_lbgfs_2D, harmonics, PERTURBATION_TYPE, rho_o, SLICE_Y\nfrom config import num_neurons, num_layers, num_of_waves\nfrom config import RANDOM_SEED, STARTUP_DT\nfrom config import ENABLE_TRAINING_DIAGNOSTICS\nfrom config import USE_MULTI_STAGE_TRAINING, STAGE1_ADAM_ITERS, STAGE2_ADAM_ITERS, STAGE3_ADAM_ITERS\nfrom config import STAGE1_LBFGS_ITERS, STAGE2_LBFGS_ITERS, STAGE3_LBFGS_ITERS\nfrom core.losses import ASTPN\nfrom core.model_architecture import PINN\nfrom visualization.Plotting import create_2d_animation\nfrom visualization.Plotting import create_1d_cross_sections_sinusoidal\nfrom visualization.Plotting import create_density_growth_plot\nfrom config import PLOT_DENSITY_GROWTH, GROWTH_PLOT_TMAX, GROWTH_PLOT_DT\nfrom config import FD_N_2D\n\n\ndef _save_trained_models(net):\n    \"\"\"Persist trained model immediately after training completes.\"\"\"\n    try:\n        from config import SNAPSHOT_DIR\n        model_dir = os.path.join(SNAPSHOT_DIR, \"GRINN\")\n        os.makedirs(model_dir, exist_ok=True)\n        model_path = os.path.join(model_dir, \"model.pth\")\n        torch.save(net.state_dict(), model_path)\n        print(f\"Saved model to {model_path}\")\n    except Exception as e:\n        print(f\"Warning: failed to save model: {e}\")\n\n\ndef _comprehensive_gpu_cleanup(nets, optimizers=None, collocation_data=None, \n                                cached_ic_values=None, keep_models_for_vis=True, \n                                target_device=None):\n    \"\"\"\n    Comprehensive GPU memory cleanup after PINN training.\n    \n    This function properly frees GPU memory by:\n    1. Moving models to CPU (if keep_models_for_vis=True) or deleting them\n    2. Deleting optimizer state (Adam momentum buffers, LBFGS history)\n    3. Deleting collocation points and related tensors\n    4. Deleting cached IC values\n    5. Clearing PyTorch's cache allocator\n    \n    Args:\n        nets: List of neural networks (or single network in a list)\n        optimizers: List of optimizers to clean up (optional)\n        collocation_data: Dict or list of collocation tensors to delete (optional)\n        cached_ic_values: List of cached IC value dicts to delete (optional)\n        keep_models_for_vis: If True, move models to CPU instead of deleting (default: True)\n        target_device: Target device for models if keep_models_for_vis=True (default: 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if not torch.cuda.is_available():\n        return\n    \n    if target_device is None:\n        target_device = 'cpu'\n    \n    print(\"Starting comprehensive GPU memory cleanup...\")\n    \n    # 1. Set models to eval mode (reduces memory usage) and optionally move to CPU\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                # Set to eval mode to disable gradient tracking and reduce memory\n                net.eval()\n                # Clear any cached gradients\n                for param in net.parameters():\n                    if param.grad is not None:\n                        param.grad = None\n                \n                if keep_models_for_vis:\n                    # Move to target device (usually CPU) for visualization\n                    # Models can be moved back to GPU later if needed\n                    net = net.to(target_device)\n                    nets[i] = net\n                    print(f\"  Moved network {i} to {target_device} (eval mode)\")\n                else:\n                    # Delete model entirely\n                    del net\n                    nets[i] = None\n                    print(f\"  Deleted network {i}\")\n    \n    # 2. Delete optimizer state (can be large, especially LBFGS history)\n    if optimizers is not None:\n        if isinstance(optimizers, (list, tuple)):\n            for i, opt in enumerate(optimizers):\n                if opt is not None:\n                    # Clear optimizer state (this frees momentum buffers, LBFGS history, etc.)\n                    opt.state.clear()\n                    opt.param_groups.clear()\n                    del opt\n                    print(f\"  Deleted optimizer {i} state\")\n        else:\n            if optimizers is not None:\n                optimizers.state.clear()\n                optimizers.param_groups.clear()\n                del optimizers\n                print(\"  Deleted optimizer state\")\n    \n    # 3. Delete collocation points (these can be very large)\n    if collocation_data is not None:\n        if isinstance(collocation_data, dict):\n            for key, value in collocation_data.items():\n                if value is not None:\n                    if isinstance(value, (list, tuple)):\n                        for item in value:\n                            if isinstance(item, torch.Tensor) and item.is_cuda:\n                                del item\n                    elif isinstance(value, torch.Tensor) and value.is_cuda:\n                        del value\n            collocation_data.clear()\n            print(\"  Deleted collocation data dict\")\n        elif isinstance(collocation_data, (list, tuple)):\n            for item in collocation_data:\n                if item is not None:\n                    if isinstance(item, (list, tuple)):\n                        for subitem in item:\n                            if isinstance(subitem, torch.Tensor) and subitem.is_cuda:\n                                del subitem\n                    elif isinstance(item, torch.Tensor) and item.is_cuda:\n                        del item\n            print(\"  Deleted collocation data list\")\n        elif isinstance(collocation_data, torch.Tensor) and collocation_data.is_cuda:\n            del collocation_data\n            print(\"  Deleted collocation tensor\")\n    \n    # 4. Delete cached IC values\n    if cached_ic_values is not None:\n        if isinstance(cached_ic_values, list):\n            for i, cached_dict in enumerate(cached_ic_values):\n                if cached_dict is not None and isinstance(cached_dict, dict):\n                    for key, value in cached_dict.items():\n                        if isinstance(value, torch.Tensor) and value.is_cuda:\n                            del value\n                    cached_dict.clear()\n            cached_ic_values.clear()\n            print(\"  Deleted cached IC values\")\n    \n    # 5. Force garbage collection\n    import gc\n    gc.collect()\n    \n    # 6. Clear PyTorch's cache allocator and synchronize\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    print(\"GPU memory cleanup completed\")\n    \n    # Note: If visualization needs models on GPU, they can be moved back with:\n    # for net in nets:\n    #     if net is not None:\n    #         net = net.to(device)\n\n\ndef _move_models_to_device(nets, target_device):\n    \"\"\"\n    Move models to target device (e.g., back to GPU for visualization).\n    \n    Args:\n        nets: List of neural networks\n        target_device: Target device string (e.g., 'cuda:0' or 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                net = net.to(target_device)\n                nets[i] = net\n\n\ndef clean_pycache(root_dir: str) -&gt; Tuple[int, int]:\n    \"\"\"Remove all __pycache__ directories and .pyc files under &#96;&#96;root_dir&#96;&#96;.\"\"\"\n    removed_dirs = 0\n    removed_files = 0\n\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        if \"__pycache__\" in dirnames:\n            pycache_path = os.path.join(dirpath, \"__pycache__\")\n            try:\n                shutil.rmtree(pycache_path)\n                print(f\"Removed: {pycache_path}\")\n                removed_dirs += 1\n            except Exception as exc:  # pragma: no cover - cleanup best effort\n                print(f\"Error removing {pycache_path}: {exc}\")\n\n        for filename in filenames:\n            if filename.endswith(\".pyc\"):\n                pyc_path = os.path.join(dirpath, filename)\n                try:\n                    os.remove(pyc_path)\n                    print(f\"Removed: {pyc_path}\")\n                    removed_files += 1\n                except Exception as exc:  # pragma: no cover - cleanup best effort\n                    print(f\"Error removing {pyc_path}: {exc}\")\n\n    print(\"=\" * 60)\n    print(\"Cleanup complete!\")\n    print(f\"  Removed {removed_dirs} __pycache__ directories\")\n    print(f\"  Removed {removed_files} .pyc files\")\n    print(\"=\" * 60)\n\n    return removed_dirs, removed_files\n\n\nif \"--clean-pycache\" in sys.argv or \"--clean-cache\" in sys.argv:\n    flag = \"--clean-pycache\" if \"--clean-pycache\" in sys.argv else \"--clean-cache\"\n    script_root = os.path.dirname(os.path.abspath(__file__))\n    print(f\"Cleaning Python cache files from: {script_root}\")\n    clean_pycache(script_root)\n    # Prevent the rest of the training script from running in cleanup-only mode\n    sys.exit(0)\n\nhas_gpu = torch.cuda.is_available()\nhas_mps = torch.backends.mps.is_built()\ndevice = \"mps\" if torch.backends.mps.is_built() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Clear GPU memory if using CUDA\nif device.startswith('cuda'):\n    torch.cuda.empty_cache()\n\n\nlam, rho_1, num_of_waves, tmax, _, _, _ = input_taker(wave, a, num_of_waves, TMAX_CFG, N_0, 0, N_r)\n\njeans, alpha = req_consts_calc(lam, rho_1)\n# Set initial velocity amplitude per perturbation type\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    k = 2*np.pi/lam\n    v_1 = (rho_1 / (rho_o if rho_o != 0 else 1.0)) * (alpha / k)\nelse:\n    v_1 = a * cs\n\nxmax = xmin + lam * num_of_waves\nymax = ymin + lam * num_of_waves\nzmax = zmin + lam * num_of_waves\n#zmax = 1.0\n\n# Initialize shared velocity fields for consistent PINN/FD initial conditions\nvx_np, vy_np, vz_np = None, None, None  # Default values for sinusoidal case\nif str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\n    if DIMENSION == 3:\n        vx_np, vy_np, vz_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=3)\n    else:\n        vx_np, vy_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=2)\n    \n    # Set shared velocity fields for plotting (only for 2D visualization)\n    if DIMENSION == 2:\n        from visualization.Plotting import set_shared_velocity_fields\n        set_shared_velocity_fields(vx_np, vy_np)\n\n# ==================== STANDARD PINN MODE ====================\nprint(\"Running in standard PINN mode (single network)...\")\n\nnet = PINN(n_harmonics=harmonics)\nnet = net.to(device)\nmse_cost_function = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\noptimizerL = torch.optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n\nif DIMENSION == 1:\n    astpn_rmin = [xmin, tmin]\n    astpn_rmax = [xmax, tmax]\nelif DIMENSION == 2:\n    astpn_rmin = [xmin, ymin, tmin]\n    astpn_rmax = [xmax, ymax, tmax]\nelif DIMENSION == 3:\n    astpn_rmin = [xmin, ymin, zmin, tmin]\n    astpn_rmax = [xmax, ymax, zmax, tmax]\nelse:\n    raise ValueError(f\"Unsupported DIMENSION={DIMENSION}\")\ncollocation_model = ASTPN(rmin=astpn_rmin, rmax=astpn_rmax, N_0=N_0, N_b=0, N_r=N_r, dimension=DIMENSION)\n\n# Set domain on the network so periodic embeddings enforce hard BCs\nspatial_rmin = [xmin]\nspatial_rmax = [xmax]\nif DIMENSION &gt;= 2:\n    spatial_rmin.append(ymin)\n    spatial_rmax.append(ymax)\nif DIMENSION &gt;= 3:\n    spatial_rmin.append(zmin)\n    spatial_rmax.append(zmax)\nnet.set_domain(rmin=spatial_rmin, rmax=spatial_rmax, dimension=DIMENSION)\n\n# IC collocation stays at t=0 throughout\ncollocation_IC = collocation_model.geo_time_coord(option=\"IC\")\n\n\n# Generate extra collocation points at t=0 for Poisson enforcement (Option 3)\nfrom core.data_generator import generate_poisson_ic_points\nfrom config import N_POISSON_IC\ncollocation_poisson_ic = generate_poisson_ic_points(\n    rmin=collocation_model.rmin,\n    rmax=collocation_model.rmax,\n    n_points=N_POISSON_IC,\n    dimension=DIMENSION,\n    device=device\n)\nprint(f\"Generated {N_POISSON_IC} extra collocation points at t=0 for Poisson enforcement\")\n\n\nstart_time = time.time()\n\n# Standard training\ncollocation_domain = collocation_model.geo_time_coord(option=\"Domain\")\n\nif USE_MULTI_STAGE_TRAINING:\n    print(\"Using multi-stage training...\")\n    training_diagnostics = train_multistage(\n        net=net,\n        model=collocation_model,\n        collocation_domain=collocation_domain,\n        collocation_IC=collocation_IC,\n        collocation_poisson_ic=collocation_poisson_ic,\n        optimizer=optimizer,\n        optimizerL=optimizerL,\n        mse_cost_function=mse_cost_function,\n        rho_1=rho_1,\n        lam=lam,\n        jeans=jeans,\n        v_1=v_1,\n        device=device,\n        stage1_iters=STAGE1_ADAM_ITERS,\n        stage2_iters=STAGE2_ADAM_ITERS,\n        stage3_iters=STAGE3_ADAM_ITERS,\n        stage1_lbfgs_iters=STAGE1_LBFGS_ITERS,\n        stage2_lbfgs_iters=STAGE2_LBFGS_ITERS,\n        stage3_lbfgs_iters=STAGE3_LBFGS_ITERS\n    )\nelse:\n    print(\"Using standard training...\")\n    training_diagnostics = train(\n        net=net,\n        model=collocation_model,\n        collocation_domain=collocation_domain,\n        collocation_IC=collocation_IC,\n        collocation_poisson_ic=collocation_poisson_ic,\n        optimizer=optimizer,\n        optimizerL=optimizerL,\n        closure=None,\n        mse_cost_function=mse_cost_function,\n        iteration_adam=iteration_adam_2D,\n        iterationL=iteration_lbgfs_2D,\n        rho_1=rho_1,\n        lam=lam,\n        jeans=jeans,\n        v_1=v_1,\n        device=device\n    )\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"[Timing] PINN training completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n\n# Save model immediately after training completes\n_save_trained_models(net)\n\n# Comprehensive GPU memory cleanup after training\n# This properly frees model parameters, optimizer state, collocation points, etc.\nif device.startswith('cuda'):\n    # Collect all training-related data for cleanup\n    cleanup_optimizers = [optimizer, optimizerL] if 'optimizer' in locals() and 'optimizerL' in locals() else None\n    \n    # Try to get collocation data references\n    try:\n        cleanup_collocation = {\n            'domain': collocation_domain if 'collocation_domain' in locals() else None,\n            'IC': collocation_IC if 'collocation_IC' in locals() else None,\n            'poisson_IC': collocation_poisson_ic if 'collocation_poisson_ic' in locals() else None\n        }\n    except:\n        cleanup_collocation = None\n    \n    # Perform comprehensive cleanup\n    # This frees GPU memory by deleting optimizer state and collocation data\n    # Models are temporarily moved to CPU during cleanup, then moved back to GPU for visualization\n    _comprehensive_gpu_cleanup(\n        nets=[net],\n        optimizers=cleanup_optimizers,\n        collocation_data=cleanup_collocation,\n        cached_ic_values=None,\n        keep_models_for_vis=True,  # Keep models for visualization\n        target_device='cpu'  # Temporarily move models to CPU during cleanup\n    )\n    \n    # Move model back to GPU for visualization (visualization code expects model on GPU)\n    # The FD solver will clear cache before it runs, so this is safe\n    if device.startswith('cuda'):\n        _move_models_to_device([net], device)\n        print(\"Model moved back to GPU for visualization\")\n\n# ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================\n# Run comprehensive diagnostics for high-tmax failure analysis\n# This generates 4 additional critical plots beyond the training diagnostics\nif ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:\n    print(\"\\n\" + \"=\"*70)\n    print(\"Running post-training comprehensive diagnostics...\")\n    print(\"=\"*70)\n    \n    try:\n        net.eval()\n        # Run comprehensive diagnostics - generates 4 additional plots:\n        # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks\n        # 2. Conservation Laws - Mass/momentum conservation\n        # 3. Spectral Evolution - Frequency content analysis\n        # 4. Temporal Statistics - Error accumulation tracking\n        training_diagnostics.run_comprehensive_diagnostics(\n            model=net,\n            dimension=DIMENSION,\n            tmax=tmax\n        )\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"All diagnostic plots generated successfully!\")\n        print(\"Check ./diagnostics/ folder for:\")\n        print(\"  1. training_diagnostics.png - Training convergence\")\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violations\")\n        print(\"  3. conservation_laws.png - Physical consistency\")\n        print(\"  4. spectral_evolution.png - Frequency content\")\n        print(\"  5. temporal_statistics.png - Field evolution\")\n        print(\"=\"*70 + \"\\n\")\n    except Exception as e:\n        print(f\"[WARNING] Post-training diagnostics failed: {e}\")\n        print(\"Training completed successfully, but diagnostic plots may be incomplete.\")\n        import traceback\n        traceback.print_exc()\n\n# ==================== VISUALIZATION ====================\ninitial_params = (xmin, xmax, ymin, ymax, rho_1, alpha, lam, \"temp\", tmax)\n\nanim_density = create_2d_animation(net, initial_params, which=\"density\", fps=10, verbose=False)\nanim_velocity = create_2d_animation(net, initial_params, which=\"velocity\", fps=10, verbose=False)\n\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    create_1d_cross_sections_sinusoidal(net, initial_params, time_points=None, y_fixed=SLICE_Y, N_fd=600, nu_fd=0.5)\n\nif PLOT_DENSITY_GROWTH:\n    try:\n        tmax_growth = float(GROWTH_PLOT_TMAX)\n    except Exception:\n        tmax_growth = float(TMAX_CFG)\n    dt_growth = float(GROWTH_PLOT_DT)\n    create_density_growth_plot(net, initial_params, tmax=tmax_growth, dt=dt_growth)\n\n# ==================== FINAL CACHE CLEANUP ====================\nscript_root = os.path.dirname(os.path.abspath(__file__))\nprint(\"Performing final Python cache cleanup...\")\nclean_pycache(script_root)\n\n"
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 1 file</summary>
      
Lint paths:

- `train.py`

Lint results


**No lint errors found**
</details>
</tool-use>

---

Moved the post‑training diagnostics block to run **after cleanup + eval** and after the model is moved back to GPU, matching your requested flow. Also ensured `net.eval()` is set before diagnostics run.

If you want diagnostics to run on CPU instead (to save GPU for visualization), I can switch that too.

---

