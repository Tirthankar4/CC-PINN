<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 9eaa10db-364f-4af9-9ce8-c5077aada666 (2026-01-26 13:05Z) -->

# Multi stage training code removal (2026-01-26 13:05Z)

_**User (2026-01-26 13:00Z)**_

Multi stage training was implemented for some tests. I don't need it anymore, so remove all codes related to that please

---

_**Agent (model gpt-5.2-codex, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Exploring config.py for file reading utilities**

I'm searching config.py to find how read_file is implemented and whether it can help with the requested edits. I'll decide next steps based on what I find there.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\config.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Searching multi-stage usage**</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "multi[-_ ]stage|STAGE\\d|USE_MULTI_STAGE_TRAINING",
  "path": ".",
  "outputMode": "content",
  "caseInsensitive": false,
  "headLimit": 200
}
```
Additional data:

```json
{
  "isPruned": true,
  "pattern": "multi[-_ ]stage|STAGE\\d|USE_MULTI_STAGE_TRAINING",
  "path": ".",
  "outputMode": "content",
  "totalFiles": 3,
  "totalMatches": 23,
  "topFiles": [
    {
      "uri": ".\\config.py",
      "matchCount": 9
    },
    {
      "uri": ".\\train.py",
      "matchCount": 10
    },
    {
      "uri": ".\\training\\trainer.py",
      "matchCount": 4
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Reviewing multi-stage code removal**

I'm inspecting train.py and trainer.py to identify and remove multi-stage code.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\train.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Identifying multi-stage training code**

I'm locating multi-stage logic and related config variables in trainer.py to plan their removal.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\trainer.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\trainer.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Removing multistage training references**

I'm planning to delete train_multistage and all related config variables, including STAGE3_POISSON_WEIGHT, from the config and trainer imports. Next, I'll search the trainer code for older function usages tied to this multistage approach to clean them out.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "STAGE3_POISSON_WEIGHT|train_multistage|multi-stage|multistage|STAGE\\d|USE_MULTI_STAGE_TRAINING",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations",
  "outputMode": "content",
  "caseInsensitive": false,
  "headLimit": 200
}
```
Additional data:

```json
{
  "isPruned": true,
  "pattern": "STAGE3_POISSON_WEIGHT|train_multistage|multi-stage|multistage|STAGE\\d|USE_MULTI_STAGE_TRAINING",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations",
  "outputMode": "content",
  "totalFiles": 3,
  "totalMatches": 26,
  "topFiles": [
    {
      "uri": ".\\config.py",
      "matchCount": 9
    },
    {
      "uri": ".\\train.py",
      "matchCount": 12
    },
    {
      "uri": ".\\training\\trainer.py",
      "matchCount": 5
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Inspecting training function for multistage removal**

I'm verifying the train() function to confirm it uses the correct Poisson IC weights and to remove multistage logic and config variables accordingly.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "def train\\(|train\\(",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\trainer.py",
  "outputMode": "content",
  "caseInsensitive": false,
  "headLimit": 20
}
```
Additional data:

```json
{
  "isPruned": true,
  "pattern": "def train\\(|train\\(",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\trainer.py",
  "outputMode": "content",
  "totalFiles": 1,
  "totalMatches": 1,
  "topFiles": [
    {
      "uri": "training/trainer.py",
      "matchCount": 1
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\trainer.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Planning multi-stage removal**

Identifying all multi-stage related code and config references for removal, including the train_multistage function, related imports, constants, and possibly the _compute_poisson_ic_penalties function if it’s not used outside multi-stage training. Next, I'll verify _compute_poisson_ic_penalties usage across the codebase before deleting.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "_compute_poisson_ic_penalties",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations",
  "outputMode": "content",
  "caseInsensitive": false,
  "headLimit": 20
}
```
Additional data:

```json
{
  "isPruned": true,
  "pattern": "_compute_poisson_ic_penalties",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations",
  "outputMode": "content",
  "totalFiles": 1,
  "totalMatches": 3,
  "topFiles": [
    {
      "uri": ".\\training\\trainer.py",
      "matchCount": 3
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Removing multistage training artifacts**

I'm eliminating the multistage training function and related config values except for non-multistage ones like N_POISSON_IC, cleaning up imports and branches in train.py accordingly, and preparing patches for these removals.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\config.py"
}
```
Additional data:

```json
{
  "codeblockId": "d87f2191-102e-4449-b260-efe40e939d5d",
  "reviewData": {
    "status": "None",
    "selectedOption": "accept",
    "isShowingInput": false,
    "firstTimeReviewMode": false
  },
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/config.py",
      "query": "",
      "fragment": "",
      "_formatted": null,
      "_fsPath": null
    },
    "codeblockId": "d87f2191-102e-4449-b260-efe40e939d5d",
    "codeBlockIdx": 0,
    "content": "import numpy as np\n\n# Random seed for reproducibility across all functions\nRANDOM_SEED = 93\n\n# Perturbation selection: \"power_spectrum\" or \"sinusoidal\"\nPERTURBATION_TYPE = \"power_spectrum\"\n\nxmin = 0.0\nymin = 0.0\nzmin = 0.0\ncs = 1.0\nrho_o = 1.0\nconst = 1.0\nG = 1.0\n\n# Collocation point parameters\nN_0 = 40000  # Number of initial condition points\nN_r = 40000 # Number of residual/collocation points\nDIMENSION = 2  # Spatial dimension\n\n# Number of collocation/IC points per mini-batch and\n# how many such mini-batches to aggregate in a single optimizer step\nBATCH_SIZE = 80000\nNUM_BATCHES = 1\n\na = 0.05\n\ntmin = 0.\ntmax = 3.0\n\nnum_neurons = 64\nharmonics = 3\nnum_layers = 5\nDEFAULT_ACTIVATION = 'sin'  # Default activation function for PINN ('sin', 'tanh', 'relu', etc.)\n\nwave = 7.0\nk = 2 * np.pi / wave\nnum_of_waves = 2.0\n\niteration_adam_2D = 1001\niteration_lbgfs_2D = 201\n\nIC_WEIGHT = 1.0\n\n# Extra collocation points at t=0 to strongly enforce Poisson equation\nN_POISSON_IC = 0  # Number of extra spatial points at t=0 for Poisson\nPOISSON_IC_WEIGHT = 0.0  # Weight for Poisson residual at t=0\nPHI_MEAN_CONSTRAINT_WEIGHT = 0.0  # Weight for mean(φ)=0 constraint at t=0 (fixes gauge freedom)\n\n# Output/snapshot controls\nSAVE_STATIC_SNAPSHOTS = False\n\n# Directory to save snapshots; default keeps Kaggle working dir\nSNAPSHOT_DIR = \"/kaggle/working/\"\n\n# Training diagnostics\nENABLE_TRAINING_DIAGNOSTICS = True  # Enable automatic training diagnostics plots and logging\n\n# Density growth comparison plot controls\nPLOT_DENSITY_GROWTH = False\nGROWTH_PLOT_TMAX = 4.0\nGROWTH_PLOT_DT = 0.1\n\nKX = k\nKY = 0\nKZ = 0\nTIMES_1D = [2.0, 2.5, 3.0] # 1D cross-section times to plot (used for sinusoidal panel plots\nFD_N_1D = 300  # Grid points for 1D LAX (when used)\nFD_N_2D = 300  # Grid points per dimension for 2D LAX\nFD_N_3D = 300  # Grid points per dimension for 3D LAX slices\nSLICE_Y = 0.6  # Default y slice for visualization/cross-sections\nSLICE_Z = 0.6  # Default z slice for visualization/cross-sections\nSHOW_LINEAR_THEORY = False\n\n# 3D Interactive plot settings\nENABLE_INTERACTIVE_3D = True  # Toggle interactive 3D plot generation\nINTERACTIVE_3D_RESOLUTION = 50  # Grid resolution per axis\nINTERACTIVE_3D_TIME_STEPS = 10  # Number of time steps in slider\n\n# Power spectrum parameters\nN_GRID = 400  # Grid resolution for power spectrum generation\nN_GRID_3D = 400 #In 3D\nPOWER_EXPONENT = 0  # Power spectrum exponent\nSTARTUP_DT = 0.01 # Time offset after which PDE is enforced (ICs remain at t=0)\nUSE_PARAMETERIZATION = \"exponential\"  # Options: \"exponential\", \"linear\", \"none\"",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "  \n  IC_WEIGHT = 1.0\n- \n- # Training mode: standard single-stage or multi-stage\n- USE_MULTI_STAGE_TRAINING = False\n- \n- # Multi-stage iteration counts (Adam + L-BFGS) for each stage\n- STAGE1_ADAM_ITERS = 100\n- STAGE2_ADAM_ITERS = 300\n- STAGE3_ADAM_ITERS = 500\n- \n- STAGE1_LBFGS_ITERS = 0\n- STAGE2_LBFGS_ITERS = 75\n- STAGE3_LBFGS_ITERS = 100\n- \n- # Multi-stage loss weights\n- STAGE3_POISSON_WEIGHT = 10.0\n  \n  # Extra collocation points at t=0 to strongly enforce Poisson equation",
        "oldStart": 43,
        "newStart": 43,
        "oldLines": 19,
        "newLines": 4,
        "linesRemoved": 15
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\config.py",
  "contentsAfterEdit": "import numpy as np\n\n# Random seed for reproducibility across all functions\nRANDOM_SEED = 93\n\n# Perturbation selection: \"power_spectrum\" or \"sinusoidal\"\nPERTURBATION_TYPE = \"power_spectrum\"\n\nxmin = 0.0\nymin = 0.0\nzmin = 0.0\ncs = 1.0\nrho_o = 1.0\nconst = 1.0\nG = 1.0\n\n# Collocation point parameters\nN_0 = 40000  # Number of initial condition points\nN_r = 40000 # Number of residual/collocation points\nDIMENSION = 2  # Spatial dimension\n\n# Number of collocation/IC points per mini-batch and\n# how many such mini-batches to aggregate in a single optimizer step\nBATCH_SIZE = 80000\nNUM_BATCHES = 1\n\na = 0.05\n\ntmin = 0.\ntmax = 3.0\n\nnum_neurons = 64\nharmonics = 3\nnum_layers = 5\nDEFAULT_ACTIVATION = 'sin'  # Default activation function for PINN ('sin', 'tanh', 'relu', etc.)\n\nwave = 7.0\nk = 2 * np.pi / wave\nnum_of_waves = 2.0\n\niteration_adam_2D = 1001\niteration_lbgfs_2D = 201\n\nIC_WEIGHT = 1.0\n\n# Extra collocation points at t=0 to strongly enforce Poisson equation\nN_POISSON_IC = 0  # Number of extra spatial points at t=0 for Poisson\nPOISSON_IC_WEIGHT = 0.0  # Weight for Poisson residual at t=0\nPHI_MEAN_CONSTRAINT_WEIGHT = 0.0  # Weight for mean(φ)=0 constraint at t=0 (fixes gauge freedom)\n\n# Output/snapshot controls\nSAVE_STATIC_SNAPSHOTS = False\n\n# Directory to save snapshots; default keeps Kaggle working dir\nSNAPSHOT_DIR = \"/kaggle/working/\"\n\n# Training diagnostics\nENABLE_TRAINING_DIAGNOSTICS = True  # Enable automatic training diagnostics plots and logging\n\n# Density growth comparison plot controls\nPLOT_DENSITY_GROWTH = False\nGROWTH_PLOT_TMAX = 4.0\nGROWTH_PLOT_DT = 0.1\n\nKX = k\nKY = 0\nKZ = 0\nTIMES_1D = [2.0, 2.5, 3.0] # 1D cross-section times to plot (used for sinusoidal panel plots\nFD_N_1D = 300  # Grid points for 1D LAX (when used)\nFD_N_2D = 300  # Grid points per dimension for 2D LAX\nFD_N_3D = 300  # Grid points per dimension for 3D LAX slices\nSLICE_Y = 0.6  # Default y slice for visualization/cross-sections\nSLICE_Z = 0.6  # Default z slice for visualization/cross-sections\nSHOW_LINEAR_THEORY = False\n\n# 3D Interactive plot settings\nENABLE_INTERACTIVE_3D = True  # Toggle interactive 3D plot generation\nINTERACTIVE_3D_RESOLUTION = 50  # Grid resolution per axis\nINTERACTIVE_3D_TIME_STEPS = 10  # Number of time steps in slider\n\n# Power spectrum parameters\nN_GRID = 400  # Grid resolution for power spectrum generation\nN_GRID_3D = 400 #In 3D\nPOWER_EXPONENT = 0  # Power spectrum exponent\nSTARTUP_DT = 0.01 # Time offset after which PDE is enforced (ICs remain at t=0)\nUSE_PARAMETERIZATION = \"exponential\"  # Options: \"exponential\", \"linear\", \"none\""
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Updating train.py to use apply_patch**

I'm planning to remove certain imports and branching logic from train.py and replace them with apply_patch for a cleaner approach.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\train.py"
}
```
Additional data:

```json
{
  "codeblockId": "c7d9aa43-2b53-447b-996d-60b1e8574ff6",
  "reviewData": {
    "status": "None",
    "selectedOption": "accept",
    "isShowingInput": false,
    "firstTimeReviewMode": false
  },
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/train.py",
      "query": "",
      "fragment": "",
      "_formatted": null,
      "_fsPath": null
    },
    "codeblockId": "c7d9aa43-2b53-447b-996d-60b1e8574ff6",
    "codeBlockIdx": 0,
    "content": "import os\nimport sys\nimport shutil\nimport numpy as np\nimport time\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\n\n# ==================== PyTorch Performance Optimizations ====================\n# Enable cuDNN autotuner - finds optimal convolution algorithms\n# This helps when input sizes are consistent (which they are in PINN training)\ntorch.backends.cudnn.benchmark = True\n\n# Enable TF32 on Ampere GPUs (A100, RTX 3090, RTX 4090, etc.) for 20-30% speedup\n# TF32 provides faster matrix multiplication with minimal accuracy loss\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    # Prefer explicit device/dtype settings rather than deprecated tensor-type override\n    torch.set_default_dtype(torch.float32)\n    if hasattr(torch, \"set_default_device\"):\n        torch.set_default_device(\"cuda\")\n    print(\"PyTorch performance optimizations enabled (cuDNN benchmark, TF32)\")\n\nfrom core.data_generator import input_taker, req_consts_calc\nfrom training.trainer import train\nfrom core.initial_conditions import initialize_shared_velocity_fields\nfrom config import BATCH_SIZE, NUM_BATCHES, N_0, N_r, DIMENSION\nfrom config import a, wave, cs, xmin, ymin, zmin, tmin, tmax as TMAX_CFG, iteration_adam_2D, iteration_lbgfs_2D, harmonics, PERTURBATION_TYPE, rho_o, SLICE_Y\nfrom config import num_neurons, num_layers, num_of_waves\nfrom config import RANDOM_SEED, STARTUP_DT\nfrom config import ENABLE_TRAINING_DIAGNOSTICS\nfrom core.losses import ASTPN\nfrom core.model_architecture import PINN\nfrom visualization.Plotting import create_2d_animation\nfrom visualization.Plotting import create_1d_cross_sections_sinusoidal\nfrom visualization.Plotting import create_density_growth_plot\nfrom config import PLOT_DENSITY_GROWTH, GROWTH_PLOT_TMAX, GROWTH_PLOT_DT\nfrom config import FD_N_2D\nfrom config import ENABLE_INTERACTIVE_3D, INTERACTIVE_3D_RESOLUTION, INTERACTIVE_3D_TIME_STEPS\n\n\ndef _save_trained_models(net):\n    \"\"\"Persist trained model immediately after training completes.\"\"\"\n    try:\n        from config import SNAPSHOT_DIR\n        model_dir = os.path.join(SNAPSHOT_DIR, \"GRINN\")\n        os.makedirs(model_dir, exist_ok=True)\n        model_path = os.path.join(model_dir, \"model.pth\")\n        torch.save(net.state_dict(), model_path)\n        print(f\"Saved model to {model_path}\")\n    except Exception as e:\n        print(f\"Warning: failed to save model: {e}\")\n\n\ndef _comprehensive_gpu_cleanup(nets, optimizers=None, collocation_data=None, \n                                cached_ic_values=None, keep_models_for_vis=True, \n                                target_device=None):\n    \"\"\"\n    Comprehensive GPU memory cleanup after PINN training.\n    \n    This function properly frees GPU memory by:\n    1. Moving models to CPU (if keep_models_for_vis=True) or deleting them\n    2. Deleting optimizer state (Adam momentum buffers, LBFGS history)\n    3. Deleting collocation points and related tensors\n    4. Deleting cached IC values\n    5. Clearing PyTorch's cache allocator\n    \n    Args:\n        nets: List of neural networks (or single network in a list)\n        optimizers: List of optimizers to clean up (optional)\n        collocation_data: Dict or list of collocation tensors to delete (optional)\n        cached_ic_values: List of cached IC value dicts to delete (optional)\n        keep_models_for_vis: If True, move models to CPU instead of deleting (default: True)\n        target_device: Target device for models if keep_models_for_vis=True (default: 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if not torch.cuda.is_available():\n        return\n    \n    if target_device is None:\n        target_device = 'cpu'\n    \n    print(\"Starting comprehensive GPU memory cleanup...\")\n    \n    # 1. Set models to eval mode (reduces memory usage) and optionally move to CPU\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                # Set to eval mode to disable gradient tracking and reduce memory\n                net.eval()\n                # Clear any cached gradients\n                for param in net.parameters():\n                    if param.grad is not None:\n                        param.grad = None\n                \n                if keep_models_for_vis:\n                    # Move to target device (usually CPU) for visualization\n                    # Models can be moved back to GPU later if needed\n                    net = net.to(target_device)\n                    nets[i] = net\n                    print(f\"  Moved network {i} to {target_device} (eval mode)\")\n                else:\n                    # Delete model entirely\n                    del net\n                    nets[i] = None\n                    print(f\"  Deleted network {i}\")\n    \n    # 2. Delete optimizer state (can be large, especially LBFGS history)\n    if optimizers is not None:\n        if isinstance(optimizers, (list, tuple)):\n            for i, opt in enumerate(optimizers):\n                if opt is not None:\n                    # Clear optimizer state (this frees momentum buffers, LBFGS history, etc.)\n                    opt.state.clear()\n                    opt.param_groups.clear()\n                    del opt\n                    print(f\"  Deleted optimizer {i} state\")\n        else:\n            if optimizers is not None:\n                optimizers.state.clear()\n                optimizers.param_groups.clear()\n                del optimizers\n                print(\"  Deleted optimizer state\")\n    \n    # 3. Delete collocation points (these can be very large)\n    if collocation_data is not None:\n        if isinstance(collocation_data, dict):\n            for key, value in collocation_data.items():\n                if value is not None:\n                    if isinstance(value, (list, tuple)):\n                        for item in value:\n                            if isinstance(item, torch.Tensor) and item.is_cuda:\n                                del item\n                    elif isinstance(value, torch.Tensor) and value.is_cuda:\n                        del value\n            collocation_data.clear()\n            print(\"  Deleted collocation data dict\")\n        elif isinstance(collocation_data, (list, tuple)):\n            for item in collocation_data:\n                if item is not None:\n                    if isinstance(item, (list, tuple)):\n                        for subitem in item:\n                            if isinstance(subitem, torch.Tensor) and subitem.is_cuda:\n                                del subitem\n                    elif isinstance(item, torch.Tensor) and item.is_cuda:\n                        del item\n            print(\"  Deleted collocation data list\")\n        elif isinstance(collocation_data, torch.Tensor) and collocation_data.is_cuda:\n            del collocation_data\n            print(\"  Deleted collocation tensor\")\n    \n    # 4. Delete cached IC values\n    if cached_ic_values is not None:\n        if isinstance(cached_ic_values, list):\n            for i, cached_dict in enumerate(cached_ic_values):\n                if cached_dict is not None and isinstance(cached_dict, dict):\n                    for key, value in cached_dict.items():\n                        if isinstance(value, torch.Tensor) and value.is_cuda:\n                            del value\n                    cached_dict.clear()\n            cached_ic_values.clear()\n            print(\"  Deleted cached IC values\")\n    \n    # 5. Force garbage collection\n    import gc\n    gc.collect()\n    \n    # 6. Clear PyTorch's cache allocator and synchronize\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    print(\"GPU memory cleanup completed\")\n    \n    # Note: If visualization needs models on GPU, they can be moved back with:\n    # for net in nets:\n    #     if net is not None:\n    #         net = net.to(device)\n\n\ndef _move_models_to_device(nets, target_device):\n    \"\"\"\n    Move models to target device (e.g., back to GPU for visualization).\n    \n    Args:\n        nets: List of neural networks\n        target_device: Target device string (e.g., 'cuda:0' or 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                net = net.to(target_device)\n                nets[i] = net\n\n\ndef clean_pycache(root_dir: str) -&gt; Tuple[int, int]:\n    \"\"\"Remove all __pycache__ directories and .pyc files under &#96;&#96;root_dir&#96;&#96;.\"\"\"\n    removed_dirs = 0\n    removed_files = 0\n\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        if \"__pycache__\" in dirnames:\n            pycache_path = os.path.join(dirpath, \"__pycache__\")\n            try:\n                shutil.rmtree(pycache_path)\n                print(f\"Removed: {pycache_path}\")\n                removed_dirs += 1\n            except Exception as exc:  # pragma: no cover - cleanup best effort\n                print(f\"Error removing {pycache_path}: {exc}\")\n\n        for filename in filenames:\n            if filename.endswith(\".pyc\"):\n                pyc_path = os.path.join(dirpath, filename)\n                try:\n                    os.remove(pyc_path)\n                    print(f\"Removed: {pyc_path}\")\n                    removed_files += 1\n                except Exception as exc:  # pragma: no cover - cleanup best effort\n                    print(f\"Error removing {pyc_path}: {exc}\")\n\n    print(\"=\" * 60)\n    print(\"Cleanup complete!\")\n    print(f\"  Removed {removed_dirs} __pycache__ directories\")\n    print(f\"  Removed {removed_files} .pyc files\")\n    print(\"=\" * 60)\n\n    return removed_dirs, removed_files\n\n\nif \"--clean-pycache\" in sys.argv or \"--clean-cache\" in sys.argv:\n    flag = \"--clean-pycache\" if \"--clean-pycache\" in sys.argv else \"--clean-cache\"\n    script_root = os.path.dirname(os.path.abspath(__file__))\n    print(f\"Cleaning Python cache files from: {script_root}\")\n    clean_pycache(script_root)\n    # Prevent the rest of the training script from running in cleanup-only mode\n    sys.exit(0)\n\nhas_gpu = torch.cuda.is_available()\nhas_mps = torch.backends.mps.is_built()\ndevice = \"mps\" if torch.backends.mps.is_built() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Clear GPU memory if using CUDA\nif device.startswith('cuda'):\n    torch.cuda.empty_cache()\n\n\nlam, rho_1, num_of_waves, tmax, _, _, _ = input_taker(wave, a, num_of_waves, TMAX_CFG, N_0, 0, N_r)\n\njeans, alpha = req_consts_calc(lam, rho_1)\n# Set initial velocity amplitude per perturbation type\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    k = 2*np.pi/lam\n    v_1 = (rho_1 / (rho_o if rho_o != 0 else 1.0)) * (alpha / k)\nelse:\n    v_1 = a * cs\n\nxmax = xmin + lam * num_of_waves\nymax = ymin + lam * num_of_waves\nzmax = zmin + lam * num_of_waves\n#zmax = 1.0\n\n# Initialize shared velocity fields for consistent PINN/FD initial conditions\nvx_np, vy_np, vz_np = None, None, None  # Default values for sinusoidal case\nif str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\n    if DIMENSION == 3:\n        vx_np, vy_np, vz_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=3)\n    else:\n        vx_np, vy_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=2)\n    \n    # Set shared velocity fields for plotting (only for 2D visualization)\n    if DIMENSION == 2:\n        from visualization.Plotting import set_shared_velocity_fields\n        set_shared_velocity_fields(vx_np, vy_np)\n\n# ==================== STANDARD PINN MODE ====================\nprint(\"Running in standard PINN mode (single network)...\")\n\nnet = PINN(n_harmonics=harmonics)\nnet = net.to(device)\nmse_cost_function = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\noptimizerL = torch.optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n\nif DIMENSION == 1:\n    astpn_rmin = [xmin, tmin]\n    astpn_rmax = [xmax, tmax]\nelif DIMENSION == 2:\n    astpn_rmin = [xmin, ymin, tmin]\n    astpn_rmax = [xmax, ymax, tmax]\nelif DIMENSION == 3:\n    astpn_rmin = [xmin, ymin, zmin, tmin]\n    astpn_rmax = [xmax, ymax, zmax, tmax]\nelse:\n    raise ValueError(f\"Unsupported DIMENSION={DIMENSION}\")\ncollocation_model = ASTPN(rmin=astpn_rmin, rmax=astpn_rmax, N_0=N_0, N_b=0, N_r=N_r, dimension=DIMENSION)\n\n# Set domain on the network so periodic embeddings enforce hard BCs\nspatial_rmin = [xmin]\nspatial_rmax = [xmax]\nif DIMENSION &gt;= 2:\n    spatial_rmin.append(ymin)\n    spatial_rmax.append(ymax)\nif DIMENSION &gt;= 3:\n    spatial_rmin.append(zmin)\n    spatial_rmax.append(zmax)\nnet.set_domain(rmin=spatial_rmin, rmax=spatial_rmax, dimension=DIMENSION)\n\n# IC collocation stays at t=0 throughout\ncollocation_IC = collocation_model.geo_time_coord(option=\"IC\")\n\n\n# Generate extra collocation points at t=0 for Poisson enforcement (Option 3)\nfrom core.data_generator import generate_poisson_ic_points\nfrom config import N_POISSON_IC\ncollocation_poisson_ic = generate_poisson_ic_points(\n    rmin=collocation_model.rmin,\n    rmax=collocation_model.rmax,\n    n_points=N_POISSON_IC,\n    dimension=DIMENSION,\n    device=device\n)\nprint(f\"Generated {N_POISSON_IC} extra collocation points at t=0 for Poisson enforcement\")\n\n\nstart_time = time.time()\n\n# Standard training\ncollocation_domain = collocation_model.geo_time_coord(option=\"Domain\")\n\nprint(\"Using standard training...\")\ntraining_diagnostics = train(\n    net=net,\n    model=collocation_model,\n    collocation_domain=collocation_domain,\n    collocation_IC=collocation_IC,\n    collocation_poisson_ic=collocation_poisson_ic,\n    optimizer=optimizer,\n    optimizerL=optimizerL,\n    closure=None,\n    mse_cost_function=mse_cost_function,\n    iteration_adam=iteration_adam_2D,\n    iterationL=iteration_lbgfs_2D,\n    rho_1=rho_1,\n    lam=lam,\n    jeans=jeans,\n    v_1=v_1,\n    device=device\n)\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"[Timing] PINN training completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n\n# Save model immediately after training completes\n_save_trained_models(net)\n\n# Comprehensive GPU memory cleanup after training\n# This properly frees model parameters, optimizer state, collocation points, etc.\nif device.startswith('cuda'):\n    # Collect all training-related data for cleanup\n    cleanup_optimizers = [optimizer, optimizerL] if 'optimizer' in locals() and 'optimizerL' in locals() else None\n    \n    # Try to get collocation data references\n    try:\n        cleanup_collocation = {\n            'domain': collocation_domain if 'collocation_domain' in locals() else None,\n            'IC': collocation_IC if 'collocation_IC' in locals() else None,\n            'poisson_IC': collocation_poisson_ic if 'collocation_poisson_ic' in locals() else None\n        }\n    except:\n        cleanup_collocation = None\n    \n    # Perform comprehensive cleanup\n    # This frees GPU memory by deleting optimizer state and collocation data\n    # Models are temporarily moved to CPU during cleanup, then moved back to GPU for visualization\n    _comprehensive_gpu_cleanup(\n        nets=[net],\n        optimizers=cleanup_optimizers,\n        collocation_data=cleanup_collocation,\n        cached_ic_values=None,\n        keep_models_for_vis=True,  # Keep models for visualization\n        target_device='cpu'  # Temporarily move models to CPU during cleanup\n    )\n    \n    # Move model back to GPU for visualization (visualization code expects model on GPU)\n    # The FD solver will clear cache before it runs, so this is safe\n    if device.startswith('cuda'):\n        _move_models_to_device([net], device)\n        print(\"Model moved back to GPU for visualization\")\n\n# ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================\n# Run comprehensive diagnostics for high-tmax failure analysis\n# This generates 4 additional critical plots beyond the training diagnostics\nif ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:\n    print(\"\\n\" + \"=\"*70)\n    print(\"Running post-training comprehensive diagnostics...\")\n    print(\"=\"*70)\n    \n    try:\n        net.eval()\n        # Run comprehensive diagnostics - generates 4 additional plots:\n        # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks\n        # 2. Conservation Laws - Mass/momentum conservation\n        # 3. Spectral Evolution - Frequency content analysis\n        # 4. Temporal Statistics - Error accumulation tracking\n        training_diagnostics.run_comprehensive_diagnostics(\n            model=net,\n            dimension=DIMENSION,\n            tmax=tmax\n        )\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"All diagnostic plots generated successfully!\")\n        print(\"Check ./diagnostics/ folder for:\")\n        print(\"  1. training_diagnostics.png - Training convergence\")\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violations\")\n        print(\"  3. conservation_laws.png - Physical consistency\")\n        print(\"  4. spectral_evolution.png - Frequency content\")\n        print(\"  5. temporal_statistics.png - Field evolution\")\n        print(\"=\"*70 + \"\\n\")\n    except Exception as e:\n        print(f\"[WARNING] Post-training diagnostics failed: {e}\")\n        print(\"Training completed successfully, but diagnostic plots may be incomplete.\")\n        import traceback\n        traceback.print_exc()\n\n# ==================== VISUALIZATION ====================\ninitial_params = (xmin, xmax, ymin, ymax, rho_1, alpha, lam, \"temp\", tmax)\n\nanim_density = create_2d_animation(net, initial_params, which=\"density\", fps=10, verbose=False)\nanim_velocity = create_2d_animation(net, initial_params, which=\"velocity\", fps=10, verbose=False)\n\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    create_1d_cross_sections_sinusoidal(net, initial_params, time_points=None, y_fixed=SLICE_Y, N_fd=600, nu_fd=0.5)\n\nif PLOT_DENSITY_GROWTH:\n    try:\n        tmax_growth = float(GROWTH_PLOT_TMAX)\n    except Exception:\n        tmax_growth = float(TMAX_CFG)\n    dt_growth = float(GROWTH_PLOT_DT)\n    create_density_growth_plot(net, initial_params, tmax=tmax_growth, dt=dt_growth)\n\n# ==================== 3D INTERACTIVE VISUALIZATION ====================\nif DIMENSION == 3 and ENABLE_INTERACTIVE_3D:\n    try:\n        from visualization.Interactive import create_interactive_3d_plot\n        from config import SNAPSHOT_DIR\n\n        print(\"\\n\" + \"=\"*70)\n        print(\"Generating 3D interactive plot...\")\n        print(\"=\"*70)\n\n        save_path = os.path.join(SNAPSHOT_DIR, \"interactive_3d_plot.html\")\n        create_interactive_3d_plot(\n            net=net,\n            initial_params=initial_params,\n            time_range=(tmin, tmax),\n            time_steps=INTERACTIVE_3D_TIME_STEPS,\n            resolution=INTERACTIVE_3D_RESOLUTION,\n            save_path=save_path,\n        )\n\n        print(f\"Interactive 3D plot saved to: {save_path}\")\n        print(\"=\"*70 + \"\\n\")\n    except Exception as e:\n        print(f\"[WARNING] 3D interactive plot generation failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# ==================== FINAL CACHE CLEANUP ====================\nscript_root = os.path.dirname(os.path.abspath(__file__))\nprint(\"Performing final Python cache cleanup...\")\nclean_pycache(script_root)\n\n",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "  \n  from core.data_generator import input_taker, req_consts_calc\n- from training.trainer import train, train_multistage\n+ from training.trainer import train\n  from core.initial_conditions import initialize_shared_velocity_fields\n  from config import BATCH_SIZE, NUM_BATCHES, N_0, N_r, DIMENSION",
        "oldStart": 25,
        "newStart": 25,
        "oldLines": 5,
        "newLines": 5,
        "linesRemoved": 1,
        "linesAdded": 1
      },
      {
        "diffString": "  from config import RANDOM_SEED, STARTUP_DT\n  from config import ENABLE_TRAINING_DIAGNOSTICS\n- from config import USE_MULTI_STAGE_TRAINING, STAGE1_ADAM_ITERS, STAGE2_ADAM_ITERS, STAGE3_ADAM_ITERS\n- from config import STAGE1_LBFGS_ITERS, STAGE2_LBFGS_ITERS, STAGE3_LBFGS_ITERS\n  from core.losses import ASTPN\n  from core.model_architecture import PINN",
        "oldStart": 32,
        "newStart": 32,
        "oldLines": 6,
        "newLines": 4,
        "linesRemoved": 2
      },
      {
        "diffString": "  collocation_domain = collocation_model.geo_time_coord(option=\"Domain\")\n  \n- if USE_MULTI_STAGE_TRAINING:\n-     print(\"Using multi-stage training...\")\n-     training_diagnostics = train_multistage(\n-         net=net,\n-         model=collocation_model,\n-         collocation_domain=collocation_domain,\n-         collocation_IC=collocation_IC,\n-         collocation_poisson_ic=collocation_poisson_ic,\n-         optimizer=optimizer,\n-         optimizerL=optimizerL,\n-         mse_cost_function=mse_cost_function,\n-         rho_1=rho_1,\n-         lam=lam,\n-         jeans=jeans,\n-         v_1=v_1,\n-         device=device,\n-         stage1_iters=STAGE1_ADAM_ITERS,\n-         stage2_iters=STAGE2_ADAM_ITERS,\n-         stage3_iters=STAGE3_ADAM_ITERS,\n-         stage1_lbfgs_iters=STAGE1_LBFGS_ITERS,\n-         stage2_lbfgs_iters=STAGE2_LBFGS_ITERS,\n-         stage3_lbfgs_iters=STAGE3_LBFGS_ITERS\n-     )\n- else:\n-     print(\"Using standard training...\")\n-     training_diagnostics = train(\n-         net=net,\n-         model=collocation_model,\n-         collocation_domain=collocation_domain,\n-         collocation_IC=collocation_IC,\n-         collocation_poisson_ic=collocation_poisson_ic,\n-         optimizer=optimizer,\n-         optimizerL=optimizerL,\n-         closure=None,\n-         mse_cost_function=mse_cost_function,\n-         iteration_adam=iteration_adam_2D,\n-         iterationL=iteration_lbgfs_2D,\n-         rho_1=rho_1,\n-         lam=lam,\n-         jeans=jeans,\n-         v_1=v_1,\n-         device=device\n-     )\n+ print(\"Using standard training...\")\n+ training_diagnostics = train(\n+     net=net,\n+     model=collocation_model,\n+     collocation_domain=collocation_domain,\n+     collocation_IC=collocation_IC,\n+     collocation_poisson_ic=collocation_poisson_ic,\n+     optimizer=optimizer,\n+     optimizerL=optimizerL,\n+     closure=None,\n+     mse_cost_function=mse_cost_function,\n+     iteration_adam=iteration_adam_2D,\n+     iterationL=iteration_lbgfs_2D,\n+     rho_1=rho_1,\n+     lam=lam,\n+     jeans=jeans,\n+     v_1=v_1,\n+     device=device\n+ )\n  \n  end_time = time.time()",
        "oldStart": 336,
        "newStart": 334,
        "oldLines": 47,
        "newLines": 23,
        "linesRemoved": 43,
        "linesAdded": 19
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\train.py",
  "contentsAfterEdit": "import os\nimport sys\nimport shutil\nimport numpy as np\nimport time\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\n\n# ==================== PyTorch Performance Optimizations ====================\n# Enable cuDNN autotuner - finds optimal convolution algorithms\n# This helps when input sizes are consistent (which they are in PINN training)\ntorch.backends.cudnn.benchmark = True\n\n# Enable TF32 on Ampere GPUs (A100, RTX 3090, RTX 4090, etc.) for 20-30% speedup\n# TF32 provides faster matrix multiplication with minimal accuracy loss\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    # Prefer explicit device/dtype settings rather than deprecated tensor-type override\n    torch.set_default_dtype(torch.float32)\n    if hasattr(torch, \"set_default_device\"):\n        torch.set_default_device(\"cuda\")\n    print(\"PyTorch performance optimizations enabled (cuDNN benchmark, TF32)\")\n\nfrom core.data_generator import input_taker, req_consts_calc\nfrom training.trainer import train\nfrom core.initial_conditions import initialize_shared_velocity_fields\nfrom config import BATCH_SIZE, NUM_BATCHES, N_0, N_r, DIMENSION\nfrom config import a, wave, cs, xmin, ymin, zmin, tmin, tmax as TMAX_CFG, iteration_adam_2D, iteration_lbgfs_2D, harmonics, PERTURBATION_TYPE, rho_o, SLICE_Y\nfrom config import num_neurons, num_layers, num_of_waves\nfrom config import RANDOM_SEED, STARTUP_DT\nfrom config import ENABLE_TRAINING_DIAGNOSTICS\nfrom core.losses import ASTPN\nfrom core.model_architecture import PINN\nfrom visualization.Plotting import create_2d_animation\nfrom visualization.Plotting import create_1d_cross_sections_sinusoidal\nfrom visualization.Plotting import create_density_growth_plot\nfrom config import PLOT_DENSITY_GROWTH, GROWTH_PLOT_TMAX, GROWTH_PLOT_DT\nfrom config import FD_N_2D\nfrom config import ENABLE_INTERACTIVE_3D, INTERACTIVE_3D_RESOLUTION, INTERACTIVE_3D_TIME_STEPS\n\n\ndef _save_trained_models(net):\n    \"\"\"Persist trained model immediately after training completes.\"\"\"\n    try:\n        from config import SNAPSHOT_DIR\n        model_dir = os.path.join(SNAPSHOT_DIR, \"GRINN\")\n        os.makedirs(model_dir, exist_ok=True)\n        model_path = os.path.join(model_dir, \"model.pth\")\n        torch.save(net.state_dict(), model_path)\n        print(f\"Saved model to {model_path}\")\n    except Exception as e:\n        print(f\"Warning: failed to save model: {e}\")\n\n\ndef _comprehensive_gpu_cleanup(nets, optimizers=None, collocation_data=None, \n                                cached_ic_values=None, keep_models_for_vis=True, \n                                target_device=None):\n    \"\"\"\n    Comprehensive GPU memory cleanup after PINN training.\n    \n    This function properly frees GPU memory by:\n    1. Moving models to CPU (if keep_models_for_vis=True) or deleting them\n    2. Deleting optimizer state (Adam momentum buffers, LBFGS history)\n    3. Deleting collocation points and related tensors\n    4. Deleting cached IC values\n    5. Clearing PyTorch's cache allocator\n    \n    Args:\n        nets: List of neural networks (or single network in a list)\n        optimizers: List of optimizers to clean up (optional)\n        collocation_data: Dict or list of collocation tensors to delete (optional)\n        cached_ic_values: List of cached IC value dicts to delete (optional)\n        keep_models_for_vis: If True, move models to CPU instead of deleting (default: True)\n        target_device: Target device for models if keep_models_for_vis=True (default: 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if not torch.cuda.is_available():\n        return\n    \n    if target_device is None:\n        target_device = 'cpu'\n    \n    print(\"Starting comprehensive GPU memory cleanup...\")\n    \n    # 1. Set models to eval mode (reduces memory usage) and optionally move to CPU\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                # Set to eval mode to disable gradient tracking and reduce memory\n                net.eval()\n                # Clear any cached gradients\n                for param in net.parameters():\n                    if param.grad is not None:\n                        param.grad = None\n                \n                if keep_models_for_vis:\n                    # Move to target device (usually CPU) for visualization\n                    # Models can be moved back to GPU later if needed\n                    net = net.to(target_device)\n                    nets[i] = net\n                    print(f\"  Moved network {i} to {target_device} (eval mode)\")\n                else:\n                    # Delete model entirely\n                    del net\n                    nets[i] = None\n                    print(f\"  Deleted network {i}\")\n    \n    # 2. Delete optimizer state (can be large, especially LBFGS history)\n    if optimizers is not None:\n        if isinstance(optimizers, (list, tuple)):\n            for i, opt in enumerate(optimizers):\n                if opt is not None:\n                    # Clear optimizer state (this frees momentum buffers, LBFGS history, etc.)\n                    opt.state.clear()\n                    opt.param_groups.clear()\n                    del opt\n                    print(f\"  Deleted optimizer {i} state\")\n        else:\n            if optimizers is not None:\n                optimizers.state.clear()\n                optimizers.param_groups.clear()\n                del optimizers\n                print(\"  Deleted optimizer state\")\n    \n    # 3. Delete collocation points (these can be very large)\n    if collocation_data is not None:\n        if isinstance(collocation_data, dict):\n            for key, value in collocation_data.items():\n                if value is not None:\n                    if isinstance(value, (list, tuple)):\n                        for item in value:\n                            if isinstance(item, torch.Tensor) and item.is_cuda:\n                                del item\n                    elif isinstance(value, torch.Tensor) and value.is_cuda:\n                        del value\n            collocation_data.clear()\n            print(\"  Deleted collocation data dict\")\n        elif isinstance(collocation_data, (list, tuple)):\n            for item in collocation_data:\n                if item is not None:\n                    if isinstance(item, (list, tuple)):\n                        for subitem in item:\n                            if isinstance(subitem, torch.Tensor) and subitem.is_cuda:\n                                del subitem\n                    elif isinstance(item, torch.Tensor) and item.is_cuda:\n                        del item\n            print(\"  Deleted collocation data list\")\n        elif isinstance(collocation_data, torch.Tensor) and collocation_data.is_cuda:\n            del collocation_data\n            print(\"  Deleted collocation tensor\")\n    \n    # 4. Delete cached IC values\n    if cached_ic_values is not None:\n        if isinstance(cached_ic_values, list):\n            for i, cached_dict in enumerate(cached_ic_values):\n                if cached_dict is not None and isinstance(cached_dict, dict):\n                    for key, value in cached_dict.items():\n                        if isinstance(value, torch.Tensor) and value.is_cuda:\n                            del value\n                    cached_dict.clear()\n            cached_ic_values.clear()\n            print(\"  Deleted cached IC values\")\n    \n    # 5. Force garbage collection\n    import gc\n    gc.collect()\n    \n    # 6. Clear PyTorch's cache allocator and synchronize\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    print(\"GPU memory cleanup completed\")\n    \n    # Note: If visualization needs models on GPU, they can be moved back with:\n    # for net in nets:\n    #     if net is not None:\n    #         net = net.to(device)\n\n\ndef _move_models_to_device(nets, target_device):\n    \"\"\"\n    Move models to target device (e.g., back to GPU for visualization).\n    \n    Args:\n        nets: List of neural networks\n        target_device: Target device string (e.g., 'cuda:0' or 'cpu')\n    \n    Returns:\n        None (modifies nets in-place)\n    \"\"\"\n    if nets is not None:\n        for i, net in enumerate(nets):\n            if net is not None:\n                net = net.to(target_device)\n                nets[i] = net\n\n\ndef clean_pycache(root_dir: str) -&gt; Tuple[int, int]:\n    \"\"\"Remove all __pycache__ directories and .pyc files under &#96;&#96;root_dir&#96;&#96;.\"\"\"\n    removed_dirs = 0\n    removed_files = 0\n\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        if \"__pycache__\" in dirnames:\n            pycache_path = os.path.join(dirpath, \"__pycache__\")\n            try:\n                shutil.rmtree(pycache_path)\n                print(f\"Removed: {pycache_path}\")\n                removed_dirs += 1\n            except Exception as exc:  # pragma: no cover - cleanup best effort\n                print(f\"Error removing {pycache_path}: {exc}\")\n\n        for filename in filenames:\n            if filename.endswith(\".pyc\"):\n                pyc_path = os.path.join(dirpath, filename)\n                try:\n                    os.remove(pyc_path)\n                    print(f\"Removed: {pyc_path}\")\n                    removed_files += 1\n                except Exception as exc:  # pragma: no cover - cleanup best effort\n                    print(f\"Error removing {pyc_path}: {exc}\")\n\n    print(\"=\" * 60)\n    print(\"Cleanup complete!\")\n    print(f\"  Removed {removed_dirs} __pycache__ directories\")\n    print(f\"  Removed {removed_files} .pyc files\")\n    print(\"=\" * 60)\n\n    return removed_dirs, removed_files\n\n\nif \"--clean-pycache\" in sys.argv or \"--clean-cache\" in sys.argv:\n    flag = \"--clean-pycache\" if \"--clean-pycache\" in sys.argv else \"--clean-cache\"\n    script_root = os.path.dirname(os.path.abspath(__file__))\n    print(f\"Cleaning Python cache files from: {script_root}\")\n    clean_pycache(script_root)\n    # Prevent the rest of the training script from running in cleanup-only mode\n    sys.exit(0)\n\nhas_gpu = torch.cuda.is_available()\nhas_mps = torch.backends.mps.is_built()\ndevice = \"mps\" if torch.backends.mps.is_built() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Clear GPU memory if using CUDA\nif device.startswith('cuda'):\n    torch.cuda.empty_cache()\n\n\nlam, rho_1, num_of_waves, tmax, _, _, _ = input_taker(wave, a, num_of_waves, TMAX_CFG, N_0, 0, N_r)\n\njeans, alpha = req_consts_calc(lam, rho_1)\n# Set initial velocity amplitude per perturbation type\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    k = 2*np.pi/lam\n    v_1 = (rho_1 / (rho_o if rho_o != 0 else 1.0)) * (alpha / k)\nelse:\n    v_1 = a * cs\n\nxmax = xmin + lam * num_of_waves\nymax = ymin + lam * num_of_waves\nzmax = zmin + lam * num_of_waves\n#zmax = 1.0\n\n# Initialize shared velocity fields for consistent PINN/FD initial conditions\nvx_np, vy_np, vz_np = None, None, None  # Default values for sinusoidal case\nif str(PERTURBATION_TYPE).lower() == \"power_spectrum\":\n    if DIMENSION == 3:\n        vx_np, vy_np, vz_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=3)\n    else:\n        vx_np, vy_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=2)\n    \n    # Set shared velocity fields for plotting (only for 2D visualization)\n    if DIMENSION == 2:\n        from visualization.Plotting import set_shared_velocity_fields\n        set_shared_velocity_fields(vx_np, vy_np)\n\n# ==================== STANDARD PINN MODE ====================\nprint(\"Running in standard PINN mode (single network)...\")\n\nnet = PINN(n_harmonics=harmonics)\nnet = net.to(device)\nmse_cost_function = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\noptimizerL = torch.optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')\n\nif DIMENSION == 1:\n    astpn_rmin = [xmin, tmin]\n    astpn_rmax = [xmax, tmax]\nelif DIMENSION == 2:\n    astpn_rmin = [xmin, ymin, tmin]\n    astpn_rmax = [xmax, ymax, tmax]\nelif DIMENSION == 3:\n    astpn_rmin = [xmin, ymin, zmin, tmin]\n    astpn_rmax = [xmax, ymax, zmax, tmax]\nelse:\n    raise ValueError(f\"Unsupported DIMENSION={DIMENSION}\")\ncollocation_model = ASTPN(rmin=astpn_rmin, rmax=astpn_rmax, N_0=N_0, N_b=0, N_r=N_r, dimension=DIMENSION)\n\n# Set domain on the network so periodic embeddings enforce hard BCs\nspatial_rmin = [xmin]\nspatial_rmax = [xmax]\nif DIMENSION &gt;= 2:\n    spatial_rmin.append(ymin)\n    spatial_rmax.append(ymax)\nif DIMENSION &gt;= 3:\n    spatial_rmin.append(zmin)\n    spatial_rmax.append(zmax)\nnet.set_domain(rmin=spatial_rmin, rmax=spatial_rmax, dimension=DIMENSION)\n\n# IC collocation stays at t=0 throughout\ncollocation_IC = collocation_model.geo_time_coord(option=\"IC\")\n\n\n# Generate extra collocation points at t=0 for Poisson enforcement (Option 3)\nfrom core.data_generator import generate_poisson_ic_points\nfrom config import N_POISSON_IC\ncollocation_poisson_ic = generate_poisson_ic_points(\n    rmin=collocation_model.rmin,\n    rmax=collocation_model.rmax,\n    n_points=N_POISSON_IC,\n    dimension=DIMENSION,\n    device=device\n)\nprint(f\"Generated {N_POISSON_IC} extra collocation points at t=0 for Poisson enforcement\")\n\n\nstart_time = time.time()\n\n# Standard training\ncollocation_domain = collocation_model.geo_time_coord(option=\"Domain\")\n\nprint(\"Using standard training...\")\ntraining_diagnostics = train(\n    net=net,\n    model=collocation_model,\n    collocation_domain=collocation_domain,\n    collocation_IC=collocation_IC,\n    collocation_poisson_ic=collocation_poisson_ic,\n    optimizer=optimizer,\n    optimizerL=optimizerL,\n    closure=None,\n    mse_cost_function=mse_cost_function,\n    iteration_adam=iteration_adam_2D,\n    iterationL=iteration_lbgfs_2D,\n    rho_1=rho_1,\n    lam=lam,\n    jeans=jeans,\n    v_1=v_1,\n    device=device\n)\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"[Timing] PINN training completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n\n# Save model immediately after training completes\n_save_trained_models(net)\n\n# Comprehensive GPU memory cleanup after training\n# This properly frees model parameters, optimizer state, collocation points, etc.\nif device.startswith('cuda'):\n    # Collect all training-related data for cleanup\n    cleanup_optimizers = [optimizer, optimizerL] if 'optimizer' in locals() and 'optimizerL' in locals() else None\n    \n    # Try to get collocation data references\n    try:\n        cleanup_collocation = {\n            'domain': collocation_domain if 'collocation_domain' in locals() else None,\n            'IC': collocation_IC if 'collocation_IC' in locals() else None,\n            'poisson_IC': collocation_poisson_ic if 'collocation_poisson_ic' in locals() else None\n        }\n    except:\n        cleanup_collocation = None\n    \n    # Perform comprehensive cleanup\n    # This frees GPU memory by deleting optimizer state and collocation data\n    # Models are temporarily moved to CPU during cleanup, then moved back to GPU for visualization\n    _comprehensive_gpu_cleanup(\n        nets=[net],\n        optimizers=cleanup_optimizers,\n        collocation_data=cleanup_collocation,\n        cached_ic_values=None,\n        keep_models_for_vis=True,  # Keep models for visualization\n        target_device='cpu'  # Temporarily move models to CPU during cleanup\n    )\n    \n    # Move model back to GPU for visualization (visualization code expects model on GPU)\n    # The FD solver will clear cache before it runs, so this is safe\n    if device.startswith('cuda'):\n        _move_models_to_device([net], device)\n        print(\"Model moved back to GPU for visualization\")\n\n# ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================\n# Run comprehensive diagnostics for high-tmax failure analysis\n# This generates 4 additional critical plots beyond the training diagnostics\nif ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:\n    print(\"\\n\" + \"=\"*70)\n    print(\"Running post-training comprehensive diagnostics...\")\n    print(\"=\"*70)\n    \n    try:\n        net.eval()\n        # Run comprehensive diagnostics - generates 4 additional plots:\n        # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks\n        # 2. Conservation Laws - Mass/momentum conservation\n        # 3. Spectral Evolution - Frequency content analysis\n        # 4. Temporal Statistics - Error accumulation tracking\n        training_diagnostics.run_comprehensive_diagnostics(\n            model=net,\n            dimension=DIMENSION,\n            tmax=tmax\n        )\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"All diagnostic plots generated successfully!\")\n        print(\"Check ./diagnostics/ folder for:\")\n        print(\"  1. training_diagnostics.png - Training convergence\")\n        print(\"  2. residual_heatmaps.png - Spatiotemporal PDE violations\")\n        print(\"  3. conservation_laws.png - Physical consistency\")\n        print(\"  4. spectral_evolution.png - Frequency content\")\n        print(\"  5. temporal_statistics.png - Field evolution\")\n        print(\"=\"*70 + \"\\n\")\n    except Exception as e:\n        print(f\"[WARNING] Post-training diagnostics failed: {e}\")\n        print(\"Training completed successfully, but diagnostic plots may be incomplete.\")\n        import traceback\n        traceback.print_exc()\n\n# ==================== VISUALIZATION ====================\ninitial_params = (xmin, xmax, ymin, ymax, rho_1, alpha, lam, \"temp\", tmax)\n\nanim_density = create_2d_animation(net, initial_params, which=\"density\", fps=10, verbose=False)\nanim_velocity = create_2d_animation(net, initial_params, which=\"velocity\", fps=10, verbose=False)\n\nif str(PERTURBATION_TYPE).lower() == \"sinusoidal\":\n    create_1d_cross_sections_sinusoidal(net, initial_params, time_points=None, y_fixed=SLICE_Y, N_fd=600, nu_fd=0.5)\n\nif PLOT_DENSITY_GROWTH:\n    try:\n        tmax_growth = float(GROWTH_PLOT_TMAX)\n    except Exception:\n        tmax_growth = float(TMAX_CFG)\n    dt_growth = float(GROWTH_PLOT_DT)\n    create_density_growth_plot(net, initial_params, tmax=tmax_growth, dt=dt_growth)\n\n# ==================== 3D INTERACTIVE VISUALIZATION ====================\nif DIMENSION == 3 and ENABLE_INTERACTIVE_3D:\n    try:\n        from visualization.Interactive import create_interactive_3d_plot\n        from config import SNAPSHOT_DIR\n\n        print(\"\\n\" + \"=\"*70)\n        print(\"Generating 3D interactive plot...\")\n        print(\"=\"*70)\n\n        save_path = os.path.join(SNAPSHOT_DIR, \"interactive_3d_plot.html\")\n        create_interactive_3d_plot(\n            net=net,\n            initial_params=initial_params,\n            time_range=(tmin, tmax),\n            time_steps=INTERACTIVE_3D_TIME_STEPS,\n            resolution=INTERACTIVE_3D_RESOLUTION,\n            save_path=save_path,\n        )\n\n        print(f\"Interactive 3D plot saved to: {save_path}\")\n        print(\"=\"*70 + \"\\n\")\n    except Exception as e:\n        print(f\"[WARNING] 3D interactive plot generation failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# ==================== FINAL CACHE CLEANUP ====================\nscript_root = os.path.dirname(os.path.abspath(__file__))\nprint(\"Performing final Python cache cleanup...\")\nclean_pycache(script_root)\n\n"
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Removing multistage functions**

I'm removing the multistage training function and the related _compute_poisson_ic_penalties function from trainer.py, including their imports and the STAGE3 import line, applying a large cleanup patch.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="apply_patch">
<details>
<summary>Tool use: **apply_patch**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\trainer.py"
}
```
Additional data:

```json
{
  "codeblockId": "b01e1f08-cda3-457a-a758-25482a87a0ae",
  "reviewData": {
    "status": "None",
    "selectedOption": "accept",
    "isShowingInput": false,
    "firstTimeReviewMode": false
  },
  "codeblock": {
    "uri": {
      "scheme": "file",
      "authority": "",
      "path": "/c:/Users/tirth/Documents/Projects/GRINN/Power spectrum perturbations/training/trainer.py",
      "query": "",
      "fragment": "",
      "_formatted": null,
      "_fsPath": null
    },
    "codeblockId": "b01e1f08-cda3-457a-a758-25482a87a0ae",
    "codeBlockIdx": 0,
    "content": "\"\"\"\nTraining loop management for PINN and XPINN.\n\nThis module handles the optimization loops (Adam and L-BFGS) for both\nsingle PINN and XPINN decomposition training.\n\"\"\"\nimport numpy as np\nimport torch\nfrom training.training_diagnostics import TrainingDiagnostics\nfrom config import BATCH_SIZE, NUM_BATCHES, ENABLE_TRAINING_DIAGNOSTICS\nfrom training.physics import closure_batched, closure_batched_cached, _random_batch_indices, _make_batch_tensors\n\n\ndef _compute_ic_component_losses(net, collocation_IC, rho_1, lam, jeans, v_1, dimension, mse_cost_function):\n    \"\"\"\n    Compute individual IC component losses for 3D power spectrum diagnostics.\n    \n    Returns:\n        Dict with component-wise losses: {'rho', 'vx', 'vy', 'vz', 'phi'}\n    \"\"\"\n    from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n    from config import rho_o\n    \n    with torch.no_grad():\n        # Get true ICs\n        rho_0 = fun_rho_0(rho_1, lam, collocation_IC)\n        vx_0 = fun_vx_0(lam, jeans, v_1, collocation_IC)\n        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_IC)\n        vz_0 = fun_vz_0(lam, jeans, v_1, collocation_IC)\n        \n        # Get predictions\n        net_ic_out = net(collocation_IC)\n        rho_ic_out = net_ic_out[:, 0:1]\n        vx_ic_out = net_ic_out[:, 1:2]\n        vy_ic_out = net_ic_out[:, 2:3]\n        vz_ic_out = net_ic_out[:, 3:4]\n        phi_ic_out = net_ic_out[:, 4:5]\n        \n        # Compute component losses\n        ic_losses = {\n            'rho': mse_cost_function(rho_ic_out, rho_0).item(),\n            'vx': mse_cost_function(vx_ic_out, vx_0).item(),\n            'vy': mse_cost_function(vy_ic_out, vy_0).item(),\n            'vz': mse_cost_function(vz_ic_out, vz_0).item(),\n            'phi': torch.mean(phi_ic_out ** 2).item(),  # phi should be close to 0 ideally\n        }\n    \n    return ic_losses\n\n\ndef train(model, net, collocation_domain, collocation_IC, optimizer, optimizerL, iteration_adam, iterationL, mse_cost_function, closure, rho_1, lam, jeans, v_1, device, data_terms=None, collocation_poisson_ic=None):\n    \"\"\"\n    Standard training loop for single PINN.\n    \n    Manages Adam and L-BFGS optimization phases with cosine scheduling for\n    continuity weight and startup_dt parameters.\n    \n    Args:\n        model: Collocation model\n        net: Neural network\n        collocation_domain: Domain collocation points\n        collocation_IC: Initial condition points\n        optimizer: Adam optimizer\n        optimizerL: L-BFGS optimizer\n        iteration_adam: Number of Adam iterations\n        iterationL: Number of L-BFGS iterations\n        mse_cost_function: MSE loss function\n        closure: Closure function (unused, kept for compatibility)\n        rho_1: Perturbation amplitude\n        lam: Wavelength\n        jeans: Jeans length\n        v_1: Velocity amplitude\n        device: PyTorch device\n        data_terms: Optional list of additional supervised datasets with weights\n        collocation_poisson_ic: Extra collocation points at t=0 for Poisson enforcement\n    \"\"\"\n    bs = int(BATCH_SIZE)\n    nb = int(NUM_BATCHES)\n\n    # Import config values for diagnostics\n    from config import PERTURBATION_TYPE, tmax\n    \n    # Create diagnostics with correct dimension and perturbation type\n    diagnostics = TrainingDiagnostics(\n        save_dir='./diagnostics/',\n        dimension=model.dimension,\n        perturbation_type=PERTURBATION_TYPE\n    ) if ENABLE_TRAINING_DIAGNOSTICS else None\n\n    for i in range(iteration_adam):\n        loss, loss_breakdown = optimizer.step(lambda: closure_batched(model, net, mse_cost_function, collocation_domain, collocation_IC, optimizer, rho_1, lam, jeans, v_1, bs, nb, update_tracker=True, iteration=i, use_fft_poisson=True, data_terms=data_terms, collocation_poisson_ic=collocation_poisson_ic))\n\n        with torch.autograd.no_grad():\n            # Diagnostics logging every 50 iterations\n            if i % 50 == 0 and diagnostics is not None:\n                try:\n                    # Compute IC component losses for 3D power spectrum tracking\n                    ic_component_losses = None\n                    if diagnostics.is_3d_power_spectrum:\n                        ic_component_losses = _compute_ic_component_losses(\n                            net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function\n                        )\n                    \n                    diagnostics.log_iteration(\n                        iteration=i,\n                        model=net,\n                        loss_dict={\n                            'total': loss.item(),\n                            'pde': float(loss_breakdown.get('PDE', 0.0)),\n                            'ic': float(loss_breakdown.get('IC', 0.0))\n                        },\n                        geomtime_col=collocation_domain,\n                        ic_component_losses=ic_component_losses\n                    )\n                except Exception as _diag_err:\n                    # Keep training robust if diagnostics fail\n                    print(f\"[WARN] Diagnostics logging failed at {i}: {_diag_err}\")\n\n            if i % 100 == 0:\n                print(f\"Training Loss at {i} for Adam (batched) in {model.dimension}D system = {loss.item():.2e}\", flush=True)\n                # Print loss breakdown\n                breakdown_str = \" | \".join([f\"{k}: {v:.2e}\" for k, v in loss_breakdown.items() if v &gt; 0])\n                if breakdown_str:\n                    print(f\"  Loss breakdown: {breakdown_str}\", flush=True)\n\n    for i in range(iterationL):\n        global_step = iteration_adam + i\n\n        # L-BFGS expects a closure that returns only scalar loss\n        # Store loss_breakdown in a list so we can access it after the step\n        loss_breakdown_holder = [None]\n        \n        # Cache batch indices for L-BFGS step (L-BFGS calls closure multiple times)\n        # Using same indices ensures consistent loss landscape during line search\n        if isinstance(collocation_domain, (list, tuple)):\n            dom_n = collocation_domain[0].size(0)\n            device = collocation_domain[0].device\n        else:\n            dom_n = collocation_domain.size(0)\n            device = collocation_domain.device\n        ic_n = collocation_IC[0].size(0)\n        \n        cached_dom_indices = [_random_batch_indices(dom_n, bs, device) for _ in range(int(max(1, nb)))]\n        cached_ic_indices = [_random_batch_indices(ic_n, bs, device) for _ in range(int(max(1, nb)))]\n        \n        def lbfgs_closure():\n            optimizerL.zero_grad()\n            loss, loss_breakdown = closure_batched_cached(\n                model, net, mse_cost_function, collocation_domain, collocation_IC,\n                optimizerL, rho_1, lam, jeans, v_1, bs, nb,\n                cached_dom_indices, cached_ic_indices,\n                data_terms=data_terms, collocation_poisson_ic=collocation_poisson_ic\n            )\n            loss_breakdown_holder[0] = loss_breakdown\n            return loss\n        \n        loss = optimizerL.step(lbfgs_closure)\n        loss_breakdown = loss_breakdown_holder[0]\n\n        with torch.autograd.no_grad():\n            # Diagnostics logging every 50 iterations in LBFGS too\n            if i % 50 == 0 and loss_breakdown is not None and diagnostics is not None:\n                try:\n                    # Compute IC component losses for 3D power spectrum tracking\n                    ic_component_losses = None\n                    if diagnostics.is_3d_power_spectrum:\n                        ic_component_losses = _compute_ic_component_losses(\n                            net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function\n                        )\n                    \n                    diagnostics.log_iteration(\n                        iteration=iteration_adam + i,\n                        model=net,\n                        loss_dict={\n                            'total': loss.item() if hasattr(loss, 'item') else float(loss),\n                            'pde': float(loss_breakdown.get('PDE', 0.0)),\n                            'ic': float(loss_breakdown.get('IC', 0.0))\n                        },\n                        geomtime_col=collocation_domain,\n                        ic_component_losses=ic_component_losses\n                    )\n                except Exception as _diag_err:\n                    print(f\"[WARN] Diagnostics logging (LBFGS) failed at {i}: {_diag_err}\")\n            if i % 50 == 0:\n                print(f\"Training Loss at {i} for LBGFS (batched) in {model.dimension}D system = {loss.item():.2e}\", flush=True)\n                # Print loss breakdown\n                breakdown_str = \" | \".join([f\"{k}: {v:.2e}\" for k, v in loss_breakdown.items() if v &gt; 0])\n                if breakdown_str:\n                    print(f\"  Loss breakdown: {breakdown_str}\", flush=True)\n    \n    # Generate diagnostic plots at the end of training\n    if diagnostics is not None:\n        try:\n            final_iteration = iteration_adam + iterationL - 1\n            \n            # Generate unified diagnostics (5 plots for all cases)\n            print(\"\\n[INFO] Generating unified training diagnostics...\")\n            from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n            \n            # Evaluate true ICs on the IC collocation points for comparison\n            with torch.no_grad():\n                true_ic_data = {\n                    'colloc_IC': collocation_IC,\n                    'rho': fun_rho_0(rho_1, lam, collocation_IC),\n                    'vx': fun_vx_0(lam, jeans, v_1, collocation_IC),\n                    'vy': fun_vy_0(lam, jeans, v_1, collocation_IC),\n                    'vz': fun_vz_0(lam, jeans, v_1, collocation_IC) if model.dimension == 3 else None,\n                }\n            \n            diagnostics.run_unified_diagnostics(\n                model=net, \n                true_ic_data=true_ic_data,\n                final_iteration=final_iteration\n            )\n            \n            print(f\"\\n[INFO] All diagnostics saved to ./diagnostics/\")\n        except Exception as _diag_err:\n            import traceback\n            print(f\"[WARN] Final diagnostics plotting failed: {_diag_err}\")\n            traceback.print_exc()\n    \n    # Return diagnostics object for potential reuse in post-training analysis\n    return diagnostics\n\n\ndef _select_batch(collocation, indices):\n    if isinstance(collocation, (list, tuple)):\n        return _make_batch_tensors(collocation, indices)\n    # Single tensor format: split into [x, y, t]\n    return [collocation[indices, i:i+1] for i in range(collocation.shape[1])]\n\n\ndef _phi_slice_index(dimension):\n    if dimension == 1:\n        return 2\n    if dimension == 2:\n        return 3\n    return 4\n\n\ndef _compute_poisson_loss_from_ic(net, collocation_points, rho_1, lam, dimension):\n    \"\"\"\n    Compute Poisson loss using IC density (Stage 1).\n    \n    The Poisson equation is: ∇²φ = const * (ρ - ρ₀)\n    We use the true IC density ρ_ic from fun_rho_0 and the network's predicted φ.\n    \"\"\"\n    from core.data_generator import diff\n    from core.initial_conditions import fun_rho_0\n    from config import const, rho_o\n    \n    # Ensure coords is a list and enable gradient tracking\n    if isinstance(collocation_points, (list, tuple)):\n        coords = []\n        for coord in collocation_points:\n            if not coord.requires_grad:\n                coord = coord.requires_grad_(True)\n            coords.append(coord)\n    else:\n        # Single tensor format: split into [x, y, (z), t] and enable gradients\n        coords = []\n        for i in range(collocation_points.shape[1]):\n            coord = collocation_points[:, i:i+1]\n            if not coord.requires_grad:\n                coord = coord.requires_grad_(True)\n            coords.append(coord)\n    \n    # Forward pass (network expects list format)\n    outputs = net(coords)\n    phi = outputs[:, _phi_slice_index(dimension):_phi_slice_index(dimension) + 1]\n\n    if not torch.isfinite(phi).all():\n        safe = torch.tensor(0.0, device=phi.device, requires_grad=True)\n        return safe\n    \n    # Extract spatial coordinates\n    x = coords[0]\n    \n    # Compute first derivative to detect blow-ups early\n    phi_x = diff(phi, x, order=1)\n    if torch.isinf(phi_x).any():\n        safe = torch.tensor(1e10, device=phi.device, requires_grad=True)\n        return safe\n\n    # Check for NaN in phi before computing laplacian\n    if torch.isnan(phi).any():\n        print(f\"[DEBUG] NaN detected in phi output!\")\n        print(f\"  phi shape: {phi.shape}\")\n        print(f\"  phi has NaN: {torch.isnan(phi).sum().item()} / {phi.numel()}\")\n        try:\n            print(f\"  phi range: [{phi.min().item():.2e}, {phi.max().item():.2e}]\")\n        except:\n            print(f\"  phi min/max computation failed\")\n    \n    # Compute Laplacian and check for NaN\n    if dimension == 1:\n        laplacian = diff(phi, x, order=2)\n    elif dimension == 2:\n        y = coords[1]\n        laplacian = diff(phi, x, order=2) + diff(phi, y, order=2)\n    else:  # dimension == 3\n        y = coords[1]\n        z = coords[2]\n        laplacian = diff(phi, x, order=2) + diff(phi, y, order=2) + diff(phi, z, order=2)\n    \n    if torch.isnan(laplacian).any():\n        pass\n\n    if torch.isinf(laplacian).any():\n        laplacian = torch.clamp(laplacian, -1e10, 1e10)\n    \n    # Get true IC density\n    rho_ic = fun_rho_0(rho_1, lam, coords)\n    \n    if torch.isnan(rho_ic).any():\n        pass\n    \n    # Poisson residual: ∇²φ - const*(ρ - ρ₀)\n    residual = laplacian - const * (rho_ic - rho_o)\n\n    if torch.isinf(residual).any():\n        # Clamp to avoid NaN in subsequent operations\n        residual = torch.clamp(residual, -1e10, 1e10)\n    \n    # Check for NaN and print debug info\n    if torch.isnan(residual).any():\n        print(f\"[DEBUG] NaN detected in Poisson residual!\")\n        print(f\"  residual has NaN: {torch.isnan(residual).sum().item()} / {residual.numel()}\")\n        try:\n            print(f\"  laplacian range: [{laplacian.min().item():.2e}, {laplacian.max().item():.2e}]\")\n            print(f\"  rho_ic range: [{rho_ic.min().item():.2e}, {rho_ic.max().item():.2e}]\")\n            print(f\"  phi range: [{phi.min().item():.2e}, {phi.max().item():.2e}]\")\n        except Exception as e:\n            print(f\"  Could not compute ranges: {e}\")\n        print(f\"  const: {const}, rho_o: {rho_o}\")\n        import sys\n        sys.stdout.flush()  # Force flush to ensure output appears\n    \n    # Normalized loss\n    scale = abs(const) * (abs(rho_o) if rho_o != 0 else 1.0)\n    scale = max(scale, 1e-10)  # Avoid division by zero\n    \n    loss = torch.mean((residual / scale) ** 2)\n    \n    if torch.isnan(loss):\n        pass\n    if torch.isinf(loss):\n        safe = torch.tensor(1e8, device=residual.device, requires_grad=True)\n        return safe\n    \n    return loss\n\n\ndef _compute_ic_loss(net, collocation_ic, rho_1, lam, jeans, v_1, dimension, mse_cost_function):\n    from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n    from config import rho_o, PERTURBATION_TYPE, KX, KY\n\n    rho_0 = fun_rho_0(rho_1, lam, collocation_ic)\n    vx_0 = fun_vx_0(lam, jeans, v_1, collocation_ic)\n\n    net_ic_out = net(collocation_ic)\n    rho_ic_out = net_ic_out[:, 0:1]\n    vx_ic_out = net_ic_out[:, 1:2]\n\n    if dimension == 2:\n        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_ic)\n        vy_ic_out = net_ic_out[:, 2:3]\n    elif dimension == 3:\n        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_ic)\n        vz_0 = fun_vz_0(lam, jeans, v_1, collocation_ic)\n        vy_ic_out = net_ic_out[:, 2:3]\n        vz_ic_out = net_ic_out[:, 3:4]\n\n    is_sin = str(PERTURBATION_TYPE).lower() == \"sinusoidal\"\n    if is_sin:\n        x_ic_for_ic = collocation_ic[0]\n        if len(collocation_ic) &gt;= 2:\n            y_ic_for_ic = collocation_ic[1]\n            rho_ic_target = rho_o + rho_1 * torch.cos(KX * x_ic_for_ic + KY * y_ic_for_ic)\n        else:\n            rho_ic_target = rho_o + rho_1 * torch.cos(2 * np.pi * x_ic_for_ic / lam)\n        mse_rho_ic = mse_cost_function(rho_ic_out, rho_ic_target)\n    else:\n        mse_rho_ic = 0.0 * torch.mean(rho_ic_out * 0)\n\n    mse_vx_ic = mse_cost_function(vx_ic_out, vx_0)\n    if dimension == 2:\n        mse_vy_ic = mse_cost_function(vy_ic_out, vy_0)\n    elif dimension == 3:\n        mse_vy_ic = mse_cost_function(vy_ic_out, vy_0)\n        mse_vz_ic = mse_cost_function(vz_ic_out, vz_0)\n\n    ic_loss = mse_vx_ic + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)\n    if dimension == 2:\n        ic_loss = ic_loss + mse_vy_ic\n    elif dimension == 3:\n        ic_loss = ic_loss + mse_vy_ic + mse_vz_ic\n\n    return ic_loss\n\n\ndef _compute_hydro_losses(model, net, batch_dom):\n    from core.losses import pde_residue\n    from config import cs, rho_o\n\n    if model.dimension == 1:\n        rho_r, vx_r, _phi_r = pde_residue(batch_dom, net, dimension=1)\n    elif model.dimension == 2:\n        rho_r, vx_r, vy_r, _phi_r = pde_residue(batch_dom, net, dimension=2)\n    else:\n        rho_r, vx_r, vy_r, vz_r, _phi_r = pde_residue(batch_dom, net, dimension=3)\n\n    continuity_scale = abs(rho_o * cs)\n    continuity_scale = continuity_scale if continuity_scale &gt; 0 else 1.0\n    momentum_scale = abs(cs ** 2) if cs != 0 else 1.0\n\n    loss_cont = torch.mean((rho_r / continuity_scale) ** 2)\n    loss_mom = torch.mean((vx_r / momentum_scale) ** 2)\n    if model.dimension == 2:\n        loss_mom = loss_mom + torch.mean((vy_r / momentum_scale) ** 2)\n    elif model.dimension == 3:\n        loss_mom = loss_mom + torch.mean((vy_r / momentum_scale) ** 2)\n        loss_mom = loss_mom + torch.mean((vz_r / momentum_scale) ** 2)\n\n    return loss_cont + loss_mom\n\n\ndef _compute_poisson_loss_from_pde(model, net, batch_dom):\n    from core.losses import pde_residue\n    from config import const, rho_o\n\n    if model.dimension == 1:\n        _rho_r, _vx_r, phi_r = pde_residue(batch_dom, net, dimension=1)\n    elif model.dimension == 2:\n        _rho_r, _vx_r, _vy_r, phi_r = pde_residue(batch_dom, net, dimension=2)\n    else:\n        _rho_r, _vx_r, _vy_r, _vz_r, phi_r = pde_residue(batch_dom, net, dimension=3)\n\n    scale = abs(const) * (abs(rho_o) if rho_o != 0 else 1.0)\n    scale = scale if scale &gt; 0 else 1.0\n    return torch.mean((phi_r / scale) ** 2)\n\n",
    "languageId": "python"
  }
}
```
Result:

```json
{
  "diff": {
    "chunks": [
      {
        "diffString": "  \n  \n- def _compute_poisson_ic_penalties(net, collocation_poisson_ic, dimension):\n-     from core.losses import poisson_residue_only\n-     from config import POISSON_IC_WEIGHT, PHI_MEAN_CONSTRAINT_WEIGHT\n- \n-     if collocation_poisson_ic is None:\n-         return None, None\n- \n-     loss_poisson = None\n-     loss_phi_mean = None\n- \n-     if POISSON_IC_WEIGHT &gt; 0:\n-         phi_r_ic = poisson_residue_only(collocation_poisson_ic, net, dimension=dimension)\n-         loss_poisson = POISSON_IC_WEIGHT * torch.mean(phi_r_ic ** 2)\n- \n-     if PHI_MEAN_CONSTRAINT_WEIGHT &gt; 0:\n-         net_output_ic = net(collocation_poisson_ic)\n-         phi_ic = net_output_ic[:, -1]\n-         mean_phi = torch.mean(phi_ic)\n-         loss_phi_mean = PHI_MEAN_CONSTRAINT_WEIGHT * (mean_phi ** 2)\n- \n-     return loss_poisson, loss_phi_mean\n- \n- \n- def train_multistage(model, net, collocation_domain, collocation_IC, optimizer, optimizerL,\n-                      mse_cost_function, rho_1, lam, jeans, v_1, device,\n-                      stage1_iters, stage2_iters, stage3_iters,\n-                      stage1_lbfgs_iters, stage2_lbfgs_iters, stage3_lbfgs_iters,\n-                      collocation_poisson_ic=None):\n-     \"\"\"\n-     Multi-stage training for a single PINN where losses are staged:\n-       Stage 1: Poisson-only using IC density (phi channel)\n-       Stage 2: Hydrodynamics (continuity + momentum) + IC loss\n-       Stage 3: Joint with weighted Poisson + hydro + IC loss\n-     \"\"\"\n-     bs = int(BATCH_SIZE)\n-     nb = int(NUM_BATCHES)\n- \n-     from config import PERTURBATION_TYPE, IC_WEIGHT, STAGE3_POISSON_WEIGHT\n- \n-     diagnostics = TrainingDiagnostics(\n-         save_dir='./diagnostics/',\n-         dimension=model.dimension,\n-         perturbation_type=PERTURBATION_TYPE\n-     ) if ENABLE_TRAINING_DIAGNOSTICS else None\n- \n-     # Stage 1 uses domain collocation (all t) for Poisson training\n-     poisson_points = collocation_domain\n- \n-     # Check if we have data for Stage 1\n-     n_poisson = poisson_points[0].size(0) if isinstance(poisson_points, (list, tuple)) else poisson_points.size(0)\n-     \n-     if n_poisson == 0:\n-         # Set stage 1 iterations to 0 to skip it\n-         stage1_iters = 0\n-         stage1_lbfgs_iters = 0\n- \n-     def _log_diagnostics(iteration, total_loss, pde_loss, ic_loss):\n-         if diagnostics is None:\n-             return\n-         try:\n-             ic_component_losses = None\n-             if diagnostics.is_3d_power_spectrum:\n-                 ic_component_losses = _compute_ic_component_losses(\n-                     net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function\n-                 )\n-             diagnostics.log_iteration(\n-                 iteration=iteration,\n-                 model=net,\n-                 loss_dict={\n-                     'total': float(total_loss),\n-                     'pde': float(pde_loss),\n-                     'ic': float(ic_loss)\n-                 },\n-                 geomtime_col=collocation_domain,\n-                 ic_component_losses=ic_component_losses\n-             )\n-         except Exception as _diag_err:\n-             print(f\"[WARN] Diagnostics logging failed at {iteration}: {_diag_err}\")\n- \n-     # ----------------------- Stage 1: Poisson only -----------------------\n-     print(\"=\" * 70)\n-     print(f\"STAGE 1: Poisson-only ({stage1_iters} Adam, {stage1_lbfgs_iters} L-BFGS)\")\n-     print(\"=\" * 70)\n- \n-     poisson_n = poisson_points[0].size(0)\n-     device = poisson_points[0].device\n-     num_effective_batches = int(max(1, nb))\n- \n-     for i in range(int(stage1_iters)):\n-         optimizer.zero_grad()\n-         total_loss_scalar = 0.0\n- \n-         # Proper gradient accumulation: backward per batch, accumulate gradients\n-         for batch_idx in range(num_effective_batches):\n-             idx = _random_batch_indices(poisson_n, bs, device)\n-             batch_poisson = _make_batch_tensors(poisson_points, idx)\n-             loss = _compute_poisson_loss_from_ic(net, batch_poisson, rho_1, lam, model.dimension)\n-             scaled_loss = loss / num_effective_batches\n-             scaled_loss.backward()\n-             total_loss_scalar += loss.item()\n- \n-         torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n-         optimizer.step()\n- \n-         avg_loss_scalar = total_loss_scalar / num_effective_batches\n-         if i % 50 == 0:\n-             print(f\"Stage 1 Adam Iter {i}: Poisson Loss = {avg_loss_scalar:.2e}\", flush=True)\n-             _log_diagnostics(i, avg_loss_scalar, avg_loss_scalar, 0.0)\n- \n-     for i in range(int(stage1_lbfgs_iters)):\n-         loss_holder = [None]\n- \n-         # Cache batch indices for L-BFGS (closure called multiple times)\n-         cached_poisson_indices = [_random_batch_indices(poisson_n, bs, device) for _ in range(num_effective_batches)]\n- \n-         def lbfgs_closure():\n-             optimizerL.zero_grad()\n-             total_loss_scalar = 0.0\n-             for batch_idx in range(num_effective_batches):\n-                 idx = cached_poisson_indices[batch_idx]\n-                 batch_poisson = _make_batch_tensors(poisson_points, idx)\n-                 loss = _compute_poisson_loss_from_ic(net, batch_poisson, rho_1, lam, model.dimension)\n-                 scaled_loss = loss / num_effective_batches\n-                 scaled_loss.backward()\n-                 total_loss_scalar += loss.item()\n-             torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n-             avg_loss = torch.tensor(total_loss_scalar / num_effective_batches, device=device)\n-             loss_holder[0] = avg_loss\n-             return avg_loss\n- \n-         loss = optimizerL.step(lbfgs_closure)\n-         if i % 50 == 0:\n-             value = loss_holder[0].item() if loss_holder[0] is not None else float(loss)\n-             print(f\"Stage 1 L-BFGS Iter {i}: Poisson Loss = {value:.2e}\", flush=True)\n-             _log_diagnostics(stage1_iters + i, value, value, 0.0)\n- \n-     # ----------------------- Stage 2: Hydro + IC -------------------------\n-     print(\"\\n\" + \"=\" * 70)\n-     print(f\"STAGE 2: Hydrodynamics ({stage2_iters} Adam, {stage2_lbfgs_iters} L-BFGS)\")\n-     print(\"=\" * 70)\n- \n-     if isinstance(collocation_domain, (list, tuple)):\n-         dom_n = collocation_domain[0].size(0)\n-         device = collocation_domain[0].device\n-     else:\n-         dom_n = collocation_domain.size(0)\n-         device = collocation_domain.device\n-     ic_n = collocation_IC[0].size(0)\n- \n-     for i in range(int(stage2_iters)):\n-         optimizer.zero_grad()\n-         total_loss_scalar = 0.0\n-         last_ic_scalar = 0.0\n- \n-         # Proper gradient accumulation\n-         for batch_idx in range(num_effective_batches):\n-             dom_idx = _random_batch_indices(dom_n, bs, device)\n-             ic_idx = _random_batch_indices(ic_n, bs, device)\n- \n-             batch_dom = _select_batch(collocation_domain, dom_idx)\n-             batch_ic = _make_batch_tensors(collocation_IC, ic_idx)\n- \n-             hydro_loss = _compute_hydro_losses(model, net, batch_dom)\n-             ic_loss = _compute_ic_loss(net, batch_ic, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function)\n-             loss = hydro_loss + IC_WEIGHT * ic_loss\n- \n-             scaled_loss = loss / num_effective_batches\n-             scaled_loss.backward()\n-             total_loss_scalar += loss.item()\n-             last_ic_scalar = ic_loss.item()\n- \n-         torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n-         optimizer.step()\n- \n-         avg_loss_scalar = total_loss_scalar / num_effective_batches\n-         if i % 50 == 0:\n-             print(f\"Stage 2 Adam Iter {i}: Hydro+IC Loss = {avg_loss_scalar:.2e}\", flush=True)\n-             _log_diagnostics(stage1_iters + stage1_lbfgs_iters + i, avg_loss_scalar, avg_loss_scalar, last_ic_scalar)\n- \n-     for i in range(int(stage2_lbfgs_iters)):\n-         loss_holder = [None]\n-         ic_holder = [0.0]\n- \n-         # Cache batch indices for L-BFGS\n-         cached_dom_indices = [_random_batch_indices(dom_n, bs, device) for _ in range(num_effective_batches)]\n-         cached_ic_indices = [_random_batch_indices(ic_n, bs, device) for _ in range(num_effective_batches)]\n- \n-         def lbfgs_closure():\n-             optimizerL.zero_grad()\n-             total_loss_scalar = 0.0\n-             last_ic = 0.0\n-             for batch_idx in range(num_effective_batches):\n-                 dom_idx = cached_dom_indices[batch_idx]\n-                 ic_idx = cached_ic_indices[batch_idx]\n- \n-                 batch_dom = _select_batch(collocation_domain, dom_idx)\n-                 batch_ic = _make_batch_tensors(collocation_IC, ic_idx)\n- \n-                 hydro_loss = _compute_hydro_losses(model, net, batch_dom)\n-                 ic_loss = _compute_ic_loss(net, batch_ic, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function)\n-                 loss = hydro_loss + IC_WEIGHT * ic_loss\n- \n-                 scaled_loss = loss / num_effective_batches\n-                 scaled_loss.backward()\n-                 total_loss_scalar += loss.item()\n-                 last_ic = ic_loss.item()\n- \n-             torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n-             avg_loss = torch.tensor(total_loss_scalar / num_effective_batches, device=device)\n-             loss_holder[0] = avg_loss\n-             ic_holder[0] = last_ic\n-             return avg_loss\n- \n-         loss = optimizerL.step(lbfgs_closure)\n-         if i % 50 == 0:\n-             value = loss_holder[0].item() if loss_holder[0] is not None else float(loss)\n-             print(f\"Stage 2 L-BFGS Iter {i}: Hydro+IC Loss = {value:.2e}\", flush=True)\n-             _log_diagnostics(stage1_iters + stage1_lbfgs_iters + stage2_iters + i, value, value, ic_holder[0])\n- \n-     # ----------------------- Stage 3: Joint -----------------------------\n-     print(\"\\n\" + \"=\" * 70)\n-     print(f\"STAGE 3: Joint ({stage3_iters} Adam, {stage3_lbfgs_iters} L-BFGS)\")\n-     print(\"=\" * 70)\n- \n-     # dom_n, ic_n, device already defined from Stage 2\n- \n-     for i in range(int(stage3_iters)):\n-         optimizer.zero_grad()\n-         total_loss_scalar = 0.0\n-         last_ic_scalar = 0.0\n-         last_pde_scalar = 0.0\n- \n-         # Proper gradient accumulation\n-         for batch_idx in range(num_effective_batches):\n-             dom_idx = _random_batch_indices(dom_n, bs, device)\n-             ic_idx = _random_batch_indices(ic_n, bs, device)\n- \n-             batch_dom = _select_batch(collocation_domain, dom_idx)\n-             batch_ic = _make_batch_tensors(collocation_IC, ic_idx)\n- \n-             hydro_loss = _compute_hydro_losses(model, net, batch_dom)\n-             poisson_loss = _compute_poisson_loss_from_pde(model, net, batch_dom)\n-             ic_loss = _compute_ic_loss(net, batch_ic, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function)\n- \n-             loss = STAGE3_POISSON_WEIGHT * poisson_loss + hydro_loss + IC_WEIGHT * ic_loss\n- \n-             scaled_loss = loss / num_effective_batches\n-             scaled_loss.backward()\n-             total_loss_scalar += loss.item()\n-             last_ic_scalar = ic_loss.item()\n-             last_pde_scalar = (STAGE3_POISSON_WEIGHT * poisson_loss + hydro_loss).item()\n- \n-         # Optional extra t=0 Poisson constraints (outside batch loop)\n-         extra_poisson, extra_phi_mean = _compute_poisson_ic_penalties(net, collocation_poisson_ic, model.dimension)\n-         if extra_poisson is not None:\n-             extra_poisson.backward()\n-             total_loss_scalar += extra_poisson.item()\n-         if extra_phi_mean is not None:\n-             extra_phi_mean.backward()\n-             total_loss_scalar += extra_phi_mean.item()\n- \n-         torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n-         optimizer.step()\n- \n-         avg_loss_scalar = total_loss_scalar / num_effective_batches\n-         if i % 50 == 0:\n-             print(f\"Stage 3 Adam Iter {i}: Total Loss = {avg_loss_scalar:.2e}\", flush=True)\n-             _log_diagnostics(\n-                 stage1_iters + stage1_lbfgs_iters + stage2_iters + stage2_lbfgs_iters + i,\n-                 avg_loss_scalar,\n-                 last_pde_scalar,\n-                 last_ic_scalar\n-             )\n- \n-     for i in range(int(stage3_lbfgs_iters)):\n-         loss_holder = [None]\n-         ic_holder = [0.0]\n- \n-         # Cache batch indices for L-BFGS\n-         cached_dom_indices = [_random_batch_indices(dom_n, bs, device) for _ in range(num_effective_batches)]\n-         cached_ic_indices = [_random_batch_indices(ic_n, bs, device) for _ in range(num_effective_batches)]\n- \n-         def lbfgs_closure():\n-             optimizerL.zero_grad()\n-             total_loss_scalar = 0.0\n-             last_ic = 0.0\n-             for batch_idx in range(num_effective_batches):\n-                 dom_idx = cached_dom_indices[batch_idx]\n-                 ic_idx = cached_ic_indices[batch_idx]\n- \n-                 batch_dom = _select_batch(collocation_domain, dom_idx)\n-                 batch_ic = _make_batch_tensors(collocation_IC, ic_idx)\n- \n-                 hydro_loss = _compute_hydro_losses(model, net, batch_dom)\n-                 poisson_loss = _compute_poisson_loss_from_pde(model, net, batch_dom)\n-                 ic_loss = _compute_ic_loss(net, batch_ic, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function)\n- \n-                 loss = STAGE3_POISSON_WEIGHT * poisson_loss + hydro_loss + IC_WEIGHT * ic_loss\n-                 scaled_loss = loss / num_effective_batches\n-                 scaled_loss.backward()\n-                 total_loss_scalar += loss.item()\n-                 last_ic = ic_loss.item()\n- \n-             # Extra Poisson IC penalties\n-             extra_poisson, extra_phi_mean = _compute_poisson_ic_penalties(net, collocation_poisson_ic, model.dimension)\n-             if extra_poisson is not None:\n-                 extra_poisson.backward()\n-                 total_loss_scalar += extra_poisson.item()\n-             if extra_phi_mean is not None:\n-                 extra_phi_mean.backward()\n-                 total_loss_scalar += extra_phi_mean.item()\n- \n-             torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n-             avg_loss = torch.tensor(total_loss_scalar / num_effective_batches, device=device)\n-             loss_holder[0] = avg_loss\n-             ic_holder[0] = last_ic\n-             return avg_loss\n- \n-         loss = optimizerL.step(lbfgs_closure)\n-         if i % 50 == 0:\n-             value = loss_holder[0].item() if loss_holder[0] is not None else float(loss)\n-             print(f\"Stage 3 L-BFGS Iter {i}: Total Loss = {value:.2e}\", flush=True)\n-             _log_diagnostics(\n-                 stage1_iters + stage1_lbfgs_iters + stage2_iters + stage2_lbfgs_iters + stage3_iters + i,\n-                 value,\n-                 value,\n-                 ic_holder[0]\n-             )\n- \n-     # Final diagnostics plotting\n-     if diagnostics is not None:\n-         try:\n-             final_iteration = (\n-                 stage1_iters + stage1_lbfgs_iters +\n-                 stage2_iters + stage2_lbfgs_iters +\n-                 stage3_iters + stage3_lbfgs_iters - 1\n-             )\n-             from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n-             with torch.no_grad():\n-                 true_ic_data = {\n-                     'colloc_IC': collocation_IC,\n-                     'rho': fun_rho_0(rho_1, lam, collocation_IC),\n-                     'vx': fun_vx_0(lam, jeans, v_1, collocation_IC),\n-                     'vy': fun_vy_0(lam, jeans, v_1, collocation_IC),\n-                     'vz': fun_vz_0(lam, jeans, v_1, collocation_IC) if model.dimension == 3 else None,\n-                 }\n-             diagnostics.run_unified_diagnostics(\n-                 model=net,\n-                 true_ic_data=true_ic_data,\n-                 final_iteration=final_iteration\n-             )\n-             print(f\"\\n[INFO] All diagnostics saved to ./diagnostics/\")\n-         except Exception as _diag_err:\n-             import traceback\n-             print(f\"[WARN] Final diagnostics plotting failed: {_diag_err}\")\n-             traceback.print_exc()\n- \n-     return diagnostics",
        "oldStart": 442,
        "newStart": 442,
        "oldLines": 360,
        "newLines": 2,
        "linesRemoved": 358
      }
    ],
    "editor": "EDITOR_AI"
  },
  "shouldAutoFixLints": true,
  "resultForModel": "Success. Updated the following files:\nM c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations\\training\\trainer.py",
  "contentsAfterEdit": "\"\"\"\nTraining loop management for PINN and XPINN.\n\nThis module handles the optimization loops (Adam and L-BFGS) for both\nsingle PINN and XPINN decomposition training.\n\"\"\"\nimport numpy as np\nimport torch\nfrom training.training_diagnostics import TrainingDiagnostics\nfrom config import BATCH_SIZE, NUM_BATCHES, ENABLE_TRAINING_DIAGNOSTICS\nfrom training.physics import closure_batched, closure_batched_cached, _random_batch_indices, _make_batch_tensors\n\n\ndef _compute_ic_component_losses(net, collocation_IC, rho_1, lam, jeans, v_1, dimension, mse_cost_function):\n    \"\"\"\n    Compute individual IC component losses for 3D power spectrum diagnostics.\n    \n    Returns:\n        Dict with component-wise losses: {'rho', 'vx', 'vy', 'vz', 'phi'}\n    \"\"\"\n    from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n    from config import rho_o\n    \n    with torch.no_grad():\n        # Get true ICs\n        rho_0 = fun_rho_0(rho_1, lam, collocation_IC)\n        vx_0 = fun_vx_0(lam, jeans, v_1, collocation_IC)\n        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_IC)\n        vz_0 = fun_vz_0(lam, jeans, v_1, collocation_IC)\n        \n        # Get predictions\n        net_ic_out = net(collocation_IC)\n        rho_ic_out = net_ic_out[:, 0:1]\n        vx_ic_out = net_ic_out[:, 1:2]\n        vy_ic_out = net_ic_out[:, 2:3]\n        vz_ic_out = net_ic_out[:, 3:4]\n        phi_ic_out = net_ic_out[:, 4:5]\n        \n        # Compute component losses\n        ic_losses = {\n            'rho': mse_cost_function(rho_ic_out, rho_0).item(),\n            'vx': mse_cost_function(vx_ic_out, vx_0).item(),\n            'vy': mse_cost_function(vy_ic_out, vy_0).item(),\n            'vz': mse_cost_function(vz_ic_out, vz_0).item(),\n            'phi': torch.mean(phi_ic_out ** 2).item(),  # phi should be close to 0 ideally\n        }\n    \n    return ic_losses\n\n\ndef train(model, net, collocation_domain, collocation_IC, optimizer, optimizerL, iteration_adam, iterationL, mse_cost_function, closure, rho_1, lam, jeans, v_1, device, data_terms=None, collocation_poisson_ic=None):\n    \"\"\"\n    Standard training loop for single PINN.\n    \n    Manages Adam and L-BFGS optimization phases with cosine scheduling for\n    continuity weight and startup_dt parameters.\n    \n    Args:\n        model: Collocation model\n        net: Neural network\n        collocation_domain: Domain collocation points\n        collocation_IC: Initial condition points\n        optimizer: Adam optimizer\n        optimizerL: L-BFGS optimizer\n        iteration_adam: Number of Adam iterations\n        iterationL: Number of L-BFGS iterations\n        mse_cost_function: MSE loss function\n        closure: Closure function (unused, kept for compatibility)\n        rho_1: Perturbation amplitude\n        lam: Wavelength\n        jeans: Jeans length\n        v_1: Velocity amplitude\n        device: PyTorch device\n        data_terms: Optional list of additional supervised datasets with weights\n        collocation_poisson_ic: Extra collocation points at t=0 for Poisson enforcement\n    \"\"\"\n    bs = int(BATCH_SIZE)\n    nb = int(NUM_BATCHES)\n\n    # Import config values for diagnostics\n    from config import PERTURBATION_TYPE, tmax\n    \n    # Create diagnostics with correct dimension and perturbation type\n    diagnostics = TrainingDiagnostics(\n        save_dir='./diagnostics/',\n        dimension=model.dimension,\n        perturbation_type=PERTURBATION_TYPE\n    ) if ENABLE_TRAINING_DIAGNOSTICS else None\n\n    for i in range(iteration_adam):\n        loss, loss_breakdown = optimizer.step(lambda: closure_batched(model, net, mse_cost_function, collocation_domain, collocation_IC, optimizer, rho_1, lam, jeans, v_1, bs, nb, update_tracker=True, iteration=i, use_fft_poisson=True, data_terms=data_terms, collocation_poisson_ic=collocation_poisson_ic))\n\n        with torch.autograd.no_grad():\n            # Diagnostics logging every 50 iterations\n            if i % 50 == 0 and diagnostics is not None:\n                try:\n                    # Compute IC component losses for 3D power spectrum tracking\n                    ic_component_losses = None\n                    if diagnostics.is_3d_power_spectrum:\n                        ic_component_losses = _compute_ic_component_losses(\n                            net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function\n                        )\n                    \n                    diagnostics.log_iteration(\n                        iteration=i,\n                        model=net,\n                        loss_dict={\n                            'total': loss.item(),\n                            'pde': float(loss_breakdown.get('PDE', 0.0)),\n                            'ic': float(loss_breakdown.get('IC', 0.0))\n                        },\n                        geomtime_col=collocation_domain,\n                        ic_component_losses=ic_component_losses\n                    )\n                except Exception as _diag_err:\n                    # Keep training robust if diagnostics fail\n                    print(f\"[WARN] Diagnostics logging failed at {i}: {_diag_err}\")\n\n            if i % 100 == 0:\n                print(f\"Training Loss at {i} for Adam (batched) in {model.dimension}D system = {loss.item():.2e}\", flush=True)\n                # Print loss breakdown\n                breakdown_str = \" | \".join([f\"{k}: {v:.2e}\" for k, v in loss_breakdown.items() if v &gt; 0])\n                if breakdown_str:\n                    print(f\"  Loss breakdown: {breakdown_str}\", flush=True)\n\n    for i in range(iterationL):\n        global_step = iteration_adam + i\n\n        # L-BFGS expects a closure that returns only scalar loss\n        # Store loss_breakdown in a list so we can access it after the step\n        loss_breakdown_holder = [None]\n        \n        # Cache batch indices for L-BFGS step (L-BFGS calls closure multiple times)\n        # Using same indices ensures consistent loss landscape during line search\n        if isinstance(collocation_domain, (list, tuple)):\n            dom_n = collocation_domain[0].size(0)\n            device = collocation_domain[0].device\n        else:\n            dom_n = collocation_domain.size(0)\n            device = collocation_domain.device\n        ic_n = collocation_IC[0].size(0)\n        \n        cached_dom_indices = [_random_batch_indices(dom_n, bs, device) for _ in range(int(max(1, nb)))]\n        cached_ic_indices = [_random_batch_indices(ic_n, bs, device) for _ in range(int(max(1, nb)))]\n        \n        def lbfgs_closure():\n            optimizerL.zero_grad()\n            loss, loss_breakdown = closure_batched_cached(\n                model, net, mse_cost_function, collocation_domain, collocation_IC,\n                optimizerL, rho_1, lam, jeans, v_1, bs, nb,\n                cached_dom_indices, cached_ic_indices,\n                data_terms=data_terms, collocation_poisson_ic=collocation_poisson_ic\n            )\n            loss_breakdown_holder[0] = loss_breakdown\n            return loss\n        \n        loss = optimizerL.step(lbfgs_closure)\n        loss_breakdown = loss_breakdown_holder[0]\n\n        with torch.autograd.no_grad():\n            # Diagnostics logging every 50 iterations in LBFGS too\n            if i % 50 == 0 and loss_breakdown is not None and diagnostics is not None:\n                try:\n                    # Compute IC component losses for 3D power spectrum tracking\n                    ic_component_losses = None\n                    if diagnostics.is_3d_power_spectrum:\n                        ic_component_losses = _compute_ic_component_losses(\n                            net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function\n                        )\n                    \n                    diagnostics.log_iteration(\n                        iteration=iteration_adam + i,\n                        model=net,\n                        loss_dict={\n                            'total': loss.item() if hasattr(loss, 'item') else float(loss),\n                            'pde': float(loss_breakdown.get('PDE', 0.0)),\n                            'ic': float(loss_breakdown.get('IC', 0.0))\n                        },\n                        geomtime_col=collocation_domain,\n                        ic_component_losses=ic_component_losses\n                    )\n                except Exception as _diag_err:\n                    print(f\"[WARN] Diagnostics logging (LBFGS) failed at {i}: {_diag_err}\")\n            if i % 50 == 0:\n                print(f\"Training Loss at {i} for LBGFS (batched) in {model.dimension}D system = {loss.item():.2e}\", flush=True)\n                # Print loss breakdown\n                breakdown_str = \" | \".join([f\"{k}: {v:.2e}\" for k, v in loss_breakdown.items() if v &gt; 0])\n                if breakdown_str:\n                    print(f\"  Loss breakdown: {breakdown_str}\", flush=True)\n    \n    # Generate diagnostic plots at the end of training\n    if diagnostics is not None:\n        try:\n            final_iteration = iteration_adam + iterationL - 1\n            \n            # Generate unified diagnostics (5 plots for all cases)\n            print(\"\\n[INFO] Generating unified training diagnostics...\")\n            from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n            \n            # Evaluate true ICs on the IC collocation points for comparison\n            with torch.no_grad():\n                true_ic_data = {\n                    'colloc_IC': collocation_IC,\n                    'rho': fun_rho_0(rho_1, lam, collocation_IC),\n                    'vx': fun_vx_0(lam, jeans, v_1, collocation_IC),\n                    'vy': fun_vy_0(lam, jeans, v_1, collocation_IC),\n                    'vz': fun_vz_0(lam, jeans, v_1, collocation_IC) if model.dimension == 3 else None,\n                }\n            \n            diagnostics.run_unified_diagnostics(\n                model=net, \n                true_ic_data=true_ic_data,\n                final_iteration=final_iteration\n            )\n            \n            print(f\"\\n[INFO] All diagnostics saved to ./diagnostics/\")\n        except Exception as _diag_err:\n            import traceback\n            print(f\"[WARN] Final diagnostics plotting failed: {_diag_err}\")\n            traceback.print_exc()\n    \n    # Return diagnostics object for potential reuse in post-training analysis\n    return diagnostics\n\n\ndef _select_batch(collocation, indices):\n    if isinstance(collocation, (list, tuple)):\n        return _make_batch_tensors(collocation, indices)\n    # Single tensor format: split into [x, y, t]\n    return [collocation[indices, i:i+1] for i in range(collocation.shape[1])]\n\n\ndef _phi_slice_index(dimension):\n    if dimension == 1:\n        return 2\n    if dimension == 2:\n        return 3\n    return 4\n\n\ndef _compute_poisson_loss_from_ic(net, collocation_points, rho_1, lam, dimension):\n    \"\"\"\n    Compute Poisson loss using IC density (Stage 1).\n    \n    The Poisson equation is: ∇²φ = const * (ρ - ρ₀)\n    We use the true IC density ρ_ic from fun_rho_0 and the network's predicted φ.\n    \"\"\"\n    from core.data_generator import diff\n    from core.initial_conditions import fun_rho_0\n    from config import const, rho_o\n    \n    # Ensure coords is a list and enable gradient tracking\n    if isinstance(collocation_points, (list, tuple)):\n        coords = []\n        for coord in collocation_points:\n            if not coord.requires_grad:\n                coord = coord.requires_grad_(True)\n            coords.append(coord)\n    else:\n        # Single tensor format: split into [x, y, (z), t] and enable gradients\n        coords = []\n        for i in range(collocation_points.shape[1]):\n            coord = collocation_points[:, i:i+1]\n            if not coord.requires_grad:\n                coord = coord.requires_grad_(True)\n            coords.append(coord)\n    \n    # Forward pass (network expects list format)\n    outputs = net(coords)\n    phi = outputs[:, _phi_slice_index(dimension):_phi_slice_index(dimension) + 1]\n\n    if not torch.isfinite(phi).all():\n        safe = torch.tensor(0.0, device=phi.device, requires_grad=True)\n        return safe\n    \n    # Extract spatial coordinates\n    x = coords[0]\n    \n    # Compute first derivative to detect blow-ups early\n    phi_x = diff(phi, x, order=1)\n    if torch.isinf(phi_x).any():\n        safe = torch.tensor(1e10, device=phi.device, requires_grad=True)\n        return safe\n\n    # Check for NaN in phi before computing laplacian\n    if torch.isnan(phi).any():\n        print(f\"[DEBUG] NaN detected in phi output!\")\n        print(f\"  phi shape: {phi.shape}\")\n        print(f\"  phi has NaN: {torch.isnan(phi).sum().item()} / {phi.numel()}\")\n        try:\n            print(f\"  phi range: [{phi.min().item():.2e}, {phi.max().item():.2e}]\")\n        except:\n            print(f\"  phi min/max computation failed\")\n    \n    # Compute Laplacian and check for NaN\n    if dimension == 1:\n        laplacian = diff(phi, x, order=2)\n    elif dimension == 2:\n        y = coords[1]\n        laplacian = diff(phi, x, order=2) + diff(phi, y, order=2)\n    else:  # dimension == 3\n        y = coords[1]\n        z = coords[2]\n        laplacian = diff(phi, x, order=2) + diff(phi, y, order=2) + diff(phi, z, order=2)\n    \n    if torch.isnan(laplacian).any():\n        pass\n\n    if torch.isinf(laplacian).any():\n        laplacian = torch.clamp(laplacian, -1e10, 1e10)\n    \n    # Get true IC density\n    rho_ic = fun_rho_0(rho_1, lam, coords)\n    \n    if torch.isnan(rho_ic).any():\n        pass\n    \n    # Poisson residual: ∇²φ - const*(ρ - ρ₀)\n    residual = laplacian - const * (rho_ic - rho_o)\n\n    if torch.isinf(residual).any():\n        # Clamp to avoid NaN in subsequent operations\n        residual = torch.clamp(residual, -1e10, 1e10)\n    \n    # Check for NaN and print debug info\n    if torch.isnan(residual).any():\n        print(f\"[DEBUG] NaN detected in Poisson residual!\")\n        print(f\"  residual has NaN: {torch.isnan(residual).sum().item()} / {residual.numel()}\")\n        try:\n            print(f\"  laplacian range: [{laplacian.min().item():.2e}, {laplacian.max().item():.2e}]\")\n            print(f\"  rho_ic range: [{rho_ic.min().item():.2e}, {rho_ic.max().item():.2e}]\")\n            print(f\"  phi range: [{phi.min().item():.2e}, {phi.max().item():.2e}]\")\n        except Exception as e:\n            print(f\"  Could not compute ranges: {e}\")\n        print(f\"  const: {const}, rho_o: {rho_o}\")\n        import sys\n        sys.stdout.flush()  # Force flush to ensure output appears\n    \n    # Normalized loss\n    scale = abs(const) * (abs(rho_o) if rho_o != 0 else 1.0)\n    scale = max(scale, 1e-10)  # Avoid division by zero\n    \n    loss = torch.mean((residual / scale) ** 2)\n    \n    if torch.isnan(loss):\n        pass\n    if torch.isinf(loss):\n        safe = torch.tensor(1e8, device=residual.device, requires_grad=True)\n        return safe\n    \n    return loss\n\n\ndef _compute_ic_loss(net, collocation_ic, rho_1, lam, jeans, v_1, dimension, mse_cost_function):\n    from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0\n    from config import rho_o, PERTURBATION_TYPE, KX, KY\n\n    rho_0 = fun_rho_0(rho_1, lam, collocation_ic)\n    vx_0 = fun_vx_0(lam, jeans, v_1, collocation_ic)\n\n    net_ic_out = net(collocation_ic)\n    rho_ic_out = net_ic_out[:, 0:1]\n    vx_ic_out = net_ic_out[:, 1:2]\n\n    if dimension == 2:\n        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_ic)\n        vy_ic_out = net_ic_out[:, 2:3]\n    elif dimension == 3:\n        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_ic)\n        vz_0 = fun_vz_0(lam, jeans, v_1, collocation_ic)\n        vy_ic_out = net_ic_out[:, 2:3]\n        vz_ic_out = net_ic_out[:, 3:4]\n\n    is_sin = str(PERTURBATION_TYPE).lower() == \"sinusoidal\"\n    if is_sin:\n        x_ic_for_ic = collocation_ic[0]\n        if len(collocation_ic) &gt;= 2:\n            y_ic_for_ic = collocation_ic[1]\n            rho_ic_target = rho_o + rho_1 * torch.cos(KX * x_ic_for_ic + KY * y_ic_for_ic)\n        else:\n            rho_ic_target = rho_o + rho_1 * torch.cos(2 * np.pi * x_ic_for_ic / lam)\n        mse_rho_ic = mse_cost_function(rho_ic_out, rho_ic_target)\n    else:\n        mse_rho_ic = 0.0 * torch.mean(rho_ic_out * 0)\n\n    mse_vx_ic = mse_cost_function(vx_ic_out, vx_0)\n    if dimension == 2:\n        mse_vy_ic = mse_cost_function(vy_ic_out, vy_0)\n    elif dimension == 3:\n        mse_vy_ic = mse_cost_function(vy_ic_out, vy_0)\n        mse_vz_ic = mse_cost_function(vz_ic_out, vz_0)\n\n    ic_loss = mse_vx_ic + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)\n    if dimension == 2:\n        ic_loss = ic_loss + mse_vy_ic\n    elif dimension == 3:\n        ic_loss = ic_loss + mse_vy_ic + mse_vz_ic\n\n    return ic_loss\n\n\ndef _compute_hydro_losses(model, net, batch_dom):\n    from core.losses import pde_residue\n    from config import cs, rho_o\n\n    if model.dimension == 1:\n        rho_r, vx_r, _phi_r = pde_residue(batch_dom, net, dimension=1)\n    elif model.dimension == 2:\n        rho_r, vx_r, vy_r, _phi_r = pde_residue(batch_dom, net, dimension=2)\n    else:\n        rho_r, vx_r, vy_r, vz_r, _phi_r = pde_residue(batch_dom, net, dimension=3)\n\n    continuity_scale = abs(rho_o * cs)\n    continuity_scale = continuity_scale if continuity_scale &gt; 0 else 1.0\n    momentum_scale = abs(cs ** 2) if cs != 0 else 1.0\n\n    loss_cont = torch.mean((rho_r / continuity_scale) ** 2)\n    loss_mom = torch.mean((vx_r / momentum_scale) ** 2)\n    if model.dimension == 2:\n        loss_mom = loss_mom + torch.mean((vy_r / momentum_scale) ** 2)\n    elif model.dimension == 3:\n        loss_mom = loss_mom + torch.mean((vy_r / momentum_scale) ** 2)\n        loss_mom = loss_mom + torch.mean((vz_r / momentum_scale) ** 2)\n\n    return loss_cont + loss_mom\n\n\ndef _compute_poisson_loss_from_pde(model, net, batch_dom):\n    from core.losses import pde_residue\n    from config import const, rho_o\n\n    if model.dimension == 1:\n        _rho_r, _vx_r, phi_r = pde_residue(batch_dom, net, dimension=1)\n    elif model.dimension == 2:\n        _rho_r, _vx_r, _vy_r, phi_r = pde_residue(batch_dom, net, dimension=2)\n    else:\n        _rho_r, _vx_r, _vy_r, _vz_r, phi_r = pde_residue(batch_dom, net, dimension=3)\n\n    scale = abs(const) * (abs(rho_o) if rho_o != 0 else 1.0)\n    scale = scale if scale &gt; 0 else 1.0\n    return torch.mean((phi_r / scale) ** 2)\n\n"
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Verifying no dangling references**</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "multi[-_ ]stage|train_multistage|STAGE\\d|USE_MULTI_STAGE_TRAINING",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations",
  "outputMode": "content",
  "caseInsensitive": false,
  "headLimit": 200
}
```
Additional data:

```json
{
  "isPruned": true,
  "pattern": "multi[-_ ]stage|train_multistage|STAGE\\d|USE_MULTI_STAGE_TRAINING",
  "path": "c:\\Users\\tirth\\Documents\\Projects\\GRINN\\Power spectrum perturbations",
  "outputMode": "content",
  "totalFiles": 0,
  "totalMatches": 0,
  "topFiles": []
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Noting need for reading lints on edited files**</details></think>

---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 3 files</summary>
      
Lint paths:

- `c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\config.py`
- `c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\train.py`
- `c:\Users\tirth\Documents\Projects\GRINN\Power spectrum perturbations\training\trainer.py`

Lint results


**No lint errors found**
</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Summarizing final changes without tests**

No tests were run during this update. The multi-stage configuration and related function have been removed to simplify the setup. Final response will reflect these key changes without code references.</details></think>

---

Removed all multi-stage training hooks and config so training always runs the standard single-stage flow, and deleted the unused multistage implementation code.

Details:
- Cleaned multi-stage flags and iteration/weight settings from `config.py`.
- Simplified `train.py` to always call `train()` and removed multistage imports.
- Deleted multistage helper logic and `train_multistage()` from `training/trainer.py`.

Tests not run (not requested).

---

