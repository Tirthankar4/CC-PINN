# Auto-generated by package_for_kaggle.py
# Do not edit manually; regenerate from the modular codebase instead.
import sys
import types

def _register_module(module_name, attr_names):
    module = types.ModuleType(module_name)
    module.__package__ = module_name
    for attr in attr_names:
        if attr in globals():
            module.__dict__[attr] = globals()[attr]
    sys.modules[module_name] = module
    parts = module_name.split('.')
    for i in range(1, len(parts)):
        parent_name = '.'.join(parts[:i])
        if parent_name not in sys.modules:
            parent_module = types.ModuleType(parent_name)
            parent_module.__package__ = parent_name
            parent_module.__path__ = []
            sys.modules[parent_name] = parent_module
            if i == 1:
                globals()[parts[0]] = parent_module
    if len(parts) > 1:
        parent = sys.modules['.'.join(parts[:-1])]
        setattr(parent, parts[-1], module)
        module.__package__ = parent.__name__
    else:
        globals()[parts[0]] = module
    if '__path__' not in module.__dict__:
        module.__path__ = []
    return module

if 'core' not in sys.modules:
    core_module = types.ModuleType('core')
    core_module.__package__ = 'core'
    core_module.__path__ = []
    sys.modules['core'] = core_module
    globals()['core'] = core_module

if 'methods' not in sys.modules:
    methods_module = types.ModuleType('methods')
    methods_module.__package__ = 'methods'
    methods_module.__path__ = []
    sys.modules['methods'] = methods_module
    globals()['methods'] = methods_module

if 'numerical_solvers' not in sys.modules:
    numerical_solvers_module = types.ModuleType('numerical_solvers')
    numerical_solvers_module.__package__ = 'numerical_solvers'
    numerical_solvers_module.__path__ = []
    sys.modules['numerical_solvers'] = numerical_solvers_module
    globals()['numerical_solvers'] = numerical_solvers_module

if 'training' not in sys.modules:
    training_module = types.ModuleType('training')
    training_module.__package__ = 'training'
    training_module.__path__ = []
    sys.modules['training'] = training_module
    globals()['training'] = training_module

if 'utilities' not in sys.modules:
    utilities_module = types.ModuleType('utilities')
    utilities_module.__package__ = 'utilities'
    utilities_module.__path__ = []
    sys.modules['utilities'] = utilities_module
    globals()['utilities'] = utilities_module

if 'visualization' not in sys.modules:
    visualization_module = types.ModuleType('visualization')
    visualization_module.__package__ = 'visualization'
    visualization_module.__path__ = []
    sys.modules['visualization'] = visualization_module
    globals()['visualization'] = visualization_module

# ==== Module: core.data_generator (core/data_generator.py) ====
import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
from config import STARTUP_DT
from config import cs, const, G, rho_o

def diff(u, var, order = 1): #The derivative of a variable with respect to another.
    
    u.requires_grad_()
    var.requires_grad_()
    ones = torch.ones_like(u)
    der, = torch.autograd.grad(u, var, create_graph = True, grad_outputs = ones, allow_unused = True)
    if der is None:
        return torch.zeros_like(var, requires_grad = True)
    else:
        der.requires_grad_()
    for i in range(1, order):
        ones = torch.ones_like(der)
        der, = torch.autograd.grad(der, var, create_graph = True, grad_outputs = ones, allow_unused = True)
        if der is None:
            return torch.zeros_like(var, requires_grad = True)
        else:
            der.requires_grad_()
    return der


class col_gen(object):
    """
    Collocation point generator for PINNs.
    
    Generates collocation points for:
    - Domain (PDE enforcement)
    - Initial conditions (IC)
    - Boundary conditions (BC)
    
    Args:
        rmin: List of minimum values [xmin, ymin, zmin, tmin] (depending on dimension)
        rmax: List of maximum values [xmax, ymax, zmax, tmax] (depending on dimension)
        N_0: Number of initial condition collocation points
        N_b: Number of boundary condition points (per boundary)
        N_r: Number of residual/domain collocation points
        dimension: Spatial dimension (1, 2, or 3)
    """

    def __init__(self, rmin = [0, 0, 0, 0], rmax = [1, 1, 1, 1], N_0 = 1000, N_b = 1000, N_r = 3000, dimension = 1):
        self.rmin = rmin
        self.rmax = rmax
        self.N_0 = N_0
        self.N_b = N_b
        self.N_r = N_r
        self.dimension = dimension
    
    def _generate_uniform_tensor(self, n_points, lower, upper, device = 'cuda', requires_grad = True):
        """
        Helper function to generate uniformly distributed tensor.
        
        Args:
            n_points: Number of points to generate
            lower: Lower bound
            upper: Upper bound
            device: PyTorch device
            requires_grad: Whether tensor requires gradients
        
        Returns:
            Tensor of shape [n_points, 1]
        """
        tensor = torch.empty(n_points, 1, device = device, dtype = torch.float32).uniform_(lower, upper)
        if requires_grad:
            tensor = tensor.requires_grad_()
        return tensor
    
    def _generate_constant_tensor(self, n_points, value, device='cuda', requires_grad=True):
        """
        Helper function to generate constant-valued tensor.
        
        Args:
            n_points: Number of points
            value: Constant value
            device: PyTorch device
            requires_grad: Whether tensor requires gradients
        
        Returns:
            Tensor of shape [n_points, 1]
        """
        tensor = torch.empty(n_points, 1, device=device, dtype=torch.float32).fill_(value)
        if requires_grad:
            tensor = tensor.requires_grad_()
        return tensor
        
        
    def geo_time_coord(self,option,coordinate=1):
        '''
        option: Takes arguments: "Domain", "BC" for Boundary conditions, "IC" for initial conditions:
        
        '''
        
        if self.dimension == 1: 
            if option == "Domain":
                x_coor = self._generate_uniform_tensor(self.N_r, self.rmin[0], self.rmax[0])
                t_coor = self._generate_uniform_tensor(self.N_r, max(self.rmin[1], STARTUP_DT), self.rmax[1])
                return [x_coor, t_coor]

            if option == "IC":
                x_0 = self._generate_uniform_tensor(self.N_0, self.rmin[0], self.rmax[0])
                t_0 = self._generate_constant_tensor(self.N_0, 0.0)
                return [x_0, t_0]

            if option == "BC":
                if self.N_b == 0:
                    return [], []
                
                x_bc_l = self._generate_constant_tensor(self.N_b, self.rmin[coordinate-1])
                x_bc_r = self._generate_constant_tensor(self.N_b, self.rmax[coordinate-1])
                t_bc = self._generate_uniform_tensor(self.N_b, self.rmin[self.dimension], self.rmax[self.dimension])
                
                return [x_bc_l, t_bc], [x_bc_r, t_bc]
            
        if self.dimension == 2:
            if option == "Domain":
                x_dom = self._generate_uniform_tensor(self.N_r, self.rmin[0], self.rmax[0])
                y_dom = self._generate_uniform_tensor(self.N_r, self.rmin[1], self.rmax[1])
                t_dom = self._generate_uniform_tensor(self.N_r, max(self.rmin[2], STARTUP_DT), self.rmax[2])
                return [x_dom, y_dom, t_dom]
            
            if option == "IC":
                x_0 = self._generate_uniform_tensor(self.N_0, self.rmin[0], self.rmax[0])
                y_0 = self._generate_uniform_tensor(self.N_0, self.rmin[1], self.rmax[1])
                t_0 = self._generate_constant_tensor(self.N_0, 0.0)
                return [x_0, y_0, t_0]
            
            if option == "BC":
                if self.N_b == 0:
                    return [], []
                
                t_bc = self._generate_uniform_tensor(self.N_b, max(self.rmin[self.dimension], STARTUP_DT), self.rmax[self.dimension])
                
                if coordinate == 1:
                    x_bc_l = self._generate_constant_tensor(self.N_b, self.rmin[0])
                    x_bc_r = self._generate_constant_tensor(self.N_b, self.rmax[0])
                    y_bc = self._generate_uniform_tensor(self.N_b, self.rmin[1], self.rmax[1])
                    return [x_bc_l, y_bc, t_bc], [x_bc_r, y_bc, t_bc]
                
                if coordinate == 2:
                    y_bc_l = self._generate_constant_tensor(self.N_b, self.rmin[1])
                    y_bc_r = self._generate_constant_tensor(self.N_b, self.rmax[1])
                    x_bc = self._generate_uniform_tensor(self.N_b, self.rmin[0], self.rmax[0])
                    return [x_bc, y_bc_l, t_bc], [x_bc, y_bc_r, t_bc]
    
            
        if self.dimension == 3: 
            if option == "Domain":
                x_dom = self._generate_uniform_tensor(self.N_r, self.rmin[0], self.rmax[0])
                y_dom = self._generate_uniform_tensor(self.N_r, self.rmin[1], self.rmax[1])
                z_dom = self._generate_uniform_tensor(self.N_r, self.rmin[2], self.rmax[2])
                t_dom = self._generate_uniform_tensor(self.N_r, max(self.rmin[3], STARTUP_DT), self.rmax[3])
                return [x_dom, y_dom, z_dom, t_dom]

            if option == "IC":
                x_0 = self._generate_uniform_tensor(self.N_0, self.rmin[0], self.rmax[0])
                y_0 = self._generate_uniform_tensor(self.N_0, self.rmin[1], self.rmax[1])
                z_0 = self._generate_uniform_tensor(self.N_0, self.rmin[2], self.rmax[2])
                t_0 = self._generate_constant_tensor(self.N_0, 0.0)
                return [x_0, y_0, z_0, t_0]
             

            if option == "BC":
                if self.N_b == 0:
                    return [], []
                
                t_bc = self._generate_uniform_tensor(self.N_b, max(self.rmin[self.dimension], STARTUP_DT), self.rmax[self.dimension])
                
                if coordinate == 1:
                    x_bc_l = self._generate_constant_tensor(self.N_b, self.rmin[0])
                    x_bc_r = self._generate_constant_tensor(self.N_b, self.rmax[0])
                    y_bc = self._generate_uniform_tensor(self.N_b, self.rmin[1], self.rmax[1])
                    z_bc = self._generate_uniform_tensor(self.N_b, self.rmin[2], self.rmax[2])
                    return [x_bc_l, y_bc, z_bc, t_bc], [x_bc_r, y_bc, z_bc, t_bc]
                
                if coordinate == 2:
                    y_bc_l = self._generate_constant_tensor(self.N_b, self.rmin[1])
                    y_bc_r = self._generate_constant_tensor(self.N_b, self.rmax[1])
                    x_bc = self._generate_uniform_tensor(self.N_b, self.rmin[0], self.rmax[0])
                    z_bc = self._generate_uniform_tensor(self.N_b, self.rmin[2], self.rmax[2])
                    return [x_bc, y_bc_l, z_bc, t_bc], [x_bc, y_bc_r, z_bc, t_bc]
                
                if coordinate == 3:
                    z_bc_l = self._generate_constant_tensor(self.N_b, self.rmin[2])
                    z_bc_r = self._generate_constant_tensor(self.N_b, self.rmax[2])
                    x_bc = self._generate_uniform_tensor(self.N_b, self.rmin[0], self.rmax[0])
                    y_bc = self._generate_uniform_tensor(self.N_b, self.rmin[1], self.rmax[1])
                    return [x_bc, y_bc, z_bc_l, t_bc], [x_bc, y_bc, z_bc_r, t_bc]
    
    def geo_time_coord_subdomain(self, option, subdomain_bounds, device='cuda', coordinate=1):
        """
        Generate collocation points within a subdomain.
        
        Args:
            option: "Domain", "IC", or "BC"
            subdomain_bounds: Tuple (x_min, x_max, y_min, y_max) for 2D
            device: PyTorch device
            coordinate: For BC generation (1=x, 2=y)
        
        Returns:
            Collocation points list constrained to subdomain
        """
        # For 2D (current focus)
        if self.dimension == 2:
            x_min, x_max, y_min, y_max = subdomain_bounds
            
            if option == "Domain":
                coor = []
                x_dom = torch.empty(self.N_r, 1, device=device, dtype=torch.float32).uniform_(x_min, x_max).requires_grad_()
                y_dom = torch.empty(self.N_r, 1, device=device, dtype=torch.float32).uniform_(y_min, y_max).requires_grad_()
                t_dom = torch.empty(self.N_r, 1, device=device, dtype=torch.float32).uniform_(max(self.rmin[2], STARTUP_DT), self.rmax[2]).requires_grad_()
                coor.append(x_dom)
                coor.append(y_dom)
                coor.append(t_dom)
                return coor
            
            elif option == "IC":
                coor = []
                x_ic = torch.empty(self.N_0, 1, device=device, dtype=torch.float32).uniform_(x_min, x_max).requires_grad_()
                y_ic = torch.empty(self.N_0, 1, device=device, dtype=torch.float32).uniform_(y_min, y_max).requires_grad_()
                t_ic = torch.empty(self.N_0, 1, device=device, dtype=torch.float32).fill_(0).requires_grad_()
                coor.append(x_ic)
                coor.append(y_ic)
                coor.append(t_ic)
                return coor
            
            elif option == "BC":
                # Only generate BC if subdomain touches exterior boundary
                # This is handled by get_exterior_boundary_info
                if self.N_b == 0:
                    return [], []
                
                t_bc = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).uniform_(max(self.rmin[2], STARTUP_DT), self.rmax[2])
                t_bc.requires_grad_()
                
                if coordinate == 1:
                    x_bc_l = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).fill_(x_min).requires_grad_()
                    x_bc_r = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).fill_(x_max).requires_grad_()
                    y_bc = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).uniform_(y_min, y_max).requires_grad_()
                    
                    coor_l = [x_bc_l, y_bc, t_bc]
                    coor_r = [x_bc_r, y_bc, t_bc]
                    return coor_l, coor_r
                
                elif coordinate == 2:
                    y_bc_l = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).fill_(y_min).requires_grad_()
                    y_bc_r = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).fill_(y_max).requires_grad_()
                    x_bc = torch.empty(self.N_b, 1, device=device, dtype=torch.float32).uniform_(x_min, x_max).requires_grad_()
                    
                    coor_l = [x_bc, y_bc_l, t_bc]
                    coor_r = [x_bc, y_bc_r, t_bc]
                    return coor_l, coor_r
        
        else:
            raise NotImplementedError(f"Subdomain collocation not yet implemented for dimension {self.dimension}")
    
    def get_exterior_boundary_info(self, subdomain_idx, nx_sub, ny_sub):
        """
        Determine which boundaries of a subdomain are exterior boundaries.
        Uses xpinn_decomposition utilities.
        
        Args:
            subdomain_idx: Linear subdomain index
            nx_sub: Number of subdomain splits in x-direction
            ny_sub: Number of subdomain splits in y-direction
        
        Returns:
            Dict with keys 'left', 'right', 'bottom', 'top' indicating if boundary is exterior
        """
        from methods.xpinn_decomposition import get_exterior_boundary_info
        
        # Extract global domain bounds from self.rmin, self.rmax
        if self.dimension == 2:
            xmin, ymin = self.rmin[0], self.rmin[1]
            xmax, ymax = self.rmax[0], self.rmax[1]
        else:
            raise NotImplementedError(f"Exterior boundary info not yet implemented for dimension {self.dimension}")
        
        return get_exterior_boundary_info(subdomain_idx, nx_sub, ny_sub, xmin, xmax, ymin, ymax)


# ==================== Utility Functions ====================

def input_taker(lam, rho_1, num_of_waves, tmax, N_0, N_b, N_r):
    """
    Parse and validate input parameters for simulation.
    
    Args:
        lam: Wavelength
        rho_1: Amplitude of perturbation
        num_of_waves: Number of waves
        tmax: Maximum time
        N_0: Number of initial condition points
        N_b: Number of boundary points (legacy, not used with hard constraints)
        N_r: Number of collocation points
    
    Returns:
        Tuple (lam, rho_1, num_of_waves, tmax, N_0, N_b, N_r) with correct types
    """
    lam = float(lam)
    rho_1 = float(rho_1)
    num_of_waves = float(num_of_waves)  # Allow fractional values for flexible domain sizing
    tmax = float(tmax)
    N_0 = int(N_0)
    N_r = int(N_r)
    
    return lam, rho_1, num_of_waves, tmax, N_0, N_b, N_r


def req_consts_calc(lam, rho_1):
    """
    Calculate required physical constants for Jeans instability.
    
    Args:
        lam: Wavelength
        rho_1: Amplitude of perturbation (not used in calculation but kept for API consistency)
    
    Returns:
        Tuple (jeans_length, alpha) where:
        - jeans_length: Jeans wavelength (critical wavelength for instability)
        - alpha: Growth rate or oscillation frequency depending on lam vs jeans_length
    """
    
    if rho_o != 0:
        jeans = np.sqrt(4*np.pi**2*cs**2/(const*G*rho_o))
    else:
        jeans = np.sqrt(4*np.pi**2*cs**2/(const*G*(rho_o + 1)))

    if lam > jeans:
        # Unstable regime: exponential growth
        if rho_o != 0:
            alpha = np.sqrt(const*G*rho_o-cs**2*(2*np.pi/lam)**2)
        else:
            alpha = np.sqrt(const*G*(rho_o + 1)-cs**2*(2*np.pi/lam)**2)
    else:
        # Stable regime: oscillations
        if rho_o != 0:
            alpha = np.sqrt(cs**2*(2*np.pi/lam)**2 - const*G*rho_o)
        else:
            alpha = np.sqrt(cs**2*(2*np.pi/lam)**2 - const*G*(rho_o + 1))

    return jeans, alpha


def distribute_collocation_points(n_total, num_subdomains):
    """
    Distribute collocation points across subdomains for XPINN.
    
    Args:
        n_total: Total number of collocation points
        num_subdomains: Number of subdomains
    
    Returns:
        List of point counts per subdomain
    """
    
    # If per-subdomain count is specified, use it
    if n_total == N_r_PER_SUBDOMAIN and N_r_PER_SUBDOMAIN is not None:
        return [N_r_PER_SUBDOMAIN] * num_subdomains
    elif n_total == N_0_PER_SUBDOMAIN and N_0_PER_SUBDOMAIN is not None:
        return [N_0_PER_SUBDOMAIN] * num_subdomains
    
    # Otherwise, auto-distribute evenly
    base_count = n_total // num_subdomains
    remainder = n_total % num_subdomains
    
    counts = [base_count] * num_subdomains
    # Distribute remainder to first few subdomains
    for i in range(remainder):
        counts[i] += 1
    
    return counts


def generate_poisson_ic_points(rmin, rmax, n_points, dimension=2, device='cuda'):
    """
    Generate extra collocation points at t=0 specifically for enforcing Poisson equation.
    
    This implements Option 3: Pure ML approach to fix initial phi by sampling many
    spatial points at t=0 where Poisson equation ∇²φ = const*(ρ-ρ₀) must be satisfied.
    
    Args:
        rmin: List of minimum values [xmin, ymin, (zmin), tmin]
        rmax: List of maximum values [xmax, ymax, (zmax), tmax]
        n_points: Number of spatial points to generate at t=0
        dimension: Spatial dimension (1, 2, or 3)
        device: PyTorch device ('cuda' or 'cpu')
    
    Returns:
        List of tensors [x, y, (z), t] where t=0 everywhere
    """
    import torch
    
    coor = []
    
    if dimension == 1:
        x_ic = torch.empty(n_points, 1, device=device, dtype=torch.float32).uniform_(rmin[0], rmax[0]).requires_grad_()
        t_ic = torch.zeros(n_points, 1, device=device, dtype=torch.float32).requires_grad_()
        coor = [x_ic, t_ic]
    
    elif dimension == 2:
        x_ic = torch.empty(n_points, 1, device=device, dtype=torch.float32).uniform_(rmin[0], rmax[0]).requires_grad_()
        y_ic = torch.empty(n_points, 1, device=device, dtype=torch.float32).uniform_(rmin[1], rmax[1]).requires_grad_()
        t_ic = torch.zeros(n_points, 1, device=device, dtype=torch.float32).requires_grad_()
        coor = [x_ic, y_ic, t_ic]
    
    elif dimension == 3:
        x_ic = torch.empty(n_points, 1, device=device, dtype=torch.float32).uniform_(rmin[0], rmax[0]).requires_grad_()
        y_ic = torch.empty(n_points, 1, device=device, dtype=torch.float32).uniform_(rmin[1], rmax[1]).requires_grad_()
        z_ic = torch.empty(n_points, 1, device=device, dtype=torch.float32).uniform_(rmin[2], rmax[2]).requires_grad_()
        t_ic = torch.zeros(n_points, 1, device=device, dtype=torch.float32).requires_grad_()
        coor = [x_ic, y_ic, z_ic, t_ic]
    
    return coor
_register_module('core.data_generator', ['col_gen', 'diff', 'distribute_collocation_points', 'generate_poisson_ic_points', 'input_taker', 'req_consts_calc'])

# ==== Module: numerical_solvers.LAX (numerical_solvers/LAX.py) ====
import numpy as np
import os
import matplotlib.pyplot as plt
import scipy
from dataclasses import dataclass
from typing import Optional

## For the FFT solver

from numpy.fft import fft, ifft, fft2, ifft2, fftn, ifftn

from config import RANDOM_SEED, DIMENSION

# Import wave vector components and physical constants for 2D sinusoidal perturbations
try:
    from config import KX, KY, KZ, cs, rho_o, const, G
except ImportError:
    # Fallback if config not available
    KX = 2*np.pi/7.0  # Default wavelength
    KY = 0.0
    KZ = 0.0
    cs = 1.0
    rho_o = 1.0
    const = 1.0
    G = 1.0

np.random.seed(RANDOM_SEED)
#tf.random.set_seed(1234)


@dataclass
class DomainParams:
    """Domain parameters for the simulation."""
    Lx: float  # Domain size in x direction
    Ly: float  # Domain size in y direction
    nx: int  # Number of grid points in x direction
    ny: int  # Number of grid points in y direction
    Lz: Optional[float] = None  # Domain size in z direction (for 3D)
    nz: Optional[int] = None  # Number of grid points in z direction (for 3D)
    
    @property
    def dx(self) -> float:
        """Grid spacing in x direction."""
        return self.Lx / self.nx
    
    @property
    def dy(self) -> float:
        """Grid spacing in y direction."""
        return self.Ly / self.ny
    
    @property
    def dz(self) -> Optional[float]:
        """Grid spacing in z direction (for 3D)."""
        return self.Lz / self.nz if self.Lz is not None and self.nz is not None else None
    
    @property
    def is_3d(self) -> bool:
        """Check if this is a 3D domain."""
        return self.Lz is not None and self.nz is not None


@dataclass
class SimulationParams:
    """Simulation parameters for the LAX solver."""
    time: float  # Simulation time
    N: int  # Grid resolution (Nx = Ny = N)
    nu: float  # Courant number
    lam: float  # Wavelength
    num_of_waves: int  # Number of waves in domain
    rho_1: float  # Density perturbation amplitude
    gravity: bool = False  # Whether to include self-gravity
    isplot: Optional[bool] = None  # Whether to plot results
    comparison: Optional[bool] = None  # Whether to compare with linear theory
    animation: Optional[bool] = None  # Whether to animate
    use_velocity_ps: bool = False  # Whether to use power spectrum velocity initialization
    ps_index: float = -3.0  # Power spectrum index
    vel_rms: float = 0.02  # RMS velocity amplitude
    random_seed: Optional[int] = None  # Random seed for reproducibility
    vx0_shared: Optional[np.ndarray] = None  # Shared initial x-velocity field
    vy0_shared: Optional[np.ndarray] = None  # Shared initial y-velocity field


@dataclass
class SimulationResult:
    """Result container for LAX solver simulations."""
    dimension: int  # 1, 2, or 3
    density: np.ndarray  # Density field
    velocity_components: list  # List of velocity arrays [vx, vy, ...] or [vx, vy, vz]
    coordinates: dict  # Dict with 'x', 'y' (2D/3D), 'z' (3D) coordinate arrays
    metadata: dict  # Dict with time, iterations, rho_max, etc.
    potential: Optional[np.ndarray] = None  # Gravitational potential (if gravity=True)
    linear_theory_comparison: Optional[dict] = None  # Optional linear theory comparison data


# ============================================================================
# Helper Functions
# ============================================================================

def compute_jeans_length(c_s, rho_o, const, G):
    """
    Compute the Jeans length for gravitational instability.
    
    Args:
        c_s: Sound speed
        rho_o: Background density
        const: Constant factor
        G: Gravitational constant
    
    Returns:
        Jeans length
    """
    return np.sqrt(4 * np.pi**2 * c_s**2 / (const * G * rho_o))


def compute_perturbation_velocity(rho_1, rho_o, lam, c_s, jeans, gravity):
    """
    Compute the velocity perturbation amplitude v_1 and growth rate alpha.
    
    Args:
        rho_1: Density perturbation amplitude
        rho_o: Background density
        lam: Wavelength
        c_s: Sound speed
        jeans: Jeans length
        gravity: Whether gravity is enabled
    
    Returns:
        tuple: (v_1, alpha, is_unstable)
            v_1: Velocity perturbation amplitude
            alpha: Growth/oscillation rate (positive for instability, real for oscillation)
            is_unstable: True if gravitational instability (lam >= jeans), False otherwise
    """
    if not gravity:
        # No gravity: sound wave
        v_1 = (c_s * rho_1) / rho_o
        return v_1, None, False
    
    # With gravity: check if unstable
    if lam >= jeans:
        # Gravitational instability
        alpha = np.sqrt(const * G * rho_o - c_s**2 * (2 * np.pi / lam)**2)
        v_1 = (rho_1 / rho_o) * (alpha / (2 * np.pi / lam))
        return v_1, alpha, True
    else:
        # Oscillatory regime
        alpha = np.sqrt(c_s**2 * (2 * np.pi / lam)**2 - const * G * rho_o)
        v_1 = (rho_1 / rho_o) * (alpha / (2 * np.pi / lam))
        return v_1, alpha, False


def build_k_space_grid(shape, domain_lengths, dimension):
    """
    Build k-space grid for FFT operations.
    
    Args:
        shape: Tuple of grid dimensions (nx, ny) or (nx, ny, nz)
        domain_lengths: Tuple of domain sizes (Lx, Ly) or (Lx, Ly, Lz)
        dimension: 2 or 3
    
    Returns:
        tuple: (kx, ky, kz (optional), kx_mesh, ky_mesh, kz_mesh (optional))
            For 2D: (kx, ky, kx_mesh, ky_mesh)
            For 3D: (kx, ky, kz, kx_mesh, ky_mesh, kz_mesh)
    """
    if dimension == 2:
        nx, ny = shape
        Lx, Ly = domain_lengths
        dx = Lx / nx
        dy = Ly / ny
        kx = 2 * np.pi * np.fft.fftfreq(nx, d=dx)
        ky = 2 * np.pi * np.fft.fftfreq(ny, d=dy)
        kx_mesh, ky_mesh = np.meshgrid(kx, ky, indexing='ij')
        return kx, ky, kx_mesh, ky_mesh
    elif dimension == 3:
        nx, ny, nz = shape
        Lx, Ly, Lz = domain_lengths
        dx = Lx / nx
        dy = Ly / ny
        dz = Lz / nz
        kx = 2 * np.pi * np.fft.fftfreq(nx, d=dx)
        ky = 2 * np.pi * np.fft.fftfreq(ny, d=dy)
        kz = 2 * np.pi * np.fft.fftfreq(nz, d=dz)
        kx_mesh, ky_mesh, kz_mesh = np.meshgrid(kx, ky, kz, indexing='ij')
        return kx, ky, kz, kx_mesh, ky_mesh, kz_mesh
    else:
        raise ValueError(f"Unsupported dimension={dimension}. Use 2 or 3.")


def compute_adaptive_timestep(velocities, c_s, dx, nu, include_sound_speed=False):
    """
    Compute adaptive timestep based on CFL condition.
    
    Args:
        velocities: List of velocity arrays [vx, vy, ...] or single velocity array, or None
        c_s: Sound speed
        dx: Grid spacing
        nu: Courant number
        include_sound_speed: If True, include c_s in the max velocity calculation (for initial timestep)
    
    Returns:
        Adaptive timestep
    """
    if include_sound_speed:
        if velocities is None:
            vmax = c_s
        elif isinstance(velocities, (list, tuple)):
            vmax = max(max(np.max(np.abs(v)) for v in velocities), c_s)
        else:
            vmax = max(np.max(np.abs(velocities)), c_s)
        return nu * dx / vmax
    else:
        if velocities is None:
            vmax = c_s
        elif isinstance(velocities, (list, tuple)):
            vmax = max(np.max(np.abs(v)) for v in velocities)
        else:
            vmax = np.max(np.abs(velocities))
        
        dt1 = nu * dx / vmax if vmax > 1e-9 else float('inf')
        dt2 = nu * dx / c_s
        return min(dt1, dt2)


def compute_lax_density_update(rho, velocities, dt, grid_spacings, dimension):
    """
    Compute LAX density update using dimension-agnostic approach.
    
    This function implements the LAX method for updating density in the continuity equation:
    ∂ρ/∂t + ∇·(ρv) = 0
    
    The update uses a dimension-agnostic approach that works for 1D, 2D, and 3D without
    if-else chains by iterating over dimensions.
    
    Args:
        rho: Current density array (1D, 2D, or 3D numpy array)
        velocities: List of velocity arrays [vx, vy, ...] or [vx, vy, vz] for 3D
                   Length must match dimension
        dt: Time step
        grid_spacings: List of grid spacings [dx, dy, ...] or [dx, dy, dz] for 3D
                       Length must match dimension
        dimension: Integer dimension (1, 2, or 3)
    
    Returns:
        Updated density array (same shape as input rho)
    
    Examples:
        # 1D
        rho_new = compute_lax_density_update(rho, [vx], dt, [dx], 1)
        
        # 2D
        rho_new = compute_lax_density_update(rho, [vx, vy], dt, [dx, dy], 2)
        
        # 3D
        rho_new = compute_lax_density_update(rho, [vx, vy, vz], dt, [dx, dy, dz], 3)
    """
    if len(velocities) != dimension:
        raise ValueError(f"Number of velocity arrays ({len(velocities)}) must match dimension ({dimension})")
    if len(grid_spacings) != dimension:
        raise ValueError(f"Number of grid spacings ({len(grid_spacings)}) must match dimension ({dimension})")
    
    # Compute mu values for each dimension: mu_i = dt / (2 * dx_i)
    mu_values = [dt / (2 * dx) for dx in grid_spacings]
    
    # Initialize with averaging term: (1/(2*dimension)) * sum of all rolled neighbors
    # For each dimension, we add contributions from both +1 and -1 rolls
    rho_new = np.zeros_like(rho)
    
    # Sum all rolled neighbors (forward and backward for each dimension)
    for axis in range(dimension):
        rho_new += np.roll(rho, -1, axis=axis)  # Forward roll
        rho_new += np.roll(rho, 1, axis=axis)   # Backward roll
    
    # Normalize by 1/(2*dimension)
    rho_new = rho_new / (2 * dimension)
    
    # Subtract flux terms for each dimension: -mu_i * (flux_forward - flux_backward)
    # where flux = rho * velocity
    for axis in range(dimension):
        mu = mu_values[axis]
        vel = velocities[axis]
        
        # Forward flux: rho_rolled_forward * vel_rolled_forward
        rho_forward = np.roll(rho, -1, axis=axis)
        vel_forward = np.roll(vel, -1, axis=axis)
        flux_forward = rho_forward * vel_forward
        
        # Backward flux: rho_rolled_backward * vel_rolled_backward
        rho_backward = np.roll(rho, 1, axis=axis)
        vel_backward = np.roll(vel, 1, axis=axis)
        flux_backward = rho_backward * vel_backward
        
        # Subtract the flux difference
        rho_new -= mu * (flux_forward - flux_backward)
    
    return rho_new


def compute_lax_momentum_update(momenta, velocities, rho, phi, c_s, dt, grid_spacings, dimension, gravity=True):
    """
    Compute LAX momentum update using dimension-agnostic approach.
    
    This function implements the LAX method for updating momentum in the momentum equation:
    ∂(ρv)/∂t + ∇·(ρv⊗v) = -∇P - ρ∇φ
    
    The update uses a dimension-agnostic approach that works for 1D, 2D, and 3D without
    if-else chains by iterating over dimensions.
    
    Args:
        momenta: List of momentum arrays [Px, Py, ...] or [Px, Py, Pz] for 3D
                 Length must match dimension
        velocities: List of velocity arrays [vx, vy, ...] or [vx, vy, vz] for 3D
                   Length must match dimension
        rho: Current density array (same shape as momentum arrays)
        phi: Gravitational potential array (same shape as momentum arrays)
             Only used if gravity=True
        c_s: Sound speed
        dt: Time step
        grid_spacings: List of grid spacings [dx, dy, ...] or [dx, dy, dz] for 3D
                       Length must match dimension
        dimension: Integer dimension (1, 2, or 3)
        gravity: Whether to include gravity term (default: True)
    
    Returns:
        List of updated momentum arrays (same shape as input momenta)
    
    Examples:
        # 1D
        P_new = compute_lax_momentum_update([Px], [vx], rho, phi, c_s, dt, [dx], 1, gravity=True)
        
        # 2D
        Px_new, Py_new = compute_lax_momentum_update([Px, Py], [vx, vy], rho, phi, c_s, dt, [dx, dy], 2, gravity=True)
        
        # 3D
        Px_new, Py_new, Pz_new = compute_lax_momentum_update([Px, Py, Pz], [vx, vy, vz], rho, phi, c_s, dt, [dx, dy, dz], 3, gravity=True)
    """
    if len(momenta) != dimension:
        raise ValueError(f"Number of momentum arrays ({len(momenta)}) must match dimension ({dimension})")
    if len(velocities) != dimension:
        raise ValueError(f"Number of velocity arrays ({len(velocities)}) must match dimension ({dimension})")
    if len(grid_spacings) != dimension:
        raise ValueError(f"Number of grid spacings ({len(grid_spacings)}) must match dimension ({dimension})")
    
    # Compute mu values for each dimension: mu_i = dt / (2 * dx_i)
    mu_values = [dt / (2 * dx) for dx in grid_spacings]
    
    # Initialize list to store updated momenta
    momenta_new = []
    
    # Update each momentum component
    for comp_dim in range(dimension):
        P = momenta[comp_dim]
        mu_comp = mu_values[comp_dim]  # mu for this component's direction
        
        # Initialize with averaging term: (1/(2*dimension)) * sum of all rolled neighbors
        P_new = np.zeros_like(P)
        
        # Sum all rolled neighbors (forward and backward for each dimension)
        for axis in range(dimension):
            P_new += np.roll(P, -1, axis=axis)  # Forward roll
            P_new += np.roll(P, 1, axis=axis)   # Backward roll
        
        # Normalize by 1/(2*dimension)
        P_new = P_new / (2 * dimension)
        
        # Advection terms: For each dimension, subtract mu_i * (flux_forward - flux_backward)
        # where flux = momentum * velocity
        # Each momentum component is advected by ALL velocity components
        for axis in range(dimension):
            mu = mu_values[axis]
            vel = velocities[axis]
            
            # Forward flux: P_rolled_forward * vel_rolled_forward
            P_forward = np.roll(P, -1, axis=axis)
            vel_forward = np.roll(vel, -1, axis=axis)
            flux_forward = P_forward * vel_forward
            
            # Backward flux: P_rolled_backward * vel_rolled_backward
            P_backward = np.roll(P, 1, axis=axis)
            vel_backward = np.roll(vel, 1, axis=axis)
            flux_backward = P_backward * vel_backward
            
            # Subtract the flux difference
            P_new -= mu * (flux_forward - flux_backward)
        
        # Pressure gradient term: Only in the component's own direction
        # -c_s^2 * mu_comp * (rho_forward - rho_backward) in direction comp_dim
        rho_forward = np.roll(rho, -1, axis=comp_dim)
        rho_backward = np.roll(rho, 1, axis=comp_dim)
        P_new -= (c_s**2) * mu_comp * (rho_forward - rho_backward)
        
        # Gravity term: Only in the component's own direction (if enabled)
        # -mu_comp * rho * (phi_forward - phi_backward) in direction comp_dim
        if gravity:
            phi_forward = np.roll(phi, -1, axis=comp_dim)
            phi_backward = np.roll(phi, 1, axis=comp_dim)
            P_new -= mu_comp * rho * (phi_forward - phi_backward)
        
        momenta_new.append(P_new)
    
    return momenta_new


def lax_time_step(state_dict, dt, params_dict, dimension, gravity=True):
    """
    Perform a single LAX time step, updating all state variables.
    
    This function orchestrates a complete time step by:
    1. Updating density using compute_lax_density_update()
    2. Updating momenta using compute_lax_momentum_update()
    3. Computing velocities from updated momenta and density
    4. Updating gravitational potential if gravity is enabled
    
    Args:
        state_dict: Dictionary containing current state with keys:
                   - 'rho': density array
                   - 'velocities': list of velocity arrays [vx, vy, ...] or [vx, vy, vz]
                   - 'momenta': list of momentum arrays [Px, Py, ...] or [Px, Py, Pz]
                   - 'phi': gravitational potential array (used if gravity=True)
        dt: Time step
        params_dict: Dictionary containing simulation parameters with keys:
                    - 'Lx': Domain size in x direction
                    - 'Ly': Domain size in y direction (required for 2D/3D)
                    - 'Lz': Domain size in z direction (required for 3D)
                    - 'nx': Number of grid points in x direction
                    - 'ny': Number of grid points in y direction (required for 2D/3D)
                    - 'nz': Number of grid points in z direction (required for 3D)
                    - 'c_s': Sound speed
                    - 'rho_o': Background density (for Poisson solver)
                    - 'const': Constant factor (for Poisson solver)
        dimension: Integer dimension (1, 2, or 3)
        gravity: Whether to include gravity (default: True)
    
    Returns:
        Dictionary with updated state, same structure as input state_dict
    
    Examples:
        # 2D example
        state = {
            'rho': rho0,
            'velocities': [vx0, vy0],
            'momenta': [Px0, Py0],
            'phi': phi0
        }
        params = {
            'Lx': 10.0, 'Ly': 10.0,
            'nx': 100, 'ny': 100,
            'c_s': 1.0, 'rho_o': 1.0, 'const': 1.0
        }
        new_state = lax_time_step(state, dt, params, dimension=2, gravity=True)
    """
    # Extract state variables
    rho = state_dict['rho']
    velocities = state_dict['velocities']
    momenta = state_dict['momenta']
    phi = state_dict.get('phi', None)
    
    # Extract parameters
    Lx = params_dict['Lx']
    nx = params_dict['nx']
    c_s = params_dict['c_s']
    rho_o = params_dict['rho_o']
    const = params_dict['const']
    
    # Extract dimension-specific parameters
    if dimension >= 2:
        Ly = params_dict['Ly']
        ny = params_dict['ny']
    if dimension == 3:
        Lz = params_dict['Lz']
        nz = params_dict['nz']
    
    # Compute grid spacings
    grid_spacings = [Lx / nx]
    if dimension >= 2:
        grid_spacings.append(Ly / ny)
    if dimension == 3:
        grid_spacings.append(Lz / nz)
    
    # Step 1: Update density
    rho_new = compute_lax_density_update(rho, velocities, dt, grid_spacings, dimension)
    
    # Step 2: Update momenta
    momenta_new = compute_lax_momentum_update(
        momenta, velocities, rho, phi, c_s, dt, grid_spacings, dimension, gravity=gravity
    )
    
    # Step 3: Compute velocities from updated momenta and density
    velocities_new = [momenta_new[i] / rho_new for i in range(dimension)]
    
    # Step 4: Update gravitational potential if gravity is enabled
    if gravity:
        # Call fft_poisson_solver with appropriate parameters based on dimension
        if dimension == 1:
            # 1D Poisson solver (not implemented in fft_poisson_solver, but we can handle it)
            # For now, raise an error or use a simple implementation
            # Note: fft_poisson_solver doesn't support 1D, so we'll skip it for 1D
            # In practice, 1D gravity might use a different approach
            phi_new = np.zeros_like(rho_new)  # 1D gravity not typically used with this solver
        elif dimension == 2:
            phi_new = fft_poisson_solver(const * (rho_new - rho_o), Lx, nx, Ly=Ly, ny=ny)
        else:  # dimension == 3
            phi_new = fft_poisson_solver(const * (rho_new - rho_o), Lx, nx, Ly=Ly, ny=ny, Lz=Lz, nz=nz)
    else:
        # If no gravity, keep phi as zeros or copy existing if provided
        phi_new = phi.copy() if phi is not None else np.zeros_like(rho_new)
    
    # Return new state dictionary
    return {
        'rho': rho_new,
        'velocities': velocities_new,
        'momenta': momenta_new,
        'phi': phi_new
    }


def initialize_arrays(shape, dimension):
    """
    Initialize arrays for LAX solver.
    
    Args:
        shape: Tuple of grid dimensions (nx, ny) or (nx, ny, nz)
        dimension: 2 or 3
    
    Returns:
        dict: {
            'rho': density array,
            'velocities': [vx, vy, ...] list of velocity arrays,
            'momenta': [Px, Py, ...] list of momentum arrays,
            'phi': potential array
        }
    """
    if dimension == 2:
        nx, ny = shape
        arrays = {
            'rho': np.zeros((nx, ny)),
            'velocities': [np.zeros((nx, ny)), np.zeros((nx, ny))],  # vx, vy
            'momenta': [np.zeros((nx, ny)), np.zeros((nx, ny))],  # Px, Py
            'phi': np.zeros((nx, ny))
        }
    elif dimension == 3:
        nx, ny, nz = shape
        arrays = {
            'rho': np.zeros((nx, ny, nz)),
            'velocities': [np.zeros((nx, ny, nz)), np.zeros((nx, ny, nz)), np.zeros((nx, ny, nz))],  # vx, vy, vz
            'momenta': [np.zeros((nx, ny, nz)), np.zeros((nx, ny, nz)), np.zeros((nx, ny, nz))],  # Px, Py, Pz
            'phi': np.zeros((nx, ny, nz))
        }
    else:
        raise ValueError(f"Unsupported dimension={dimension}. Use 2 or 3.")
    
    return arrays


# ============================================================================
# Initial Conditions Setup Functions
# ============================================================================

def setup_sinusoidal_ic(domain_params, physics_params, dimension, xx=None, yy=None, zz=None):
    """
    Set up sinusoidal initial conditions with KX, KY, KZ wave patterns.
    
    Args:
        domain_params: DomainParams dataclass or dict with Lx, Ly, (Lz), nx, ny, (nz)
        physics_params: Dict with rho_o, rho_1, lam, c_s, gravity, jeans, KX, KY, (KZ)
        dimension: 2 or 3
        xx, yy, zz: Coordinate meshes (optional, will be created if not provided)
    
    Returns:
        dict: {
            'rho': density field,
            'vx': x-velocity field,
            'vy': y-velocity field,
            'vz': z-velocity field (3D only),
            'v_1': velocity perturbation amplitude,
            'alpha': growth/oscillation rate (if gravity),
            'is_unstable': bool (if gravity)
        }
    """
    # Extract parameters
    if isinstance(domain_params, dict):
        Lx, Ly = domain_params['Lx'], domain_params['Ly']
        nx, ny = domain_params['nx'], domain_params['ny']
        if dimension == 3:
            Lz, nz = domain_params['Lz'], domain_params['nz']
    else:
        Lx, Ly = domain_params.Lx, domain_params.Ly
        nx, ny = domain_params.nx, domain_params.ny
        if dimension == 3:
            Lz, nz = domain_params.Lz, domain_params.nz
    
    rho_o = physics_params['rho_o']
    rho_1 = physics_params['rho_1']
    lam = physics_params['lam']
    c_s = physics_params['c_s']
    gravity = physics_params.get('gravity', False)
    jeans = physics_params.get('jeans', None)
    KX = physics_params.get('KX', 2*np.pi/lam)
    KY = physics_params.get('KY', 0.0)
    KZ = physics_params.get('KZ', 0.0) if dimension == 3 else 0.0
    
    # Create coordinate meshes if not provided
    if xx is None:
        x = np.linspace(0, Lx, nx, endpoint=False)
        y = np.linspace(0, Ly, ny, endpoint=False)
        if dimension == 2:
            xx, yy = np.meshgrid(x, y, indexing='ij')
        else:
            z = np.linspace(0, Lz, nz, endpoint=False)
            xx, yy, zz = np.meshgrid(x, y, z, indexing='ij')
    
    # Set up density
    if dimension == 2:
        rho = rho_o + rho_1 * np.cos(KX * xx + KY * yy)
    else:
        rho = rho_o + rho_1 * np.cos(KX * xx + KY * yy + KZ * zz)
    
    # Compute velocity perturbation
    v_1, alpha, is_unstable = compute_perturbation_velocity(rho_1, rho_o, lam, c_s, jeans, gravity)
    
    # Set up velocity fields
    k_magnitude = np.sqrt(KX**2 + KY**2 + (KZ**2 if dimension == 3 else 0))
    
    if dimension == 2:
        if gravity and is_unstable:
            # Unstable: use sin
            wave_field = -v_1 * np.sin(KX * xx + KY * yy)
        else:
            # Stable or no gravity: use cos
            wave_field = v_1 * np.cos(KX * xx + KY * yy)
        
        if k_magnitude > 0:
            vx = wave_field * (KX / k_magnitude)
            vy = wave_field * (KY / k_magnitude)
        else:
            vx = wave_field
            vy = np.zeros_like(xx)
        
        result = {
            'rho': rho,
            'vx': vx,
            'vy': vy,
            'v_1': v_1,
            'alpha': alpha,
            'is_unstable': is_unstable
        }
    else:  # dimension == 3
        if gravity and is_unstable:
            wave_field = -v_1 * np.sin(KX * xx + KY * yy + KZ * zz)
        else:
            wave_field = v_1 * np.cos(KX * xx + KY * yy + KZ * zz)
        
        if k_magnitude > 0:
            vx = wave_field * (KX / k_magnitude)
            vy = wave_field * (KY / k_magnitude)
            vz = wave_field * (KZ / k_magnitude)
        else:
            vx = wave_field
            vy = np.zeros_like(vx)
            vz = np.zeros_like(vx)
        
        result = {
            'rho': rho,
            'vx': vx,
            'vy': vy,
            'vz': vz,
            'v_1': v_1,
            'alpha': alpha,
            'is_unstable': is_unstable
        }
    
    return result


def setup_power_spectrum_ic(domain_params, ps_params, dimension, vx0_shared=None, vy0_shared=None, vz0_shared=None):
    """
    Set up power spectrum initial conditions.
    
    Args:
        domain_params: DomainParams dataclass or dict with Lx, Ly, (Lz), nx, ny, (nz)
        ps_params: Dict with rho_o, power_index, amplitude, random_seed
        dimension: 2 or 3
        vx0_shared, vy0_shared, vz0_shared: Optional pre-generated velocity fields
    
    Returns:
        dict: {
            'rho': density field (uniform),
            'vx': x-velocity field,
            'vy': y-velocity field,
            'vz': z-velocity field (3D only)
        }
    """
    # Validate dimension
    if dimension not in [2, 3]:
        raise ValueError(f"setup_power_spectrum_ic() only supports dimension=2 or 3, got {dimension}")
    
    # Extract parameters
    if isinstance(domain_params, dict):
        Lx, Ly = domain_params['Lx'], domain_params['Ly']
        nx, ny = domain_params['nx'], domain_params['ny']
        if dimension == 3:
            if 'Lz' not in domain_params or 'nz' not in domain_params:
                raise ValueError("For dimension=3, domain_params must contain 'Lz' and 'nz'")
            Lz, nz = domain_params['Lz'], domain_params['nz']
    else:
        Lx, Ly = domain_params.Lx, domain_params.Ly
        nx, ny = domain_params.nx, domain_params.ny
        if dimension == 3:
            if not hasattr(domain_params, 'Lz') or not hasattr(domain_params, 'nz'):
                raise ValueError("For dimension=3, domain_params must have 'Lz' and 'nz' attributes")
            Lz, nz = domain_params.Lz, domain_params.nz
    
    rho_o = ps_params['rho_o']
    power_index = ps_params.get('power_index', -3.0)
    amplitude = ps_params.get('amplitude', 0.02)
    random_seed = ps_params.get('random_seed', None)
    
    # Uniform density
    if dimension == 2:
        rho = rho_o * np.ones((nx, ny))
    else:
        rho = rho_o * np.ones((nx, ny, nz))
    
    # Generate or use shared velocity fields
    if vx0_shared is not None and vy0_shared is not None:
        if dimension == 2:
            vx = vx0_shared.copy()
            vy = vy0_shared.copy()
            return {'rho': rho, 'vx': vx, 'vy': vy}
        else:
            if vz0_shared is not None:
                vx = vx0_shared.copy()
                vy = vy0_shared.copy()
                vz = vz0_shared.copy()
                return {'rho': rho, 'vx': vx, 'vy': vy, 'vz': vz}
    
    # Generate new velocity fields
    if dimension == 2:
        vx, vy = generate_velocity_field_power_spectrum(nx, ny, Lx, Ly, power_index, amplitude, DIMENSION=2, random_seed=random_seed)
        return {'rho': rho, 'vx': vx, 'vy': vy}
    else:
        vx, vy, vz = generate_velocity_field_power_spectrum(nx, ny, Lx, Ly, power_index, amplitude, DIMENSION=3, random_seed=random_seed, nz=nz, Lz=Lz)
        return {'rho': rho, 'vx': vx, 'vy': vy, 'vz': vz}


def setup_warm_start_ic(provided_fields):
    """
    Set up initial conditions from provided fields (for PINN integration).
    
    Args:
        provided_fields: Dict with 'rho', 'vx', 'vy', ('vz' for 3D)
    
    Returns:
        dict: Same structure as input, with arrays copied
    """
    result = {}
    for key, value in provided_fields.items():
        if value is not None:
            result[key] = value.copy() if hasattr(value, 'copy') else value
        else:
            result[key] = None
    
    return result


def lax_solver(time, domain_params, physics_params, ic_type='sinusoidal', ic_params=None, options=None):
    """
    Unified master solver for 1D, 2D, and 3D LAX method.
    
    This function provides a single entry point for running LAX simulations with different
    initial condition types and options. It automatically determines the dimension from
    domain_params and handles all setup and time-stepping internally.
    
    Args:
        time: Final simulation time (must be > 0)
        domain_params: DomainParams dataclass or dict with:
            - Lx, Ly, (Lz for 3D): Domain sizes
            - nx, ny, (nz for 3D): Grid resolutions
        physics_params: Dict with:
            - c_s: Sound speed
            - rho_o: Background density
            - const: Constant factor (for Poisson solver)
            - G: Gravitational constant
            - rho_1: Density perturbation amplitude (for sinusoidal IC)
            - lam: Wavelength (for sinusoidal IC)
        ic_type: 'sinusoidal' | 'power_spectrum' | 'warm_start'
        ic_params: Dict with IC-specific parameters:
            For 'sinusoidal': KX, KY, (KZ for 3D) - optional, defaults from config
            For 'power_spectrum': power_index, amplitude, random_seed, vx0_shared, vy0_shared, (vz0_shared for 3D)
            For 'warm_start': provided_fields dict with 'rho', 'vx', 'vy', ('vz' for 3D)
        options: Dict with:
            - gravity: bool (default: True)
            - nu: Courant number (default: 0.5)
            - comparison: bool (default: False)
            - isplot: bool (default: False)
    
    Returns:
        SimulationResult: Dataclass containing simulation results
    
    Raises:
        ValueError: If input validation fails
    """
    # ========================================================================
    # Input Validation
    # ========================================================================
    
    # Validate time
    if not isinstance(time, (int, float)) or time < 0:
        raise ValueError(f"time must be a non-negative number, got {time}")
    
    # Validate ic_type
    allowed_ic_types = ['sinusoidal', 'power_spectrum', 'warm_start']
    if ic_type not in allowed_ic_types:
        raise ValueError(f"ic_type must be one of {allowed_ic_types}, got '{ic_type}'")
    
    # Validate domain_params
    if isinstance(domain_params, dict):
        required_domain_keys = ['Lx', 'nx']
        for key in required_domain_keys:
            if key not in domain_params:
                raise ValueError(f"domain_params missing required key: '{key}'")
        
        Lx = domain_params['Lx']
        nx = domain_params['nx']
        Ly = domain_params.get('Ly', None)
        ny = domain_params.get('ny', None)
        Lz = domain_params.get('Lz', None)
        nz = domain_params.get('nz', None)
    else:
        if not hasattr(domain_params, 'Lx') or not hasattr(domain_params, 'nx'):
            raise ValueError("domain_params must have Lx and nx attributes")
        Lx = domain_params.Lx
        nx = domain_params.nx
        Ly = getattr(domain_params, 'Ly', None)
        ny = getattr(domain_params, 'ny', None)
        Lz = getattr(domain_params, 'Lz', None)
        nz = getattr(domain_params, 'nz', None)
    
    # Validate domain parameter values
    if not isinstance(Lx, (int, float)) or Lx <= 0:
        raise ValueError(f"Lx must be a positive number, got {Lx}")
    if not isinstance(nx, int) or nx <= 0:
        raise ValueError(f"nx must be a positive integer, got {nx}")
    
    # Determine dimension and validate dimension-specific parameters
    if Lz is not None and nz is not None:
        dimension = 3
        if Ly is None or ny is None:
            raise ValueError("For 3D simulations, Ly and ny must be provided")
        if not isinstance(Ly, (int, float)) or Ly <= 0:
            raise ValueError(f"Ly must be a positive number, got {Ly}")
        if not isinstance(ny, int) or ny <= 0:
            raise ValueError(f"ny must be a positive integer, got {ny}")
        if not isinstance(Lz, (int, float)) or Lz <= 0:
            raise ValueError(f"Lz must be a positive number, got {Lz}")
        if not isinstance(nz, int) or nz <= 0:
            raise ValueError(f"nz must be a positive integer, got {nz}")
    elif Ly is not None and ny is not None:
        dimension = 2
        if not isinstance(Ly, (int, float)) or Ly <= 0:
            raise ValueError(f"Ly must be a positive number, got {Ly}")
        if not isinstance(ny, int) or ny <= 0:
            raise ValueError(f"ny must be a positive integer, got {ny}")
    else:
        dimension = 1
    
    # Validate physics_params
    if not isinstance(physics_params, dict):
        raise ValueError(f"physics_params must be a dict, got {type(physics_params)}")
    
    required_physics_keys = ['c_s', 'rho_o', 'const', 'G']
    for key in required_physics_keys:
        if key not in physics_params:
            raise ValueError(f"physics_params missing required key: '{key}'")
        if not isinstance(physics_params[key], (int, float)) or physics_params[key] <= 0:
            raise ValueError(f"physics_params['{key}'] must be a positive number, got {physics_params[key]}")
    
    c_s = physics_params['c_s']
    rho_o = physics_params['rho_o']
    const = physics_params['const']
    G = physics_params['G']
    
    # Validate ic_params for warm_start
    if ic_type == 'warm_start':
        if ic_params is None:
            raise ValueError("ic_params must be provided for warm_start ic_type")
        if 'provided_fields' not in ic_params:
            raise ValueError("ic_params must contain 'provided_fields' for warm_start ic_type")
        provided_fields = ic_params['provided_fields']
        if not isinstance(provided_fields, dict):
            raise ValueError("ic_params['provided_fields'] must be a dict")
        if 'rho' not in provided_fields:
            raise ValueError("provided_fields must contain 'rho'")
        if dimension >= 2 and 'vy' not in provided_fields:
            raise ValueError(f"provided_fields must contain 'vy' for {dimension}D simulations")
        if dimension == 3 and 'vz' not in provided_fields:
            raise ValueError("provided_fields must contain 'vz' for 3D simulations")
    
    # Validate options
    if options is not None and not isinstance(options, dict):
        raise ValueError(f"options must be a dict or None, got {type(options)}")
    
    # Parse options
    if options is None:
        options = {}
    gravity = options.get('gravity', True)
    nu = options.get('nu', 0.5)
    comparison = options.get('comparison', False)
    isplot = options.get('isplot', False)
    
    # Validate option values
    if not isinstance(gravity, bool):
        raise ValueError(f"options['gravity'] must be a bool, got {type(gravity)}")
    if not isinstance(nu, (int, float)) or nu <= 0 or nu > 1:
        raise ValueError(f"options['nu'] (Courant number) must be in (0, 1], got {nu}")
    if not isinstance(comparison, bool):
        raise ValueError(f"options['comparison'] must be a bool, got {type(comparison)}")
    if not isinstance(isplot, bool):
        raise ValueError(f"options['isplot'] must be a bool, got {type(isplot)}")
    
    # Compute grid spacings
    dx = Lx / nx
    grid_spacings = [dx]
    if dimension >= 2:
        dy = Ly / ny
        grid_spacings.append(dy)
    if dimension == 3:
        dz = Lz / nz
        grid_spacings.append(dz)
    
    # Compute Jeans length if gravity is enabled
    jeans = None
    if gravity:
        jeans = compute_jeans_length(c_s, rho_o, const, G)
    
    # Create coordinate arrays first
    x = np.linspace(0, Lx, nx, endpoint=False)
    if dimension >= 2:
        y = np.linspace(0, Ly, ny, endpoint=False)
    if dimension == 3:
        z = np.linspace(0, Lz, nz, endpoint=False)
    
    # Set up initial conditions based on ic_type
    if ic_type == 'sinusoidal':
        rho_1 = physics_params.get('rho_1', 0.1)
        lam = physics_params.get('lam', 7.0)
        
        if dimension == 1:
            # 1D sinusoidal IC
            rho0 = rho_o + rho_1 * np.cos(2*np.pi*x/lam)
            v_1, alpha, is_unstable = compute_perturbation_velocity(rho_1, rho_o, lam, c_s, jeans, gravity)
            if gravity and is_unstable:
                vx0 = -v_1 * np.sin(2*np.pi*x/lam)
            else:
                vx0 = v_1 * np.cos(2*np.pi*x/lam)
            velocities = [vx0]
        else:
            # 2D/3D sinusoidal IC
            setup_physics = {
                'rho_o': rho_o,
                'rho_1': rho_1,
                'lam': lam,
                'c_s': c_s,
                'gravity': gravity,
                'jeans': jeans
            }
            if ic_params:
                setup_physics.update(ic_params)
            else:
                # Use defaults from config
                setup_physics['KX'] = KX
                setup_physics['KY'] = KY
                if dimension == 3:
                    setup_physics['KZ'] = KZ
            
            ic_result = setup_sinusoidal_ic(domain_params, setup_physics, dimension)
            rho0 = ic_result['rho']
            if dimension == 2:
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                velocities = [vx0, vy0]
            else:  # dimension == 3
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                vz0 = ic_result['vz']
                velocities = [vx0, vy0, vz0]
    
    elif ic_type == 'power_spectrum':
        if dimension == 1:
            # 1D power spectrum IC - uniform density, zero velocity for now
            # (1D power spectrum velocity generation not implemented)
            rho0 = rho_o * np.ones(nx)
            vx0 = np.zeros(nx)
            velocities = [vx0]
        else:
            # 2D/3D power spectrum IC
            ps_params = {
                'rho_o': rho_o,
                'power_index': ic_params.get('power_index', -3.0) if ic_params else -3.0,
                'amplitude': ic_params.get('amplitude', 0.02) if ic_params else 0.02,
                'random_seed': ic_params.get('random_seed', None) if ic_params else None
            }
            vx0_shared = ic_params.get('vx0_shared', None) if ic_params else None
            vy0_shared = ic_params.get('vy0_shared', None) if ic_params else None
            vz0_shared = ic_params.get('vz0_shared', None) if ic_params else None
            
            ic_result = setup_power_spectrum_ic(domain_params, ps_params, dimension, 
                                               vx0_shared, vy0_shared, vz0_shared)
            rho0 = ic_result['rho']
            if dimension == 2:
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                velocities = [vx0, vy0]
            else:  # dimension == 3
                # Validate that all 3D velocity components are present
                if 'vz' not in ic_result:
                    raise ValueError(f"setup_power_spectrum_ic() did not return 'vz' for dimension=3. "
                                   f"Result keys: {list(ic_result.keys())}")
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                vz0 = ic_result['vz']
                velocities = [vx0, vy0, vz0]
    
    elif ic_type == 'warm_start':
        ic_result = setup_warm_start_ic(ic_params['provided_fields'])
        rho0 = ic_result['rho']
        if dimension == 1:
            vx0 = ic_result.get('vx', np.zeros_like(rho0))
            velocities = [vx0]
        elif dimension == 2:
            vx0 = ic_result['vx']
            vy0 = ic_result['vy']
            velocities = [vx0, vy0]
        else:  # dimension == 3
            vx0 = ic_result['vx']
            vy0 = ic_result['vy']
            vz0 = ic_result['vz']
            velocities = [vx0, vy0, vz0]
    
    else:
        raise ValueError(f"Unsupported ic_type: {ic_type}. Use 'sinusoidal', 'power_spectrum', or 'warm_start'")
    
    # Initialize momenta
    momenta = [rho0 * v for v in velocities]
    
    # Initialize gravitational potential
    if gravity:
        if dimension == 1:
            # 1D Poisson solver
            k = 2*np.pi*np.fft.fftfreq(nx, d=dx)
            rhohat = np.fft.fft(const*(rho0 - rho_o))
            denom = -(k**2)
            denom[0] = 1.0
            phihat = rhohat / denom
            phihat[0] = 0.0
            phi0 = np.real(np.fft.ifft(phihat))
        elif dimension == 2:
            phi0 = fft_poisson_solver(const*(rho0 - rho_o), Lx, nx, Ly=Ly, ny=ny)
        else:  # dimension == 3
            phi0 = fft_poisson_solver(const*(rho0 - rho_o), Lx, nx, Ly=Ly, ny=ny, Lz=Lz, nz=nz)
    else:
        phi0 = np.zeros_like(rho0)
    
    
    # Prepare params_dict for lax_time_step
    params_dict = {
        'Lx': Lx,
        'nx': nx,
        'c_s': c_s,
        'rho_o': rho_o,
        'const': const
    }
    if dimension >= 2:
        params_dict['Ly'] = Ly
        params_dict['ny'] = ny
    if dimension == 3:
        params_dict['Lz'] = Lz
        params_dict['nz'] = nz
    
    # Initialize state dictionary
    state = {
        'rho': rho0,
        'velocities': velocities,
        'momenta': momenta,
        'phi': phi0
    }
    
    # Time-stepping loop
    t = 0.0
    n_steps = 0
    
    # Initial timestep
    dt = compute_adaptive_timestep(velocities, c_s, dx, nu, include_sound_speed=True)
    
    while t < time:
        # Ensure last step doesn't overshoot
        if t + dt > time:
            dt = time - t
        
        # Perform one time step
        state = lax_time_step(state, dt, params_dict, dimension, gravity=gravity)
        
        # Update time and step counter
        t += dt
        n_steps += 1
        
        # Calculate dt for next step
        dt = compute_adaptive_timestep(state['velocities'], c_s, dx, nu)
    
    # Prepare coordinates dictionary
    coordinates = {'x': x}
    if dimension >= 2:
        coordinates['y'] = y
    if dimension == 3:
        coordinates['z'] = z
    
    # Prepare metadata dictionary
    metadata = {
        'time': t,
        'iterations': n_steps,
        'rho_max': np.max(state['rho']),
        'gravity': gravity,
        'nu': nu,
        'dimension': dimension
    }
    
    # Prepare linear theory comparison if requested
    linear_theory_comparison = None
    if comparison:
        # Linear theory comparison would be computed here if needed
        # For now, set to None as it's optional
        linear_theory_comparison = {}
    
    # Create and return SimulationResult
    return SimulationResult(
        dimension=dimension,
        density=state['rho'],
        velocity_components=state['velocities'],
        coordinates=coordinates,
        metadata=metadata,
        potential=state['phi'] if gravity else None,
        linear_theory_comparison=linear_theory_comparison
    )


def generate_velocity_field_power_spectrum(nx, ny, Lx, Ly, power_index=-3.0, amplitude=0.02, DIMENSION=2, random_seed=None, nz=None, Lz=None):
    """
    Generate 2D or 3D velocity components (vx, vy, vz (optional)) with an isotropic power-law spectrum P(k) ~ k^{power_index}.
    
    This function generates velocity fields directly at the target resolution (nx, ny, nz (optional))
    without any downsampling or cutoff strategies. Dimension-agnostic implementation.
    
    Args:
        nx, ny: Grid dimensions (required)
        Lx, Ly: Domain sizes (required)
        power_index: Power spectrum index (default: -3.0)
        amplitude: RMS amplitude of velocity field (default: 0.02)
        DIMENSION: 2 or 3 (default: 2)
        random_seed: Random seed for reproducibility
        nz, Lz: Grid dimension and domain size for 3D (required if DIMENSION=3)
    
    Returns:
        For 2D: (vx, vy)
        For 3D: (vx, vy, vz)
    """
    if random_seed is not None:
        rng = np.random.default_rng(random_seed)
    else:
        rng = np.random.default_rng()

    # Determine shape and domain lengths based on dimension
    if DIMENSION == 2:
        shape = (nx, ny)
        domain_lengths = (Lx, Ly)
    elif DIMENSION == 3:
        if nz is None or Lz is None:
            raise ValueError("nz and Lz must be provided for 3D")
        shape = (nx, ny, nz)
        domain_lengths = (Lx, Ly, Lz)
    else:
        raise ValueError(f"Unsupported DIMENSION={DIMENSION}. Use 2 or 3.")

    def synthesize_component():
        # 1. Generate random field at target resolution (dimension-agnostic)
        field = rng.standard_normal(shape)
        F = fftn(field)  # fftn works for any dimension
        
        # 2. Construct k-space grid at target resolution
        k_grid = build_k_space_grid(shape, domain_lengths, DIMENSION)
        
        # 3. Compute |k| magnitude (dimension-agnostic)
        if DIMENSION == 2:
            kx, ky, kxg, kyg = k_grid
            kk = np.sqrt(kxg**2 + kyg**2)
        else:  # DIMENSION == 3
            kx, ky, kz, kxg, kyg, kzg = k_grid
            kk = np.sqrt(kxg**2 + kyg**2 + kzg**2)
        
        # 4. Apply power-law filter
        filt = np.zeros_like(kk)
        mask = kk > 0
        filt[mask] = kk[mask]**(power_index / 2.0)
        
        F_filtered = F * filt
        
        # 5. Transform back to real space (dimension-agnostic)
        comp = np.real(ifftn(F_filtered))
        
        # 6. Normalize the field
        comp -= np.mean(comp)
        std = np.std(comp)
        if std > 0:
            comp = comp * (amplitude / std)
        
        return comp

    # Generate velocity components
    vx0 = synthesize_component()
    vy0 = synthesize_component()
    
    if DIMENSION == 3:
        vz0 = synthesize_component()
        return vx0, vy0, vz0
    else:
        return vx0, vy0

def generate_shared_velocity_field(nx, ny, Lx, Ly, power_index=-4.0, amplitude=0.01, DIMENSION=2, random_seed=None, nz=None, Lz=None):
    """
    Generate shared velocity field for both PINN and FD to ensure identical initial conditions.
    This function creates the velocity field once and returns both numpy arrays (for FD) 
    and interpolation functions (for PINN). Dimension-agnostic implementation.
    
    Args:
        nx, ny: Grid dimensions (required)
        Lx, Ly: Domain sizes (required)
        power_index: Power spectrum index (default: -4.0)
        amplitude: RMS amplitude of velocity field (default: 0.01)
        DIMENSION: 2 or 3 (default: 2)
        random_seed: Random seed for reproducibility
        nz, Lz: Grid dimension and domain size for 3D (required if DIMENSION=3)
    
    Returns:
        For 2D: (vx_np, vy_np, vx_interp, vy_interp)
        For 3D: (vx_np, vy_np, vz_np, vx_interp, vy_interp, vz_interp)
    """
    if random_seed is None:
        random_seed = RANDOM_SEED
    
    # Generate the velocity field using the unified power spectrum generator
    if DIMENSION == 2:
        vx_np, vy_np = generate_velocity_field_power_spectrum(nx, ny, Lx, Ly, power_index, amplitude, DIMENSION=2, random_seed=random_seed)
        velocity_arrays = [vx_np, vy_np]
    elif DIMENSION == 3:
        if nz is None or Lz is None:
            raise ValueError("nz and Lz must be provided for 3D")
        vx_np, vy_np, vz_np = generate_velocity_field_power_spectrum(nx, ny, Lx, Ly, power_index, amplitude, DIMENSION=3, random_seed=random_seed, nz=nz, Lz=Lz)
        velocity_arrays = [vx_np, vy_np, vz_np]
    else:
        raise ValueError(f"Unsupported DIMENSION={DIMENSION}. Use 2 or 3.")
    
    # Create interpolation functions for PINN (dimension-agnostic)
    from scipy.interpolate import RegularGridInterpolator
    
    # Build coordinate grids (exclude right boundary for periodic domains)
    if DIMENSION == 2:
        coords = (np.linspace(0, Lx, nx, endpoint=False), np.linspace(0, Ly, ny, endpoint=False))
    else:  # DIMENSION == 3
        coords = (np.linspace(0, Lx, nx, endpoint=False), 
                  np.linspace(0, Ly, ny, endpoint=False),
                  np.linspace(0, Lz, nz, endpoint=False))
    
    # Create interpolators for all velocity components
    interpolators = [RegularGridInterpolator(coords, vel_array, method='linear', bounds_error=False, fill_value=0.0) 
                     for vel_array in velocity_arrays]
    
    # Return arrays and interpolators
    if DIMENSION == 2:
        return vx_np, vy_np, interpolators[0], interpolators[1]
    else:  # DIMENSION == 3
        return vx_np, vy_np, vz_np, interpolators[0], interpolators[1], interpolators[2]

def plot_results(x, y, rho0, vx0, vy0, time, rho_1, t, gravity=False, comparison=False, 
                 rho_LT=None, vx_LT=None, vy_LT=None, rho_LT_max=None, rho_max=None):
    """
    Plot the results from the LAX solver.
    
    Args:
        x: x coordinates (1D array)
        y: y coordinates (1D array)
        rho0: Density field (2D array)
        vx0: x-velocity field (2D array)
        vy0: y-velocity field (2D array)
        time: Final time value
        rho_1: Density perturbation amplitude
        t: Current time (for labeling)
        gravity: Whether gravity is enabled
        comparison: Whether to plot linear theory comparison
        rho_LT: Linear theory density (1D array, optional)
        vx_LT: Linear theory x-velocity (1D array, optional)
        vy_LT: Linear theory y-velocity (1D array, optional)
        rho_LT_max: Linear theory maximum density (scalar, optional)
        rho_max: FD maximum density (scalar, optional)
    """
    plt.figure(1, figsize=(6, 4))
    plt.plot(x, rho0[:, 1] - rho_o, linewidth=1, label="FD at t={}".format(round(t, 2)))
    plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
    plt.xlabel(r"$\mathbf{x}$")
    plt.title("At time {} and rho_1 = {}".format(time, rho_1))
    plt.ylabel(r"$\mathbf{\rho - \rho_{0}}$")
    if comparison and rho_LT is not None:
        plt.plot(x, rho_LT - rho_o, '--', linewidth=1, label="LT")
        plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)

    plt.figure(2, figsize=(6, 4))
    plt.plot(x, vx0[:, 1], '--', markersize=2, label="t={}".format(round(t, 2)))
    plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
    plt.xlabel(r"$\mathbf{x}$")
    plt.title(r"Lax Solution Velocity For $\rho_1$ = {}".format(rho_1))
    plt.ylabel("vx")
    if comparison and vx_LT is not None:
        plt.plot(x, vx_LT, '--', linewidth=1, label="LT")
        plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
        
    plt.figure(3, figsize=(6, 4))
    plt.plot(y, vy0[1, :], '--', markersize=2, label="t={}".format(round(t, 2)))
    plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
    plt.xlabel(r"$\mathbf{y}$")
    plt.title(r"Lax Solution Velocity For $\rho_1$ = {}".format(rho_1))
    plt.ylabel("vy")
    if comparison and vy_LT is not None:
        plt.plot(y, vy_LT, '--', linewidth=1, label="LT")
        plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)

    if gravity:
        #### Plotting the comparison of the \rho_max for FD and Linear Theory
        plt.figure(5, figsize=(6, 4))
        if rho_max is not None:
            plt.scatter(time, rho_max, label="FD")
        plt.xlabel("t")
        plt.ylabel(r"$\log (\rho_{\rm max} - \rho_{0}) $")
        plt.yscale('log')
        plt.legend(numpoints=1, loc='upper left', fancybox=True, shadow=True)
        if comparison and rho_LT_max is not None:
            plt.scatter(time, rho_LT_max, facecolors='none', edgecolors='r', label="LT")
            plt.legend(numpoints=1, loc='upper left', fancybox=True, shadow=True)
            print(time, rho_LT_max)

def fft_poisson_solver(rho, Lx, nx, Ly=None, ny=None, Lz=None, nz=None):
    """
    Unified FFT-based Poisson solver for 2D and 3D periodic domains.
    
    Uses discrete Fast Fourier Transform to solve the Poisson Equation.
    Applies correction due to the finite difference grid of phi.
    
    Args:
        rho: Source function (density) - 2D or 3D array
        Lx: Domain size in x direction
        nx: Number of grid points in x direction
        Ly: Domain size in y direction (required for 2D/3D)
        ny: Number of grid points in y direction (required for 2D/3D)
        Lz: Domain size in z direction (optional, for 3D only)
        nz: Number of grid points in z direction (optional, for 3D only)
    
    Returns:
        phi: The potential field (same shape as rho)
    """
    # Determine dimensionality based on provided parameters
    is_3d = (Lz is not None) and (nz is not None)
    
    if is_3d:
        # 3D case
        dx = Lx / nx
        dy = Ly / ny
        dz = Lz / nz
        rhohat = fftn(rho)
        kx, ky, kz, kx_mesh, ky_mesh, kz_mesh = build_k_space_grid((nx, ny, nz), (Lx, Ly, Lz), 3)
        # Use discrete Laplacian for consistency with finite-difference scheme
        laplace = (2*(np.cos(kx_mesh*dx)-1)/(dx**2) + 
                  2*(np.cos(ky_mesh*dy)-1)/(dy**2) + 
                  2*(np.cos(kz_mesh*dz)-1)/(dz**2))
        laplace[laplace == 0] = 1e-9
        phihat = rhohat / laplace
        phi = np.real(ifftn(phihat))
    else:
        # 2D case
        if Ly is None or ny is None:
            raise ValueError("Ly and ny must be provided for 2D case")
        dx = Lx / nx
        dy = Ly / ny
        # Calculate the Fourier modes of the gas density
        rhohat = fft2(rho)
        # Calculate the wave numbers in x and y directions
        kx, ky, kx_mesh, ky_mesh = build_k_space_grid((nx, ny), (Lx, Ly), 2)
        # Construct the discrete Laplacian operator in Fourier space
        # This ensures consistency with finite-difference gradients used in LAX scheme
        laplace = 2*(np.cos(kx_mesh*dx)-1)/(dx**2) + 2*(np.cos(ky_mesh*dy)-1)/(dy**2)
        laplace[laplace == 0] = 1e-9
        # Solve for the electrostatic potential in Fourier space
        phihat = rhohat / laplace
        # Transform back to real space to obtain the solution
        phi = np.real(ifft2(phihat))
    
    return phi


def lax_solution(time,N,nu,lam,num_of_waves,rho_1,gravity=False,isplot = None,comparison =None,animation=None,
                 use_velocity_ps=False, ps_index=-3.0, vel_rms=0.02, random_seed=None, vx0_shared=None, vy0_shared=None):
    '''
    DEPRECATED: This function is a wrapper around the unified lax_solver().
    Use lax_solver() directly for new code.
    
    This function solves the hydrodynamic Eqns in 2D with/without self gravity using LAX methods.
    
    Input:  Time till the system is integrated :time
            Number of Xgrid points : N
            Courant number : nu
            Wavelength : If lambda> lambdaJ (with gravity--> Instability) else waves propagation 
            Number of waves : The domain size changes with this maintain periodicity
            Density perturbation : rho1 (for linear or non-linear perturbation)
            Gravity:  If True it deploys the FFT routine to estimate the potential 
            isplot(optional): if True plots the output
            Comparison (optional) : If True then the plots are overplotted with LT solutions for comparison
            Animation (optional): Not used at the moment
    
    Output: Density, velocity + (phi and g if gravity is True)
            isplot: True then the plots are generated 
    
    '''
    import warnings
    warnings.warn(
        "lax_solution() is deprecated. Use lax_solver() instead. "
        "This function will be removed in a future version.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # Convert parameters to unified solver format
    Lx = Ly = lam * num_of_waves
    Nx = Ny = N
    c_s = cs
    
    domain_params = {'Lx': Lx, 'Ly': Ly, 'nx': Nx, 'ny': Ny}
    physics_params = {
        'c_s': c_s,
        'rho_o': rho_o,
        'const': const,
        'G': G,
        'rho_1': rho_1,
        'lam': lam
    }
    
    # Set up IC parameters
    ic_type = 'power_spectrum' if use_velocity_ps else 'sinusoidal'
    ic_params = None
    if use_velocity_ps:
        ic_params = {
            'power_index': ps_index,
            'amplitude': vel_rms,
            'random_seed': random_seed,
            'vx0_shared': vx0_shared,
            'vy0_shared': vy0_shared
        }
    else:
        ic_params = {
            'KX': KX,
            'KY': KY
        }
    
    options = {
        'gravity': gravity,
        'nu': nu,
        'comparison': comparison if comparison is not None else False,
        'isplot': isplot if isplot is not None else False
    }
    
    # Call unified solver
    result = lax_solver(time, domain_params, physics_params, ic_type=ic_type, ic_params=ic_params, options=options)
    
    # Extract values to match old return format
    x = result.coordinates['x']
    y = result.coordinates['y']
    rho0 = result.density
    vx0, vy0 = result.velocity_components
    phi0 = result.potential
    n = result.metadata['iterations']
    rho_max = result.metadata['rho_max']
    
    # Handle linear theory comparison if needed
    rho_LT = None
    rho_LT_max = None
    vx_LT = None
    if comparison:
        # Compute linear theory values (same logic as before)
        if not gravity:
            v_1, _, _ = compute_perturbation_velocity(rho_1, rho_o, lam, c_s, None, False)
            rho_LT = rho_o + rho_1*np.cos(2*np.pi * x/lam - 2*np.pi/lam *time)
            rho_LT_max = np.max(rho_LT)
            vx_LT = v_1* np.cos(2*np.pi * x/lam - 2*np.pi/lam *time)
        else:
            jeans = compute_jeans_length(c_s, rho_o, const, G)
            if lam >= jeans:
                v_1, alpha, _ = compute_perturbation_velocity(rho_1, rho_o, lam, c_s, jeans, True)
                rho_LT = rho_o + rho_1*np.exp(alpha * time)*np.cos(2*np.pi*x/lam)
                rho_LT_max = np.max(rho_LT)
                vx_LT = -v_1*np.exp(alpha * time)*np.sin(2*np.pi*x/lam)
            else:
                v_1, alpha, _ = compute_perturbation_velocity(rho_1, rho_o, lam, c_s, jeans, True)
                rho_LT = rho_o + rho_1*np.cos(alpha * time - 2*np.pi*x/lam)
                rho_LT_max = np.max(rho_o + rho_1*np.cos(alpha * time - 2*np.pi*x/lam))
                vx_LT = v_1*np.cos(alpha * time - 2*np.pi*x/lam)
    
    # Return in old format
    if isplot:
        # Plotting is handled by lax_solver if isplot=True
        return
    
    if gravity:
        if comparison:
            return x, rho0, vx0, phi0, n, rho_LT, rho_LT_max, rho_max, vx_LT
        else:
            return x, rho0, vx0, vy0, phi0, n, rho_max
    else:
        if comparison:
            return rho0, vx0, rho_LT, rho_LT_max, rho_max, vx_LT
        else:
            return rho0, vx0, rho_max


def lax_solution_3d_sinusoidal(time, N, nu, lam, num_of_waves, rho_1, gravity=True,
                               use_velocity_ps=False, ps_index=-3.0, vel_rms=0.02, 
                               random_seed=None, vx0_shared=None, vy0_shared=None, vz0_shared=None):
    """
    DEPRECATED: This function is a wrapper around the unified lax_solver().
    Use lax_solver() directly for new code.
    
    3D LAX solver for sinusoidal perturbations with optional self-gravity.
    Returns full 3D fields which can be sliced for visualization/comparison.
    
    Args:
        time: Final simulation time
        N: Grid resolution (Nx = Ny = Nz = N)
        nu: Courant number
        lam: Wavelength
        num_of_waves: Number of waves in domain
        rho_1: Density perturbation amplitude
        gravity: Whether to include self-gravity (default: True)
        use_velocity_ps: If True, use power spectrum velocity initialization instead of sinusoidal (default: False)
        ps_index: Power spectrum index (default: -3.0)
        vel_rms: RMS velocity amplitude (default: 0.02)
        random_seed: Random seed for reproducibility (default: None)
        vx0_shared: Optional pre-generated x-velocity field (default: None)
        vy0_shared: Optional pre-generated y-velocity field (default: None)
        vz0_shared: Optional pre-generated z-velocity field (default: None)
    """
    import warnings
    warnings.warn(
        "lax_solution_3d_sinusoidal() is deprecated. Use lax_solver() with dimension=3 instead. "
        "This function will be removed in a future version.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # Convert parameters to unified solver format
    Lx = Ly = Lz = lam * num_of_waves
    Nx = Ny = Nz = int(N)
    c_s = cs
    
    domain_params = {'Lx': Lx, 'Ly': Ly, 'Lz': Lz, 'nx': Nx, 'ny': Ny, 'nz': Nz}
    physics_params = {
        'c_s': c_s,
        'rho_o': rho_o,
        'const': const,
        'G': G,
        'rho_1': rho_1,
        'lam': lam
    }
    
    # Set up IC parameters based on use_velocity_ps flag
    ic_type = 'power_spectrum' if use_velocity_ps else 'sinusoidal'
    ic_params = None
    if use_velocity_ps:
        ic_params = {
            'power_index': ps_index,
            'amplitude': vel_rms,
            'random_seed': random_seed,
            'vx0_shared': vx0_shared,
            'vy0_shared': vy0_shared,
            'vz0_shared': vz0_shared
        }
    else:
        ic_params = {
            'KX': KX,
            'KY': KY,
            'KZ': KZ
        }
    
    options = {
        'gravity': gravity,
        'nu': nu,
        'comparison': False,
        'isplot': False
    }
    
    # Call unified solver
    result = lax_solver(time, domain_params, physics_params, ic_type=ic_type, ic_params=ic_params, options=options)
    
    # Extract values to match old return format
    x = result.coordinates['x']
    y = result.coordinates['y']
    z = result.coordinates['z']
    rho0 = result.density
    vx0, vy0, vz0 = result.velocity_components
    phi0 = result.potential
    k_iter = result.metadata['iterations']
    rho_max = result.metadata['rho_max']
    
    return x, y, z, rho0, vx0, vy0, vz0, phi0, k_iter, rho_max


def lax_solution_warm_start(rho_ic, vx_ic, vy_ic, x_grid, y_grid, 
                            t_start, t_end, nu=0.5, save_times=None, gravity=True):
    """
    Run FD solver from custom initial conditions (warm-start).
    
    This function allows restarting the FD solver from a PINN state or any custom state,
    enabling efficient generation of FD data for hybrid PINN-FD training.
    
    Args:
        rho_ic: Initial density field (Nx, Ny)
        vx_ic: Initial x-velocity field (Nx, Ny)
        vy_ic: Initial y-velocity field (Nx, Ny)
        x_grid: x coordinates (Nx,)
        y_grid: y coordinates (Ny,)
        t_start: Starting time
        t_end: Ending time
        nu: Courant number
        save_times: List of times to save snapshots [default: [t_end]]
        gravity: Whether to include self-gravity (default: True)
    
    Returns:
        Dictionary: {time: (rho, vx, vy, phi, x, y)} for each saved time
    """
    if save_times is None:
        save_times = [t_end]
    
    # Domain setup
    Nx, Ny = rho_ic.shape
    Lx = x_grid[-1] - x_grid[0] + (x_grid[1] - x_grid[0])  # Approximate domain size
    Ly = y_grid[-1] - y_grid[0] + (y_grid[1] - y_grid[0])
    dx = Lx / Nx
    dy = Ly / Ny
    
    # Physical constants
    c_s = cs
    
    # Use setup_warm_start_ic() helper to set up initial conditions
    provided_fields = {
        'rho': rho_ic,
        'vx': vx_ic,
        'vy': vy_ic
    }
    ic_result = setup_warm_start_ic(provided_fields)
    rho0 = ic_result['rho']
    vx0 = ic_result['vx']
    vy0 = ic_result['vy']
    
    # Calculate initial potential (gravity is always True for collapse problems)
    phi0 = fft_poisson_solver(const * (rho0 - rho_o), Lx, Nx, Ly=Ly, ny=Ny)
    
    # Initialize flux terms
    Px0 = rho0 * vx0
    Py0 = rho0 * vy0
    
    # Storage for snapshots
    snapshots = {}
    
    # Time-stepping loop
    t = t_start
    k = 0
    
    # Initial dt
    vmax_initial = max(np.max(np.abs(vx0)), np.max(np.abs(vy0)), c_s)
    dt = nu * dx / vmax_initial
    
    while t < t_end:
        # Check if we should save a snapshot before this step
        for save_t in save_times:
            if t <= save_t < t + dt and save_t not in snapshots:
                # Save current state (or interpolate if needed)
                snapshots[save_t] = (rho0.copy(), vx0.copy(), vy0.copy(), 
                                     phi0.copy(), x_grid.copy(), y_grid.copy())
        
        # Ensure last step doesn't overshoot
        if t + dt > t_end:
            dt = t_end - t
        
        # LAX time-stepping
        mux = dt / (2 * dx)
        muy = dt / (2 * dy)
        
        # Update density
        rho1 = (1/4) * (np.roll(rho0, -1, axis=0) + np.roll(rho0, 1, axis=0) +
                       np.roll(rho0, -1, axis=1) + np.roll(rho0, 1, axis=1)) - \
               (mux * (np.roll(rho0, -1, axis=0) * np.roll(vx0, -1, axis=0) -
                       np.roll(rho0, 1, axis=0) * np.roll(vx0, 1, axis=0))) - \
               (muy * (np.roll(rho0, -1, axis=1) * np.roll(vy0, -1, axis=1) -
                       np.roll(rho0, 1, axis=1) * np.roll(vy0, 1, axis=1)))
        
        # Update momentum (with gravity)
        if gravity:
            Px1 = 0.25 * (np.roll(Px0, -1, axis=0) + np.roll(Px0, 1, axis=0) +
                          np.roll(Px0, -1, axis=1) + np.roll(Px0, 1, axis=1)) - \
                  (mux * (np.roll(Px0, -1, axis=0) * np.roll(vx0, -1, axis=0) -
                          np.roll(Px0, 1, axis=0) * np.roll(vx0, 1, axis=0))) - \
                  (muy * (np.roll(Px0, -1, axis=1) * np.roll(vy0, -1, axis=1) -
                          np.roll(Px0, 1, axis=1) * np.roll(vy0, 1, axis=1))) - \
                  ((c_s**2) * mux * (np.roll(rho0, -1, axis=0) - np.roll(rho0, 1, axis=0))) - \
                  (mux * rho0 * (np.roll(phi0, -1, axis=0) - np.roll(phi0, 1, axis=0)))
            
            Py1 = 0.25 * (np.roll(Py0, -1, axis=0) + np.roll(Py0, 1, axis=0) +
                          np.roll(Py0, -1, axis=1) + np.roll(Py0, 1, axis=1)) - \
                  (muy * (np.roll(Py0, -1, axis=1) * np.roll(vy0, -1, axis=1) -
                          np.roll(Py0, 1, axis=1) * np.roll(vy0, 1, axis=1))) - \
                  (mux * (np.roll(Py0, -1, axis=0) * np.roll(vx0, -1, axis=0) -
                          np.roll(Py0, 1, axis=0) * np.roll(vx0, 1, axis=0))) - \
                  ((c_s**2) * muy * (np.roll(rho0, -1, axis=1) - np.roll(rho0, 1, axis=1))) - \
                  (muy * rho0 * (np.roll(phi0, -1, axis=1) - np.roll(phi0, 1, axis=1)))
            
            # Update potential
            phi1 = fft_poisson_solver(const * (rho1 - rho_o), Lx, Nx, Ly=Ly, ny=Ny)
        else:
            # Without gravity (shouldn't happen for collapse problems, but included for completeness)
            Px1 = 0.25 * (np.roll(Px0, -1, axis=0) + np.roll(Px0, 1, axis=0) +
                          np.roll(Px0, -1, axis=1) + np.roll(Px0, 1, axis=1)) - \
                  (mux * (np.roll(Px0, -1, axis=0) * np.roll(vx0, -1, axis=0) -
                          np.roll(Px0, 1, axis=0) * np.roll(vx0, 1, axis=0))) - \
                  (muy * (np.roll(Px0, -1, axis=1) * np.roll(vy0, -1, axis=1) -
                          np.roll(Px0, 1, axis=1) * np.roll(vy0, 1, axis=1))) - \
                  ((c_s**2) * mux * (np.roll(rho0, -1, axis=0) - np.roll(rho0, 1, axis=0)))
            
            Py1 = 0.25 * (np.roll(Py0, -1, axis=0) + np.roll(Py0, 1, axis=0) +
                          np.roll(Py0, -1, axis=1) + np.roll(Py0, 1, axis=1)) - \
                  (muy * (np.roll(Py0, -1, axis=1) * np.roll(vy0, -1, axis=1) -
                          np.roll(Py0, 1, axis=1) * np.roll(vy0, 1, axis=1))) - \
                  (mux * (np.roll(Py0, -1, axis=0) * np.roll(vx0, -1, axis=0) -
                          np.roll(Py0, 1, axis=0) * np.roll(vx0, 1, axis=0))) - \
                  ((c_s**2) * muy * (np.roll(rho0, -1, axis=1) - np.roll(rho0, 1, axis=1)))
            
            phi1 = np.zeros_like(rho1)
        
        # Update velocities
        vx1 = Px1 / rho1
        vy1 = Py1 / rho1
        
        # Update state
        rho0 = rho1
        vx0 = vx1
        vy0 = vy1
        Px0 = Px1
        Py0 = Py1
        if gravity:
            phi0 = phi1
        
        t += dt
        k += 1
        
        # Calculate dt for next step
        dt = compute_adaptive_timestep([vx0, vy0], c_s, dx, nu)
    
    # Save final snapshot if not already saved
    if t_end not in snapshots:
        snapshots[t_end] = (rho0.copy(), vx0.copy(), vy0.copy(), 
                           phi0.copy(), x_grid.copy(), y_grid.copy())
    
    return snapshots


def lax_solution1D_sinusoidal(time,N,nu,lam,num_of_waves,rho_1,gravity=False,isplot = None,comparison =None,animation=None):
    '''
    1D LAX solver for sinusoidal initial conditions with optional self-gravity and linear theory outputs.
    Returns density, velocity, potential (if gravity), and optional linear theory references.
    '''
    lam = lam
    L = lam * num_of_waves

    c_s = cs             # Sound Speed (from config)
    rho0_base = rho_o    # Background density (from config)
    # nu, const, G are already imported from config, no need to reassign

    nx = int(N)
    dx = float(L / nx)
    dt = nu * dx / c_s
    mu = dt / (2 * dx)
    n = int(time / dt)

    # Exclude right boundary for periodic domains to avoid double-counting
    x = np.linspace(0, L, nx, endpoint=False)

    rho0 = np.zeros(nx)
    phi0 = np.zeros(nx)
    v0 = np.zeros(nx)
    P0 = np.zeros(nx)

    rho1 = np.zeros(nx)
    phi1 = np.zeros(nx)
    v1 = np.zeros(nx)
    P1 = np.zeros(nx)

    if gravity:
        jeans = compute_jeans_length(c_s, rho0_base, const, G)

    # Initial conditions
    rho0 = rho0_base + rho_1 * np.cos(2*np.pi*x/lam)

    if not gravity:
        v_1, _, _ = compute_perturbation_velocity(rho_1, rho0_base, lam, c_s, None, False)
        v0 = v_1 * np.cos(2*np.pi*x/lam)
        if comparison:
            rho_LT = rho0_base + rho_1 * np.cos(2*np.pi * x/lam - 2*np.pi/lam * time)
            rho_LT_max = np.max(rho_LT)
            v_LT = v_1 * np.cos(2*np.pi * x/lam - 2*np.pi/lam * time)
    else:
        if lam >= jeans:
            v_1, alpha, _ = compute_perturbation_velocity(rho_1, rho0_base, lam, c_s, jeans, True)
            v0 = - v_1 * np.sin(2*np.pi*x/lam)
            if comparison:
                rho_LT = rho0_base + rho_1*np.exp(alpha * time)*np.cos(2*np.pi*x/lam)
                rho_LT_max = np.max(rho_LT)
                v_LT = -v_1*np.exp(alpha * time)*np.sin(2*np.pi*x/lam)
        else:
            v_1, alpha, _ = compute_perturbation_velocity(rho_1, rho0_base, lam, c_s, jeans, True)
            v0 = v_1 * np.cos(2*np.pi*x/lam)
            if comparison:
                rho_LT = rho0_base + rho_1*np.cos(alpha * time - 2*np.pi*x/lam)
                rho_LT_max = np.max(rho_LT)
                v_LT = v_1*np.cos(alpha * time - 2*np.pi*x/lam)

        # 1D Poisson (periodic) via FFT: phi_k = rho_k / (-k^2), k=0 set to 0
        k = 2*np.pi*np.fft.fftfreq(nx, d=dx)
        rhohat = np.fft.fft(const*(rho0 - rho0_base))
        denom = -(k**2)
        denom[0] = 1.0
        phihat = rhohat / denom
        phihat[0] = 0.0
        phi0 = np.real(np.fft.ifft(phihat))

    P0 = rho0 * v0

    for _ in range(1, n):
        rho1 = 0.5*(np.roll(rho0,-1)+ np.roll(rho0,1)) - mu*(np.roll(rho0,-1)*np.roll(v0,-1) - np.roll(rho0,1)*np.roll(v0,1))

        if not gravity:
            P1 = 0.5*(np.roll(P0,-1)+ np.roll(P0,1)) - mu*(np.roll(P0,-1)*np.roll(v0,-1) - np.roll(P0,1)*np.roll(v0,1)) - (c_s**2)*mu*(np.roll(rho0,-1) - np.roll(rho0,1))
        else:
            P1 = 0.5*(np.roll(P0,-1)+ np.roll(P0,1)) - mu*(np.roll(P0,-1)*np.roll(v0,-1) - np.roll(P0,1)*np.roll(v0,1)) - (c_s**2)*mu*(np.roll(rho0,-1) - np.roll(rho0,1)) - mu*rho0*(np.roll(phi0,-1) - np.roll(phi0,1))
            k = 2*np.pi*np.fft.fftfreq(nx, d=dx)
            rhohat = np.fft.fft(const*(rho1 - rho0_base))
            denom = -(k**2)
            denom[0] = 1.0
            phihat = rhohat / denom
            phihat[0] = 0.0
            phi1 = np.real(np.fft.ifft(phihat))

        v1 = P1 / rho1

        rho0, v0, P0, phi0 = rho1, v1, P1, phi1

        vmax = np.max(np.abs(v1))
        dt1 = nu*dx/(vmax if vmax != 0 else c_s)
        dt2 = nu*dx/c_s
        dt = min(dt1, dt2)
        mu = dt/(2*dx)
        n = int(time/dt)

    rho_max = np.max(rho1)

    if isplot:
        return
    else:
        if gravity:
            if comparison:
                return x, rho1, v1, phi1, n, rho_LT, rho_LT_max, rho_max, v_LT
            else:
                return x, rho1, v1, phi1, n, rho_max
        else:
            if comparison:
                return rho1, v1, rho_LT, rho_LT_max, rho_max, v_LT
            else:
                return rho1, v1, rho_max
_register_module('numerical_solvers.LAX', ['DomainParams', 'SimulationParams', 'SimulationResult', 'build_k_space_grid', 'compute_adaptive_timestep', 'compute_jeans_length', 'compute_lax_density_update', 'compute_lax_momentum_update', 'compute_perturbation_velocity', 'fft_poisson_solver', 'generate_shared_velocity_field', 'generate_velocity_field_power_spectrum', 'initialize_arrays', 'lax_solution', 'lax_solution1D_sinusoidal', 'lax_solution_3d_sinusoidal', 'lax_solution_warm_start', 'lax_solver', 'lax_time_step', 'plot_results', 'setup_power_spectrum_ic', 'setup_sinusoidal_ic', 'setup_warm_start_ic'])

# ==== Module: numerical_solvers.LAX_torch (numerical_solvers/LAX_torch.py) ====
import numpy as np
import torch
from dataclasses import dataclass
from typing import Optional
from config import cs, rho_o, const, G, KX, KY, KZ

# Device setup - check at module import
has_gpu = torch.cuda.is_available()
device = torch.device("cuda:0" if has_gpu else "cpu")
dtype = torch.float64
if has_gpu:
    print(f"LAX_torch: GPU available, using device: {device}")
    print(f"  GPU name: {torch.cuda.get_device_name(0)}")
else:
    print(f"LAX_torch: No GPU available, using device: {device}")


@dataclass
class SimulationResult:
    """Result container for LAX solver simulations (PyTorch version)."""
    dimension: int  # 1, 2, or 3
    density: np.ndarray  # Density field (converted to numpy)
    velocity_components: list  # List of velocity arrays [vx, vy, ...] or [vx, vy, vz] (numpy)
    coordinates: dict  # Dict with 'x', 'y' (2D/3D), 'z' (3D) coordinate arrays (numpy)
    metadata: dict  # Dict with time, iterations, rho_max, etc.
    potential: Optional[np.ndarray] = None  # Gravitational potential (if gravity=True, numpy)
    linear_theory_comparison: Optional[dict] = None  # Optional linear theory comparison data


# ============================================================================
# Helper Functions
# ============================================================================

def compute_jeans_length_torch(c_s, rho_o, const, G):
    """
    Compute the Jeans length for gravitational instability (PyTorch version).
    
    Args:
        c_s: Sound speed
        rho_o: Background density
        const: Constant factor
        G: Gravitational constant
    
    Returns:
        Jeans length as torch tensor
    """
    return torch.sqrt(torch.tensor(4 * np.pi**2 * c_s**2 / (const * G * rho_o), device=device, dtype=dtype))


def compute_perturbation_velocity_torch(rho_1, rho_o, lam, c_s, jeans, gravity):
    """
    Compute the velocity perturbation amplitude v_1 and growth rate alpha (PyTorch version).
    
    Args:
        rho_1: Density perturbation amplitude
        rho_o: Background density
        lam: Wavelength
        c_s: Sound speed
        jeans: Jeans length (torch tensor or float)
        gravity: Whether gravity is enabled
    
    Returns:
        tuple: (v_1, alpha, is_unstable)
            v_1: Velocity perturbation amplitude (torch tensor)
            alpha: Growth/oscillation rate (torch tensor or None)
            is_unstable: True if gravitational instability (lam >= jeans), False otherwise
    """
    if not gravity:
        # No gravity: sound wave
        v_1 = torch.tensor((c_s * rho_1) / rho_o, device=device, dtype=dtype)
        return v_1, None, False
    
    # With gravity: check if unstable
    jeans_val = jeans.item() if isinstance(jeans, torch.Tensor) else jeans
    if lam >= jeans_val:
        # Gravitational instability
        alpha = torch.sqrt(torch.tensor(const * G * rho_o - c_s**2 * (2 * np.pi / lam)**2, device=device, dtype=dtype))
        v_1 = torch.tensor((rho_1 / rho_o) * (alpha.item() / (2 * np.pi / lam)), device=device, dtype=dtype)
        return v_1, alpha, True
    else:
        # Oscillatory regime
        alpha = torch.sqrt(torch.tensor(c_s**2 * (2 * np.pi / lam)**2 - const * G * rho_o, device=device, dtype=dtype))
        v_1 = torch.tensor((rho_1 / rho_o) * (alpha.item() / (2 * np.pi / lam)), device=device, dtype=dtype)
        return v_1, alpha, False


def build_k_space_grid_torch(shape, domain_lengths, dimension):
    """
    Build k-space grid for FFT operations (PyTorch version).
    
    Args:
        shape: Tuple of grid dimensions (nx, ny) or (nx, ny, nz)
        domain_lengths: Tuple of domain sizes (Lx, Ly) or (Lx, Ly, Lz)
        dimension: 2 or 3
    
    Returns:
        tuple: (kx, ky, kz (optional), kx_mesh, ky_mesh, kz_mesh (optional))
            For 2D: (kx, ky, kx_mesh, ky_mesh)
            For 3D: (kx, ky, kz, kx_mesh, ky_mesh, kz_mesh)
    """
    if dimension == 2:
        nx, ny = shape
        Lx, Ly = domain_lengths
        dx = Lx / nx
        dy = Ly / ny
        kx = 2 * np.pi * torch.fft.fftfreq(nx, d=dx).to(device)
        ky = 2 * np.pi * torch.fft.fftfreq(ny, d=dy).to(device)
        kx_mesh, ky_mesh = torch.meshgrid(kx, ky, indexing='ij')
        return kx, ky, kx_mesh, ky_mesh
    elif dimension == 3:
        nx, ny, nz = shape
        Lx, Ly, Lz = domain_lengths
        dx = Lx / nx
        dy = Ly / ny
        dz = Lz / nz
        kx = 2 * np.pi * torch.fft.fftfreq(nx, d=dx).to(device)
        ky = 2 * np.pi * torch.fft.fftfreq(ny, d=dy).to(device)
        kz = 2 * np.pi * torch.fft.fftfreq(nz, d=dz).to(device)
        kx_mesh, ky_mesh, kz_mesh = torch.meshgrid(kx, ky, kz, indexing='ij')
        return kx, ky, kz, kx_mesh, ky_mesh, kz_mesh
    else:
        raise ValueError(f"Unsupported dimension={dimension}. Use 2 or 3.")


def compute_adaptive_timestep_torch(velocities, c_s, dx, nu, include_sound_speed=False):
    """
    Compute adaptive timestep based on CFL condition (PyTorch version).
    
    Args:
        velocities: List of velocity tensors [vx, vy, ...] or single velocity tensor, or None
        c_s: Sound speed
        dx: Grid spacing
        nu: Courant number
        include_sound_speed: If True, include c_s in the max velocity calculation (for initial timestep)
    
    Returns:
        Adaptive timestep
    """
    if include_sound_speed:
        if velocities is None:
            vmax = c_s
        elif isinstance(velocities, (list, tuple)):
            vmax = max(max(torch.max(torch.abs(v)).item() for v in velocities), c_s)
        else:
            vmax = max(torch.max(torch.abs(velocities)).item(), c_s)
        return nu * dx / vmax
    else:
        if velocities is None:
            vmax = c_s
        elif isinstance(velocities, (list, tuple)):
            vmax = max(torch.max(torch.abs(v)).item() for v in velocities)
        else:
            vmax = torch.max(torch.abs(velocities)).item()
        
        dt1 = nu * dx / vmax if vmax > 1e-9 else float('inf')
        dt2 = nu * dx / c_s
        return min(dt1, dt2)


def compute_lax_density_update_torch(rho, velocities, dt, grid_spacings, dimension):
    """
    Compute LAX density update using dimension-agnostic approach (PyTorch version).
    
    This function implements the LAX method for updating density in the continuity equation:
    ∂ρ/∂t + ∇·(ρv) = 0
    
    The update uses a dimension-agnostic approach that works for 1D, 2D, and 3D without
    if-else chains by iterating over dimensions.
    
    Args:
        rho: Current density tensor (1D, 2D, or 3D torch tensor)
        velocities: List of velocity tensors [vx, vy, ...] or [vx, vy, vz] for 3D
                   Length must match dimension
        dt: Time step
        grid_spacings: List of grid spacings [dx, dy, ...] or [dx, dy, dz] for 3D
                       Length must match dimension
        dimension: Integer dimension (1, 2, or 3)
    
    Returns:
        Updated density tensor (same shape as input rho)
    
    Examples:
        # 1D
        rho_new = compute_lax_density_update_torch(rho, [vx], dt, [dx], 1)
        
        # 2D
        rho_new = compute_lax_density_update_torch(rho, [vx, vy], dt, [dx, dy], 2)
        
        # 3D
        rho_new = compute_lax_density_update_torch(rho, [vx, vy, vz], dt, [dx, dy, dz], 3)
    """
    if len(velocities) != dimension:
        raise ValueError(f"Number of velocity tensors ({len(velocities)}) must match dimension ({dimension})")
    if len(grid_spacings) != dimension:
        raise ValueError(f"Number of grid spacings ({len(grid_spacings)}) must match dimension ({dimension})")
    
    # Compute mu values for each dimension: mu_i = dt / (2 * dx_i)
    mu_values = [dt / (2 * dx) for dx in grid_spacings]
    
    # Initialize with averaging term: (1/(2*dimension)) * sum of all rolled neighbors
    # For each dimension, we add contributions from both +1 and -1 rolls
    rho_new = torch.zeros_like(rho)
    
    # Sum all rolled neighbors (forward and backward for each dimension)
    # Note: PyTorch uses 'dims' parameter instead of 'axis'
    for dim in range(dimension):
        rho_new += torch.roll(rho, -1, dims=dim)  # Forward roll
        rho_new += torch.roll(rho, 1, dims=dim)   # Backward roll
    
    # Normalize by 1/(2*dimension)
    rho_new = rho_new / (2 * dimension)
    
    # Subtract flux terms for each dimension: -mu_i * (flux_forward - flux_backward)
    # where flux = rho * velocity
    for dim in range(dimension):
        mu = mu_values[dim]
        vel = velocities[dim]
        
        # Forward flux: rho_rolled_forward * vel_rolled_forward
        rho_forward = torch.roll(rho, -1, dims=dim)
        vel_forward = torch.roll(vel, -1, dims=dim)
        flux_forward = rho_forward * vel_forward
        
        # Backward flux: rho_rolled_backward * vel_rolled_backward
        rho_backward = torch.roll(rho, 1, dims=dim)
        vel_backward = torch.roll(vel, 1, dims=dim)
        flux_backward = rho_backward * vel_backward
        
        # Subtract the flux difference
        rho_new -= mu * (flux_forward - flux_backward)
    
    return rho_new


def compute_lax_momentum_update_torch(momenta, velocities, rho, phi, c_s, dt, grid_spacings, dimension, gravity=True):
    """
    Compute LAX momentum update using dimension-agnostic approach (PyTorch version).
    
    This function implements the LAX method for updating momentum in the momentum equation:
    ∂(ρv)/∂t + ∇·(ρv⊗v) = -∇P - ρ∇φ
    
    The update uses a dimension-agnostic approach that works for 1D, 2D, and 3D without
    if-else chains by iterating over dimensions.
    
    Args:
        momenta: List of momentum tensors [Px, Py, ...] or [Px, Py, Pz] for 3D
                 Length must match dimension
        velocities: List of velocity tensors [vx, vy, ...] or [vx, vy, vz] for 3D
                   Length must match dimension
        rho: Current density tensor (same shape as momentum tensors)
        phi: Gravitational potential tensor (same shape as momentum tensors)
             Only used if gravity=True
        c_s: Sound speed
        dt: Time step
        grid_spacings: List of grid spacings [dx, dy, ...] or [dx, dy, dz] for 3D
                       Length must match dimension
        dimension: Integer dimension (1, 2, or 3)
        gravity: Whether to include gravity term (default: True)
    
    Returns:
        List of updated momentum tensors (same shape as input momenta)
    
    Examples:
        # 1D
        P_new = compute_lax_momentum_update_torch([Px], [vx], rho, phi, c_s, dt, [dx], 1, gravity=True)
        
        # 2D
        Px_new, Py_new = compute_lax_momentum_update_torch([Px, Py], [vx, vy], rho, phi, c_s, dt, [dx, dy], 2, gravity=True)
        
        # 3D
        Px_new, Py_new, Pz_new = compute_lax_momentum_update_torch([Px, Py, Pz], [vx, vy, vz], rho, phi, c_s, dt, [dx, dy, dz], 3, gravity=True)
    """
    if len(momenta) != dimension:
        raise ValueError(f"Number of momentum tensors ({len(momenta)}) must match dimension ({dimension})")
    if len(velocities) != dimension:
        raise ValueError(f"Number of velocity tensors ({len(velocities)}) must match dimension ({dimension})")
    if len(grid_spacings) != dimension:
        raise ValueError(f"Number of grid spacings ({len(grid_spacings)}) must match dimension ({dimension})")
    
    # Compute mu values for each dimension: mu_i = dt / (2 * dx_i)
    mu_values = [dt / (2 * dx) for dx in grid_spacings]
    
    # Initialize list to store updated momenta
    momenta_new = []
    
    # Update each momentum component
    for comp_dim in range(dimension):
        P = momenta[comp_dim]
        mu_comp = mu_values[comp_dim]  # mu for this component's direction
        
        # Initialize with averaging term: (1/(2*dimension)) * sum of all rolled neighbors
        P_new = torch.zeros_like(P)
        
        # Sum all rolled neighbors (forward and backward for each dimension)
        for dim in range(dimension):
            P_new += torch.roll(P, -1, dims=dim)  # Forward roll
            P_new += torch.roll(P, 1, dims=dim)   # Backward roll
        
        # Normalize by 1/(2*dimension)
        P_new = P_new / (2 * dimension)
        
        # Advection terms: For each dimension, subtract mu_i * (flux_forward - flux_backward)
        # where flux = momentum * velocity
        # Each momentum component is advected by ALL velocity components
        for dim in range(dimension):
            mu = mu_values[dim]
            vel = velocities[dim]
            
            # Forward flux: P_rolled_forward * vel_rolled_forward
            P_forward = torch.roll(P, -1, dims=dim)
            vel_forward = torch.roll(vel, -1, dims=dim)
            flux_forward = P_forward * vel_forward
            
            # Backward flux: P_rolled_backward * vel_rolled_backward
            P_backward = torch.roll(P, 1, dims=dim)
            vel_backward = torch.roll(vel, 1, dims=dim)
            flux_backward = P_backward * vel_backward
            
            # Subtract the flux difference
            P_new -= mu * (flux_forward - flux_backward)
        
        # Pressure gradient term: Only in the component's own direction
        # -c_s^2 * mu_comp * (rho_forward - rho_backward) in direction comp_dim
        rho_forward = torch.roll(rho, -1, dims=comp_dim)
        rho_backward = torch.roll(rho, 1, dims=comp_dim)
        P_new -= (c_s**2) * mu_comp * (rho_forward - rho_backward)
        
        # Gravity term: Only in the component's own direction (if enabled)
        # -mu_comp * rho * (phi_forward - phi_backward) in direction comp_dim
        if gravity:
            phi_forward = torch.roll(phi, -1, dims=comp_dim)
            phi_backward = torch.roll(phi, 1, dims=comp_dim)
            P_new -= mu_comp * rho * (phi_forward - phi_backward)
        
        momenta_new.append(P_new)
    
    return momenta_new


def lax_time_step_torch(state_dict, dt, params_dict, dimension, gravity=True):
    """
    Perform a single LAX time step, updating all state variables (PyTorch version).
    
    This function orchestrates a complete time step by:
    1. Updating density using compute_lax_density_update_torch()
    2. Updating momenta using compute_lax_momentum_update_torch()
    3. Computing velocities from updated momenta and density
    4. Updating gravitational potential if gravity is enabled
    
    Args:
        state_dict: Dictionary containing current state with keys:
                   - 'rho': density tensor
                   - 'velocities': list of velocity tensors [vx, vy, ...] or [vx, vy, vz]
                   - 'momenta': list of momentum tensors [Px, Py, ...] or [Px, Py, Pz]
                   - 'phi': gravitational potential tensor (used if gravity=True)
        dt: Time step
        params_dict: Dictionary containing simulation parameters with keys:
                    - 'Lx': Domain size in x direction
                    - 'Ly': Domain size in y direction (required for 2D/3D)
                    - 'Lz': Domain size in z direction (required for 3D)
                    - 'nx': Number of grid points in x direction
                    - 'ny': Number of grid points in y direction (required for 2D/3D)
                    - 'nz': Number of grid points in z direction (required for 3D)
                    - 'c_s': Sound speed
                    - 'rho_o': Background density (for Poisson solver)
                    - 'const': Constant factor (for Poisson solver)
        dimension: Integer dimension (1, 2, or 3)
        gravity: Whether to include gravity (default: True)
    
    Returns:
        Dictionary with updated state, same structure as input state_dict
    
    Examples:
        # 2D example
        state = {
            'rho': rho0,
            'velocities': [vx0, vy0],
            'momenta': [Px0, Py0],
            'phi': phi0
        }
        params = {
            'Lx': 10.0, 'Ly': 10.0,
            'nx': 100, 'ny': 100,
            'c_s': 1.0, 'rho_o': 1.0, 'const': 1.0
        }
        new_state = lax_time_step_torch(state, dt, params, dimension=2, gravity=True)
    """
    # Extract state variables
    rho = state_dict['rho']
    velocities = state_dict['velocities']
    momenta = state_dict['momenta']
    phi = state_dict.get('phi', None)
    
    # Extract parameters
    Lx = params_dict['Lx']
    nx = params_dict['nx']
    c_s = params_dict['c_s']
    rho_o = params_dict['rho_o']
    const = params_dict['const']
    
    # Extract dimension-specific parameters
    if dimension >= 2:
        Ly = params_dict['Ly']
        ny = params_dict['ny']
    if dimension == 3:
        Lz = params_dict['Lz']
        nz = params_dict['nz']
    
    # Compute grid spacings
    grid_spacings = [Lx / nx]
    if dimension >= 2:
        grid_spacings.append(Ly / ny)
    if dimension == 3:
        grid_spacings.append(Lz / nz)
    
    # Step 1: Update density
    rho_new = compute_lax_density_update_torch(rho, velocities, dt, grid_spacings, dimension)
    
    # Step 2: Update momenta
    momenta_new = compute_lax_momentum_update_torch(
        momenta, velocities, rho, phi, c_s, dt, grid_spacings, dimension, gravity=gravity
    )
    
    # Step 3: Compute velocities from updated momenta and density
    velocities_new = [momenta_new[i] / rho_new for i in range(dimension)]
    
    # Step 4: Update gravitational potential if gravity is enabled
    if gravity:
        # Call appropriate fft solver based on dimension
        if dimension == 1:
            # 1D Poisson solver (not implemented in fft_solver_torch, but we can handle it)
            # For now, skip it for 1D
            phi_new = torch.zeros_like(rho_new)  # 1D gravity not typically used with this solver
        elif dimension == 2:
            phi_new = fft_solver_torch(const * (rho_new - rho_o), Lx, nx, Ly, ny)
        else:  # dimension == 3
            phi_new = fft_solver_torch_3d(const * (rho_new - rho_o), Lx, nx, Ly, ny, Lz, nz)
    else:
        # If no gravity, keep phi as zeros or copy existing if provided
        phi_new = phi.clone() if phi is not None else torch.zeros_like(rho_new)
    
    # Return new state dictionary
    return {
        'rho': rho_new,
        'velocities': velocities_new,
        'momenta': momenta_new,
        'phi': phi_new
    }


def initialize_arrays_torch(shape, dimension):
    """
    Initialize arrays for LAX solver (PyTorch version).
    
    Args:
        shape: Tuple of grid dimensions (nx, ny) or (nx, ny, nz)
        dimension: 2 or 3
    
    Returns:
        dict: {
            'rho': density tensor,
            'velocities': [vx, vy, ...] list of velocity tensors,
            'momenta': [Px, Py, ...] list of momentum tensors,
            'phi': potential tensor
        }
    """
    if dimension == 2:
        nx, ny = shape
        arrays = {
            'rho': torch.zeros((nx, ny), device=device, dtype=dtype),
            'velocities': [torch.zeros((nx, ny), device=device, dtype=dtype), 
                          torch.zeros((nx, ny), device=device, dtype=dtype)],  # vx, vy
            'momenta': [torch.zeros((nx, ny), device=device, dtype=dtype), 
                       torch.zeros((nx, ny), device=device, dtype=dtype)],  # Px, Py
            'phi': torch.zeros((nx, ny), device=device, dtype=dtype)
        }
    elif dimension == 3:
        nx, ny, nz = shape
        arrays = {
            'rho': torch.zeros((nx, ny, nz), device=device, dtype=dtype),
            'velocities': [torch.zeros((nx, ny, nz), device=device, dtype=dtype),
                          torch.zeros((nx, ny, nz), device=device, dtype=dtype),
                          torch.zeros((nx, ny, nz), device=device, dtype=dtype)],  # vx, vy, vz
            'momenta': [torch.zeros((nx, ny, nz), device=device, dtype=dtype),
                       torch.zeros((nx, ny, nz), device=device, dtype=dtype),
                       torch.zeros((nx, ny, nz), device=device, dtype=dtype)],  # Px, Py, Pz
            'phi': torch.zeros((nx, ny, nz), device=device, dtype=dtype)
        }
    else:
        raise ValueError(f"Unsupported dimension={dimension}. Use 2 or 3.")
    
    return arrays


# ============================================================================
# Initial Conditions Setup Functions
# ============================================================================

def setup_sinusoidal_ic_torch(domain_params, physics_params, dimension, xx=None, yy=None, zz=None):
    """
    Set up sinusoidal initial conditions with KX, KY, KZ wave patterns (PyTorch version).
    
    Args:
        domain_params: DomainParams dataclass or dict with Lx, Ly, (Lz), nx, ny, (nz)
        physics_params: Dict with rho_o, rho_1, lam, c_s, gravity, jeans, KX, KY, (KZ)
        dimension: 2 or 3
        xx, yy, zz: Coordinate meshes (optional, will be created if not provided)
    
    Returns:
        dict: {
            'rho': density field,
            'vx': x-velocity field,
            'vy': y-velocity field,
            'vz': z-velocity field (3D only),
            'v_1': velocity perturbation amplitude,
            'alpha': growth/oscillation rate (if gravity),
            'is_unstable': bool (if gravity)
        }
    """
    # Extract parameters
    if isinstance(domain_params, dict):
        Lx, Ly = domain_params['Lx'], domain_params['Ly']
        nx, ny = domain_params['nx'], domain_params['ny']
        if dimension == 3:
            Lz, nz = domain_params['Lz'], domain_params['nz']
    else:
        Lx, Ly = domain_params.Lx, domain_params.Ly
        nx, ny = domain_params.nx, domain_params.ny
        if dimension == 3:
            Lz, nz = domain_params.Lz, domain_params.nz
    
    rho_o = physics_params['rho_o']
    rho_1 = physics_params['rho_1']
    lam = physics_params['lam']
    c_s = physics_params['c_s']
    gravity = physics_params.get('gravity', False)
    jeans = physics_params.get('jeans', None)
    KX = physics_params.get('KX', 2*np.pi/lam)
    KY = physics_params.get('KY', 0.0)
    KZ = physics_params.get('KZ', 0.0) if dimension == 3 else 0.0
    
    # Create coordinate meshes if not provided
    if xx is None:
        x = torch.linspace(0, Lx, nx+1, device=device, dtype=dtype)[:-1]
        y = torch.linspace(0, Ly, ny+1, device=device, dtype=dtype)[:-1]
        if dimension == 2:
            xx, yy = torch.meshgrid(x, y, indexing='ij')
        else:
            z = torch.linspace(0, Lz, nz+1, device=device, dtype=dtype)[:-1]
            xx, yy, zz = torch.meshgrid(x, y, z, indexing='ij')
    
    # Set up density
    KX_tensor = torch.tensor(KX, device=device, dtype=dtype)
    KY_tensor = torch.tensor(KY, device=device, dtype=dtype)
    if dimension == 2:
        rho = rho_o + rho_1 * torch.cos(KX_tensor * xx + KY_tensor * yy)
    else:
        KZ_tensor = torch.tensor(KZ, device=device, dtype=dtype)
        rho = rho_o + rho_1 * torch.cos(KX_tensor * xx + KY_tensor * yy + KZ_tensor * zz)
    
    # Compute velocity perturbation
    v_1, alpha, is_unstable = compute_perturbation_velocity_torch(rho_1, rho_o, lam, c_s, jeans, gravity)
    
    # Set up velocity fields
    if dimension == 3:
        KZ_tensor = torch.tensor(KZ, device=device, dtype=dtype)
        k_magnitude = torch.sqrt(KX_tensor**2 + KY_tensor**2 + KZ_tensor**2)
    else:
        k_magnitude = torch.sqrt(KX_tensor**2 + KY_tensor**2)
    
    if dimension == 2:
        if gravity and is_unstable:
            # Unstable: use sin
            wave_field = -v_1 * torch.sin(KX_tensor * xx + KY_tensor * yy)
        else:
            # Stable or no gravity: use cos
            wave_field = v_1 * torch.cos(KX_tensor * xx + KY_tensor * yy)
        
        if k_magnitude > 0:
            vx = wave_field * (KX_tensor / k_magnitude)
            vy = wave_field * (KY_tensor / k_magnitude)
        else:
            vx = wave_field
            vy = torch.zeros_like(xx)
        
        result = {
            'rho': rho,
            'vx': vx,
            'vy': vy,
            'v_1': v_1,
            'alpha': alpha,
            'is_unstable': is_unstable
        }
    else:  # dimension == 3
        if gravity and is_unstable:
            wave_field = -v_1 * torch.sin(KX_tensor * xx + KY_tensor * yy + KZ_tensor * zz)
        else:
            wave_field = v_1 * torch.cos(KX_tensor * xx + KY_tensor * yy + KZ_tensor * zz)
        
        if k_magnitude > 0:
            vx = wave_field * (KX_tensor / k_magnitude)
            vy = wave_field * (KY_tensor / k_magnitude)
            vz = wave_field * (KZ_tensor / k_magnitude)
        else:
            vx = wave_field
            vy = torch.zeros_like(vx)
            vz = torch.zeros_like(vx)
        
        result = {
            'rho': rho,
            'vx': vx,
            'vy': vy,
            'vz': vz,
            'v_1': v_1,
            'alpha': alpha,
            'is_unstable': is_unstable
        }
    
    return result


def setup_power_spectrum_ic_torch(domain_params, ps_params, dimension, vx0_shared=None, vy0_shared=None, vz0_shared=None):
    """
    Set up power spectrum initial conditions (PyTorch version).
    
    Args:
        domain_params: DomainParams dataclass or dict with Lx, Ly, (Lz), nx, ny, (nz)
        ps_params: Dict with rho_o, power_index, amplitude, random_seed
        dimension: 2 or 3
        vx0_shared, vy0_shared, vz0_shared: Optional pre-generated velocity fields (numpy arrays)
    
    Returns:
        dict: {
            'rho': density field (uniform),
            'vx': x-velocity field,
            'vy': y-velocity field,
            'vz': z-velocity field (3D only)
        }
    """
    # Validate dimension
    if dimension not in [2, 3]:
        raise ValueError(f"setup_power_spectrum_ic_torch() only supports dimension=2 or 3, got {dimension}")
    
    # Extract parameters
    if isinstance(domain_params, dict):
        Lx, Ly = domain_params['Lx'], domain_params['Ly']
        nx, ny = domain_params['nx'], domain_params['ny']
        if dimension == 3:
            if 'Lz' not in domain_params or 'nz' not in domain_params:
                raise ValueError("For dimension=3, domain_params must contain 'Lz' and 'nz'")
            Lz, nz = domain_params['Lz'], domain_params['nz']
    else:
        Lx, Ly = domain_params.Lx, domain_params.Ly
        nx, ny = domain_params.nx, domain_params.ny
        if dimension == 3:
            if not hasattr(domain_params, 'Lz') or not hasattr(domain_params, 'nz'):
                raise ValueError("For dimension=3, domain_params must have 'Lz' and 'nz' attributes")
            Lz, nz = domain_params.Lz, domain_params.nz
    
    rho_o = ps_params['rho_o']
    power_index = ps_params.get('power_index', -3.0)
    amplitude = ps_params.get('amplitude', 0.02)
    random_seed = ps_params.get('random_seed', None)
    
    # Uniform density
    if dimension == 2:
        rho = rho_o * torch.ones((nx, ny), device=device, dtype=dtype)
    else:
        rho = rho_o * torch.ones((nx, ny, nz), device=device, dtype=dtype)
    
    # Generate or use shared velocity fields
    if vx0_shared is not None and vy0_shared is not None:
        vx = torch.from_numpy(vx0_shared.copy()).to(device=device, dtype=dtype)
        vy = torch.from_numpy(vy0_shared.copy()).to(device=device, dtype=dtype)
        if dimension == 2:
            return {'rho': rho, 'vx': vx, 'vy': vy}
        else:
            if vz0_shared is not None:
                vz = torch.from_numpy(vz0_shared.copy()).to(device=device, dtype=dtype)
                return {'rho': rho, 'vx': vx, 'vy': vy, 'vz': vz}
    
    # Generate new velocity fields
    if dimension == 2:
        vx, vy = generate_velocity_field_power_spectrum_torch(nx, ny, Lx, Ly, power_index, amplitude, DIMENSION=2, random_seed=random_seed)
        return {'rho': rho, 'vx': vx, 'vy': vy}
    else:
        vx, vy, vz = generate_velocity_field_power_spectrum_torch(nx, ny, Lx, Ly, power_index, amplitude, DIMENSION=3, random_seed=random_seed, nz=nz, Lz=Lz)
        return {'rho': rho, 'vx': vx, 'vy': vy, 'vz': vz}


def setup_warm_start_ic_torch(provided_fields):
    """
    Set up initial conditions from provided fields (for PINN integration, PyTorch version).
    
    Args:
        provided_fields: Dict with 'rho', 'vx', 'vy', ('vz' for 3D) - can be numpy arrays or torch tensors
    
    Returns:
        dict: Same structure as input, with arrays converted to torch tensors
    """
    result = {}
    for key, value in provided_fields.items():
        if value is not None:
            if isinstance(value, torch.Tensor):
                result[key] = value.clone()
            elif isinstance(value, np.ndarray):
                result[key] = torch.from_numpy(value.copy()).to(device=device, dtype=dtype)
            else:
                result[key] = value
        else:
            result[key] = None
    
    return result


def lax_solver_torch(time, domain_params, physics_params, ic_type='sinusoidal', ic_params=None, options=None, save_times=None):
    """
    Unified master solver for 1D, 2D, and 3D LAX method (PyTorch version).
    
    This function provides a single entry point for running LAX simulations with different
    initial condition types and options. It automatically determines the dimension from
    domain_params and handles all setup and time-stepping internally.
    
    Args:
        time: Final simulation time
        domain_params: DomainParams dataclass or dict with:
            - Lx, Ly, (Lz for 3D): Domain sizes
            - nx, ny, (nz for 3D): Grid resolutions
        physics_params: Dict with:
            - c_s: Sound speed
            - rho_o: Background density
            - const: Constant factor (for Poisson solver)
            - G: Gravitational constant
            - rho_1: Density perturbation amplitude (for sinusoidal IC)
            - lam: Wavelength (for sinusoidal IC)
        ic_type: 'sinusoidal' | 'power_spectrum' | 'warm_start'
        ic_params: Dict with IC-specific parameters:
            For 'sinusoidal': KX, KY, (KZ for 3D) - optional, defaults from config
            For 'power_spectrum': power_index, amplitude, random_seed, vx0_shared, vy0_shared, (vz0_shared for 3D)
            For 'warm_start': provided_fields dict with 'rho', 'vx', 'vy', ('vz' for 3D)
        options: Dict with:
            - gravity: bool (default: True)
            - nu: Courant number (default: 0.5)
            - comparison: bool (default: False)
            - isplot: bool (default: False)
        save_times: Optional list of times to save snapshots during integration.
                   If provided, returns dict {time: SimulationResult} instead of single SimulationResult.
                   If None, returns single SimulationResult at final time.
    
    Returns:
        If save_times is None: SimulationResult at final time
        If save_times is provided: Dict mapping time -> SimulationResult for each saved time
    
    Raises:
        ValueError: If input validation fails
    """
    # ========================================================================
    # Input Validation
    # ========================================================================
    
    # Validate time
    if not isinstance(time, (int, float)) or time < 0:
        raise ValueError(f"time must be a non-negative number, got {time}")
    
    # Validate ic_type
    allowed_ic_types = ['sinusoidal', 'power_spectrum', 'warm_start']
    if ic_type not in allowed_ic_types:
        raise ValueError(f"ic_type must be one of {allowed_ic_types}, got '{ic_type}'")
    
    # Validate domain_params
    if isinstance(domain_params, dict):
        required_domain_keys = ['Lx', 'nx']
        for key in required_domain_keys:
            if key not in domain_params:
                raise ValueError(f"domain_params missing required key: '{key}'")
        
        Lx = domain_params['Lx']
        nx = domain_params['nx']
        Ly = domain_params.get('Ly', None)
        ny = domain_params.get('ny', None)
        Lz = domain_params.get('Lz', None)
        nz = domain_params.get('nz', None)
    else:
        if not hasattr(domain_params, 'Lx') or not hasattr(domain_params, 'nx'):
            raise ValueError("domain_params must have Lx and nx attributes")
        Lx = domain_params.Lx
        nx = domain_params.nx
        Ly = getattr(domain_params, 'Ly', None)
        ny = getattr(domain_params, 'ny', None)
        Lz = getattr(domain_params, 'Lz', None)
        nz = getattr(domain_params, 'nz', None)
    
    # Validate domain parameter values
    if not isinstance(Lx, (int, float)) or Lx <= 0:
        raise ValueError(f"Lx must be a positive number, got {Lx}")
    if not isinstance(nx, int) or nx <= 0:
        raise ValueError(f"nx must be a positive integer, got {nx}")
    
    # Determine dimension and validate dimension-specific parameters
    if Lz is not None and nz is not None:
        dimension = 3
        if Ly is None or ny is None:
            raise ValueError("For 3D simulations, Ly and ny must be provided")
        if not isinstance(Ly, (int, float)) or Ly <= 0:
            raise ValueError(f"Ly must be a positive number, got {Ly}")
        if not isinstance(ny, int) or ny <= 0:
            raise ValueError(f"ny must be a positive integer, got {ny}")
        if not isinstance(Lz, (int, float)) or Lz <= 0:
            raise ValueError(f"Lz must be a positive number, got {Lz}")
        if not isinstance(nz, int) or nz <= 0:
            raise ValueError(f"nz must be a positive integer, got {nz}")
    elif Ly is not None and ny is not None:
        dimension = 2
        if not isinstance(Ly, (int, float)) or Ly <= 0:
            raise ValueError(f"Ly must be a positive number, got {Ly}")
        if not isinstance(ny, int) or ny <= 0:
            raise ValueError(f"ny must be a positive integer, got {ny}")
    else:
        dimension = 1
    
    # Validate physics_params
    if not isinstance(physics_params, dict):
        raise ValueError(f"physics_params must be a dict, got {type(physics_params)}")
    
    required_physics_keys = ['c_s', 'rho_o', 'const', 'G']
    for key in required_physics_keys:
        if key not in physics_params:
            raise ValueError(f"physics_params missing required key: '{key}'")
        if not isinstance(physics_params[key], (int, float)) or physics_params[key] <= 0:
            raise ValueError(f"physics_params['{key}'] must be a positive number, got {physics_params[key]}")
    
    c_s = physics_params['c_s']
    rho_o = physics_params['rho_o']
    const = physics_params['const']
    G = physics_params['G']
    
    # Validate ic_params for warm_start
    if ic_type == 'warm_start':
        if ic_params is None:
            raise ValueError("ic_params must be provided for warm_start ic_type")
        if 'provided_fields' not in ic_params:
            raise ValueError("ic_params must contain 'provided_fields' for warm_start ic_type")
        provided_fields = ic_params['provided_fields']
        if not isinstance(provided_fields, dict):
            raise ValueError("ic_params['provided_fields'] must be a dict")
        if 'rho' not in provided_fields:
            raise ValueError("provided_fields must contain 'rho'")
        if dimension >= 2 and 'vy' not in provided_fields:
            raise ValueError(f"provided_fields must contain 'vy' for {dimension}D simulations")
        if dimension == 3 and 'vz' not in provided_fields:
            raise ValueError("provided_fields must contain 'vz' for 3D simulations")
    
    # Validate options
    if options is not None and not isinstance(options, dict):
        raise ValueError(f"options must be a dict or None, got {type(options)}")
    
    # Parse options
    if options is None:
        options = {}
    gravity = options.get('gravity', True)
    nu = options.get('nu', 0.5)
    comparison = options.get('comparison', False)
    isplot = options.get('isplot', False)
    
    # Validate option values
    if not isinstance(gravity, bool):
        raise ValueError(f"options['gravity'] must be a bool, got {type(gravity)}")
    if not isinstance(nu, (int, float)) or nu <= 0 or nu > 1:
        raise ValueError(f"options['nu'] (Courant number) must be in (0, 1], got {nu}")
    if not isinstance(comparison, bool):
        raise ValueError(f"options['comparison'] must be a bool, got {type(comparison)}")
    if not isinstance(isplot, bool):
        raise ValueError(f"options['isplot'] must be a bool, got {type(isplot)}")
    
    # Validate save_times
    if save_times is not None:
        if not isinstance(save_times, (list, tuple, np.ndarray)):
            raise ValueError(f"save_times must be a list, tuple, or numpy array, got {type(save_times)}")
        save_times = np.array(save_times)
        if len(save_times) == 0:
            raise ValueError("save_times cannot be empty")
        if np.any(save_times < 0):
            raise ValueError("save_times must contain non-negative values")
        if np.any(save_times > time):
            raise ValueError(f"save_times contains values greater than final time={time}")
        save_times = np.sort(np.unique(save_times))  # Sort and remove duplicates
        snapshots = {}  # Dictionary to store snapshots
    
    # Compute grid spacings
    dx = Lx / nx
    grid_spacings = [dx]
    if dimension >= 2:
        dy = Ly / ny
        grid_spacings.append(dy)
    if dimension == 3:
        dz = Lz / nz
        grid_spacings.append(dz)
    
    # Compute Jeans length if gravity is enabled
    jeans = None
    if gravity:
        jeans = compute_jeans_length_torch(c_s, rho_o, const, G)
    
    # Create coordinate arrays first
    x = torch.linspace(0, Lx, nx+1, device=device, dtype=dtype)[:-1]
    if dimension >= 2:
        y = torch.linspace(0, Ly, ny+1, device=device, dtype=dtype)[:-1]
    if dimension == 3:
        z = torch.linspace(0, Lz, nz+1, device=device, dtype=dtype)[:-1]
    
    # Set up initial conditions based on ic_type
    if ic_type == 'sinusoidal':
        rho_1 = physics_params.get('rho_1', 0.1)
        lam = physics_params.get('lam', 7.0)
        
        if dimension == 1:
            # 1D sinusoidal IC
            rho0 = rho_o + rho_1 * torch.cos(2*np.pi*x/lam)
            v_1, alpha, is_unstable = compute_perturbation_velocity_torch(rho_1, rho_o, lam, c_s, jeans, gravity)
            if gravity and is_unstable:
                vx0 = -v_1 * torch.sin(2*np.pi*x/lam)
            else:
                vx0 = v_1 * torch.cos(2*np.pi*x/lam)
            velocities = [vx0]
        else:
            # 2D/3D sinusoidal IC
            setup_physics = {
                'rho_o': rho_o,
                'rho_1': rho_1,
                'lam': lam,
                'c_s': c_s,
                'gravity': gravity,
                'jeans': jeans
            }
            if ic_params:
                setup_physics.update(ic_params)
            else:
                # Use defaults from config
                setup_physics['KX'] = KX
                setup_physics['KY'] = KY
                if dimension == 3:
                    setup_physics['KZ'] = KZ
            
            ic_result = setup_sinusoidal_ic_torch(domain_params, setup_physics, dimension)
            rho0 = ic_result['rho']
            if dimension == 2:
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                velocities = [vx0, vy0]
            else:  # dimension == 3
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                vz0 = ic_result['vz']
                velocities = [vx0, vy0, vz0]
    
    elif ic_type == 'power_spectrum':
        if dimension == 1:
            # 1D power spectrum IC - uniform density, zero velocity for now
            # (1D power spectrum velocity generation not implemented)
            rho0 = rho_o * torch.ones(nx, device=device, dtype=dtype)
            vx0 = torch.zeros(nx, device=device, dtype=dtype)
            velocities = [vx0]
        else:
            # 2D/3D power spectrum IC
            ps_params = {
                'rho_o': rho_o,
                'power_index': ic_params.get('power_index', -3.0) if ic_params else -3.0,
                'amplitude': ic_params.get('amplitude', 0.02) if ic_params else 0.02,
                'random_seed': ic_params.get('random_seed', None) if ic_params else None
            }
            vx0_shared = ic_params.get('vx0_shared', None) if ic_params else None
            vy0_shared = ic_params.get('vy0_shared', None) if ic_params else None
            vz0_shared = ic_params.get('vz0_shared', None) if ic_params else None
            
            ic_result = setup_power_spectrum_ic_torch(domain_params, ps_params, dimension, 
                                                     vx0_shared, vy0_shared, vz0_shared)
            rho0 = ic_result['rho']
            if dimension == 2:
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                velocities = [vx0, vy0]
            else:  # dimension == 3
                # Validate that all 3D velocity components are present
                if 'vz' not in ic_result:
                    raise ValueError(f"setup_power_spectrum_ic_torch() did not return 'vz' for dimension=3. "
                                   f"Result keys: {list(ic_result.keys())}")
                vx0 = ic_result['vx']
                vy0 = ic_result['vy']
                vz0 = ic_result['vz']
                velocities = [vx0, vy0, vz0]
    
    elif ic_type == 'warm_start':
        ic_result = setup_warm_start_ic_torch(ic_params['provided_fields'])
        rho0 = ic_result['rho']
        if dimension == 1:
            vx0 = ic_result.get('vx', torch.zeros_like(rho0))
            velocities = [vx0]
        elif dimension == 2:
            vx0 = ic_result['vx']
            vy0 = ic_result['vy']
            velocities = [vx0, vy0]
        else:  # dimension == 3
            vx0 = ic_result['vx']
            vy0 = ic_result['vy']
            vz0 = ic_result['vz']
            velocities = [vx0, vy0, vz0]
    
    else:
        raise ValueError(f"Unsupported ic_type: {ic_type}. Use 'sinusoidal', 'power_spectrum', or 'warm_start'")
    
    # Initialize momenta
    momenta = [rho0 * v for v in velocities]
    
    # Initialize gravitational potential
    if gravity:
        if dimension == 1:
            # 1D Poisson solver
            k = 2*np.pi*torch.fft.fftfreq(nx, d=dx).to(device)
            rhohat = torch.fft.fft(const*(rho0 - rho_o))
            denom = -(k**2)
            denom[0] = 1.0
            phihat = rhohat / denom
            phihat[0] = 0.0
            phi0 = torch.real(torch.fft.ifft(phihat))
        elif dimension == 2:
            phi0 = fft_solver_torch(const*(rho0 - rho_o), Lx, nx, Ly, ny)
        else:  # dimension == 3
            phi0 = fft_solver_torch_3d(const*(rho0 - rho_o), Lx, nx, Ly, ny, Lz, nz)
    else:
        phi0 = torch.zeros_like(rho0)
    
    
    # Prepare params_dict for lax_time_step_torch
    params_dict = {
        'Lx': Lx,
        'nx': nx,
        'c_s': c_s,
        'rho_o': rho_o,
        'const': const
    }
    if dimension >= 2:
        params_dict['Ly'] = Ly
        params_dict['ny'] = ny
    if dimension == 3:
        params_dict['Lz'] = Lz
        params_dict['nz'] = nz
    
    # Initialize state dictionary
    state = {
        'rho': rho0,
        'velocities': velocities,
        'momenta': momenta,
        'phi': phi0
    }
    
    # Time-stepping loop
    t = 0.0
    n_steps = 0
    
    # Initial timestep
    dt = compute_adaptive_timestep_torch(velocities, c_s, dx, nu, include_sound_speed=True)
    
    # Save initial state if t=0 is in save_times
    if save_times is not None and len(save_times) > 0 and save_times[0] == 0.0:
        # Convert initial state to SimulationResult
        density_np = state['rho'].cpu().numpy()
        velocity_components_np = [v.cpu().numpy() for v in state['velocities']]
        coordinates = {'x': x.cpu().numpy()}
        if dimension >= 2:
            coordinates['y'] = y.cpu().numpy()
        if dimension == 3:
            coordinates['z'] = z.cpu().numpy()
        potential_np = state['phi'].cpu().numpy() if gravity else None
        snapshots[0.0] = SimulationResult(
            dimension=dimension,
            density=density_np,
            velocity_components=velocity_components_np,
            coordinates=coordinates,
            metadata={'time': 0.0, 'iterations': 0, 'rho_max': torch.max(state['rho']).item(),
                     'gravity': gravity, 'nu': nu, 'dimension': dimension},
            potential=potential_np,
            linear_theory_comparison=None
        )
    
    while t < time:
        # Ensure last step doesn't overshoot
        if t + dt > time:
            dt = time - t
        
        # Perform one time step
        state = lax_time_step_torch(state, dt, params_dict, dimension, gravity=gravity)
        
        # Update time and step counter
        t += dt
        n_steps += 1
        
        # Check if we should save a snapshot after this step
        if save_times is not None:
            for save_t in save_times:
                # Save if we've passed this time point (with small tolerance for floating point)
                if abs(t - save_t) < 1e-9 or (t > save_t and save_t not in snapshots):
                    # Save current state after stepping
                    density_np = state['rho'].cpu().numpy()
                    velocity_components_np = [v.cpu().numpy() for v in state['velocities']]
                    coordinates = {'x': x.cpu().numpy()}
                    if dimension >= 2:
                        coordinates['y'] = y.cpu().numpy()
                    if dimension == 3:
                        coordinates['z'] = z.cpu().numpy()
                    potential_np = state['phi'].cpu().numpy() if gravity else None
                    snapshots[save_t] = SimulationResult(
                        dimension=dimension,
                        density=density_np,
                        velocity_components=velocity_components_np,
                        coordinates=coordinates,
                        metadata={'time': t, 'iterations': n_steps, 'rho_max': torch.max(state['rho']).item(),
                                 'gravity': gravity, 'nu': nu, 'dimension': dimension},
                        potential=potential_np,
                        linear_theory_comparison=None
                    )
        
        # Calculate dt for next step
        dt = compute_adaptive_timestep_torch(state['velocities'], c_s, dx, nu)
    
    # Convert tensors to numpy for return
    density_np = state['rho'].cpu().numpy()
    velocity_components_np = [v.cpu().numpy() for v in state['velocities']]
    
    # Prepare coordinates dictionary
    coordinates = {'x': x.cpu().numpy()}
    if dimension >= 2:
        coordinates['y'] = y.cpu().numpy()
    if dimension == 3:
        coordinates['z'] = z.cpu().numpy()
    
    # Prepare metadata dictionary
    metadata = {
        'time': t,
        'iterations': n_steps,
        'rho_max': torch.max(state['rho']).item(),
        'gravity': gravity,
        'nu': nu,
        'dimension': dimension
    }
    
    # Prepare linear theory comparison if requested
    linear_theory_comparison = None
    if comparison:
        # Linear theory comparison would be computed here if needed
        # For now, set to None as it's optional
        linear_theory_comparison = {}
    
    # Prepare potential (convert to numpy if present)
    potential_np = None
    if gravity:
        potential_np = state['phi'].cpu().numpy()
    
    # Create final SimulationResult
    final_result = SimulationResult(
        dimension=dimension,
        density=density_np,
        velocity_components=velocity_components_np,
        coordinates=coordinates,
        metadata=metadata,
        potential=potential_np,
        linear_theory_comparison=linear_theory_comparison
    )
    
    # Return snapshots dict if save_times was provided, otherwise return single result
    if save_times is not None:
        # Ensure final time is saved if it's in save_times
        if time in save_times and time not in snapshots:
            snapshots[time] = final_result
        # Also ensure we have the final result even if not explicitly requested
        if time not in snapshots:
            snapshots[time] = final_result
        return snapshots
    else:
        return final_result


def fft_solver_torch(rho, Lx, nx, Ly, ny):
    """
    PyTorch FFT solver for Poisson equation (gravitational potential).
    """
    dx = Lx / nx
    dy = Ly / ny
    
    # Calculate the Fourier modes of the gas density
    rhohat = torch.fft.fft2(rho)
    
    # Calculate the wave numbers in x and y directions
    kx = 2 * np.pi * torch.fft.fftfreq(nx, d=dx).to(device)
    ky = 2 * np.pi * torch.fft.fftfreq(ny, d=dy).to(device)
    
    # Construct the discrete Laplacian operator in Fourier space
    # This ensures consistency with finite-difference gradients used in LAX scheme
    kx_mesh, ky_mesh = torch.meshgrid(kx, ky, indexing='xy')
    # Transpose to match FFT2 output layout (nx, ny)
    laplace = (2*(torch.cos(kx_mesh.T*dx)-1)/(dx**2) + 
               2*(torch.cos(ky_mesh.T*dy)-1)/(dy**2))
    
    # Handle zero mode (k=0) - set to small value to avoid division by zero
    laplace = torch.where(laplace == 0, torch.tensor(1e-9, device=device, dtype=dtype), laplace)
    
    # Solve for the potential in Fourier space
    phihat = rhohat / laplace
    
    # Transform back to real space
    phi = torch.real(torch.fft.ifft2(phihat))
    
    return phi

def fft_solver_torch_3d(rho, Lx, nx, Ly, ny, Lz, nz):
    dx = Lx / nx
    dy = Ly / ny
    dz = Lz / nz
    rhohat = torch.fft.fftn(rho)
    kx = 2 * np.pi * torch.fft.fftfreq(nx, d=dx).to(device)
    ky = 2 * np.pi * torch.fft.fftfreq(ny, d=dy).to(device)
    kz = 2 * np.pi * torch.fft.fftfreq(nz, d=dz).to(device)
    kx_mesh, ky_mesh, kz_mesh = torch.meshgrid(kx, ky, kz, indexing='ij')
    # Use discrete Laplacian for consistency with finite-difference scheme
    laplace = (2*(torch.cos(kx_mesh*dx)-1)/(dx**2) + 
               2*(torch.cos(ky_mesh*dy)-1)/(dy**2) + 
               2*(torch.cos(kz_mesh*dz)-1)/(dz**2))
    laplace = torch.where(laplace == 0, torch.tensor(1e-9, device=device, dtype=dtype), laplace)
    phihat = rhohat / laplace
    phi = torch.real(torch.fft.ifftn(phihat))
    return phi

def generate_velocity_field_power_spectrum_torch(nx, ny, Lx, Ly, power_index=-3.0, amplitude=0.02, DIMENSION=2, random_seed=None, nz=None, Lz=None):
    """
    PyTorch implementation for generating 2D or 3D velocity fields with a power-law spectrum.
    Dimension-agnostic implementation using fftn/ifftn.
    
    Args:
        nx, ny: Grid dimensions (required)
        Lx, Ly: Domain sizes (required)
        power_index: Power spectrum index (default: -3.0)
        amplitude: RMS amplitude of velocity field (default: 0.02)
        DIMENSION: 2 or 3 (default: 2)
        random_seed: Random seed for reproducibility
        nz, Lz: Grid dimension and domain size for 3D (required if DIMENSION=3)
    
    Returns:
        For 2D: (vx, vy)
        For 3D: (vx, vy, vz)
    """
    # Use NumPy's random generator for consistency with CPU solver
    if random_seed is not None:
        rng = np.random.default_rng(random_seed)
    else:
        rng = np.random.default_rng()

    # Determine shape and domain lengths based on dimension
    if DIMENSION == 2:
        shape = (nx, ny)
        domain_lengths = (Lx, Ly)
    elif DIMENSION == 3:
        if nz is None or Lz is None:
            raise ValueError("nz and Lz must be provided for 3D")
        shape = (nx, ny, nz)
        domain_lengths = (Lx, Ly, Lz)
    else:
        raise ValueError(f"Unsupported DIMENSION={DIMENSION}. Use 2 or 3.")

    def synthesize_component():
        # 1. Generate random field at target resolution using NumPy (for consistency)
        field_np = rng.standard_normal(shape)
        field = torch.from_numpy(field_np).to(device=device, dtype=dtype)
        F = torch.fft.fftn(field)  # fftn works for any dimension
        
        # 2. Construct k-space grid at target resolution
        k_grid = build_k_space_grid_torch(shape, domain_lengths, DIMENSION)
        
        # 3. Compute |k| magnitude (dimension-agnostic)
        if DIMENSION == 2:
            kx, ky, kxg, kyg = k_grid
            kk = torch.sqrt(kxg**2 + kyg**2)
        else:  # DIMENSION == 3
            kx, ky, kz, kxg, kyg, kzg = k_grid
            kk = torch.sqrt(kxg**2 + kyg**2 + kzg**2)
        
        # 4. Apply power-law filter (avoid computing power when kk == 0)
        filt = torch.zeros_like(kk)
        mask = kk > 0
        filt[mask] = kk[mask]**(power_index / 2.0)
        
        F_filtered = F * filt
        
        # 5. Transform back to real space (dimension-agnostic)
        comp = torch.real(torch.fft.ifftn(F_filtered))
        
        # 6. Normalize the field
        comp -= torch.mean(comp)
        std = torch.std(comp)
        if std > 0:
            comp = comp * (amplitude / std)
        
        return comp

    # Generate velocity components
    vx0 = synthesize_component()
    vy0 = synthesize_component()
    
    if DIMENSION == 3:
        vz0 = synthesize_component()
        return vx0, vy0, vz0
    else:
        return vx0, vy0

def lax_solution_torch(time_val, N, nu, lam, num_of_waves, rho_1, gravity=False, use_velocity_ps=False, 
                         ps_index=-3.0, vel_rms=0.02, random_seed=None, vx0_shared=None, vy0_shared=None):
    """
    DEPRECATED: This function is a wrapper around the unified lax_solver_torch().
    Use lax_solver_torch() directly for new code.
    
    PyTorch implementation of the LAX method for solving hydrodynamic equations.
    This version is designed to run on a GPU for accelerated computation.
    
    Args:
        vx0_shared: Optional pre-generated vx velocity field (numpy array) for consistent ICs
        vy0_shared: Optional pre-generated vy velocity field (numpy array) for consistent ICs
    """
    import warnings
    warnings.warn(
        "lax_solution_torch() is deprecated. Use lax_solver_torch() instead. "
        "This function will be removed in a future version.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # Convert parameters to unified solver format
    Lx = Ly = lam * num_of_waves
    Nx = Ny = N
    c_s = cs
    
    domain_params = {'Lx': Lx, 'Ly': Ly, 'nx': Nx, 'ny': Ny}
    physics_params = {
        'c_s': c_s,
        'rho_o': rho_o,
        'const': const,
        'G': G,
        'rho_1': rho_1,
        'lam': lam
    }
    
    # Set up IC parameters
    ic_type = 'power_spectrum' if use_velocity_ps else 'sinusoidal'
    ic_params = None
    if use_velocity_ps:
        ic_params = {
            'power_index': ps_index,
            'amplitude': vel_rms,
            'random_seed': random_seed,
            'vx0_shared': vx0_shared,
            'vy0_shared': vy0_shared
        }
    else:
        ic_params = {
            'KX': KX,
            'KY': KY
        }
    
    options = {
        'gravity': gravity,
        'nu': nu,
        'comparison': False,
        'isplot': False
    }
    
    # Call unified solver
    result = lax_solver_torch(time_val, domain_params, physics_params, ic_type=ic_type, ic_params=ic_params, options=options)
    
    # Extract values to match old return format
    x = result.coordinates['x']
    y = result.coordinates['y']
    rho0 = result.density
    vx0, vy0 = result.velocity_components
    n = result.metadata['iterations']
    rho_max = result.metadata['rho_max']
    
    return x, rho0, vx0, vy0, None, n, rho_max


def lax_solution_3d_sinusoidal_torch(time_val, N, nu, lam, num_of_waves, rho_1, gravity=True,
                                     use_velocity_ps=False, ps_index=-3.0, vel_rms=0.02, 
                                     random_seed=None, vx0_shared=None, vy0_shared=None, vz0_shared=None):
    """
    DEPRECATED: This function is a wrapper around the unified lax_solver_torch().
    Use lax_solver_torch() with dimension=3 instead.
    
    3D LAX solver for sinusoidal perturbations with optional self-gravity (PyTorch version).
    Returns full 3D fields which can be sliced for visualization/comparison.
    
    Args:
        time_val: Final simulation time
        N: Grid resolution (Nx = Ny = Nz = N)
        nu: Courant number
        lam: Wavelength
        num_of_waves: Number of waves in domain
        rho_1: Density perturbation amplitude
        gravity: Whether to include self-gravity (default: True)
        use_velocity_ps: If True, use power spectrum velocity initialization instead of sinusoidal (default: False)
        ps_index: Power spectrum index (default: -3.0)
        vel_rms: RMS velocity amplitude (default: 0.02)
        random_seed: Random seed for reproducibility (default: None)
        vx0_shared: Optional pre-generated x-velocity field (numpy array, default: None)
        vy0_shared: Optional pre-generated y-velocity field (numpy array, default: None)
        vz0_shared: Optional pre-generated z-velocity field (numpy array, default: None)
    """
    import warnings
    warnings.warn(
        "lax_solution_3d_sinusoidal_torch() is deprecated. Use lax_solver_torch() with dimension=3 instead. "
        "This function will be removed in a future version.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # Convert parameters to unified solver format
    Lx = Ly = Lz = lam * num_of_waves
    Nx = Ny = Nz = int(N)
    c_s = cs
    
    domain_params = {'Lx': Lx, 'Ly': Ly, 'Lz': Lz, 'nx': Nx, 'ny': Ny, 'nz': Nz}
    physics_params = {
        'c_s': c_s,
        'rho_o': rho_o,
        'const': const,
        'G': G,
        'rho_1': rho_1,
        'lam': lam
    }
    
    # Set up IC parameters based on use_velocity_ps flag
    ic_type = 'power_spectrum' if use_velocity_ps else 'sinusoidal'
    ic_params = None
    if use_velocity_ps:
        ic_params = {
            'power_index': ps_index,
            'amplitude': vel_rms,
            'random_seed': random_seed,
            'vx0_shared': vx0_shared,
            'vy0_shared': vy0_shared,
            'vz0_shared': vz0_shared
        }
    else:
        ic_params = {
            'KX': KX,
            'KY': KY,
            'KZ': KZ
        }
    
    options = {
        'gravity': gravity,
        'nu': nu,
        'comparison': False,
        'isplot': False
    }
    
    # Call unified solver
    result = lax_solver_torch(time_val, domain_params, physics_params, ic_type=ic_type, ic_params=ic_params, options=options)
    
    # Extract values to match old return format
    x = result.coordinates['x']
    y = result.coordinates['y']
    z = result.coordinates['z']
    rho0 = result.density
    vx0, vy0, vz0 = result.velocity_components
    phi0 = result.potential
    k_iter = result.metadata['iterations']
    rho_max = result.metadata['rho_max']
    
    return (x, y, z, rho0, vx0, vy0, vz0, phi0, k_iter, rho_max)

def lax_solution_warm_start_torch(rho_ic, vx_ic, vy_ic, x_grid, y_grid, 
                                   t_start, t_end, nu=0.5, save_times=None, gravity=True):
    """
    PyTorch implementation: Run FD solver from custom initial conditions (warm-start).
    
    This function allows restarting the FD solver from a PINN state or any custom state,
    enabling efficient generation of FD data for hybrid PINN-FD training.
    
    Args:
        rho_ic: Initial density field (Nx, Ny) - can be numpy array or torch tensor
        vx_ic: Initial x-velocity field (Nx, Ny) - can be numpy array or torch tensor
        vy_ic: Initial y-velocity field (Nx, Ny) - can be numpy array or torch tensor
        x_grid: x coordinates (Nx,) - can be numpy array or torch tensor
        y_grid: y coordinates (Ny,) - can be numpy array or torch tensor
        t_start: Starting time
        t_end: Ending time
        nu: Courant number
        save_times: List of times to save snapshots [default: [t_end]]
        gravity: Whether to include self-gravity (default: True)
    
    Returns:
        Dictionary: {time: (rho, vx, vy, phi, x, y)} for each saved time (all as numpy arrays)
    """
    if save_times is None:
        save_times = [t_end]
    
    # Domain setup (get shape from original input, before any conversion)
    if isinstance(rho_ic, np.ndarray):
        Nx, Ny = rho_ic.shape
    else:
        Nx, Ny = rho_ic.shape
    
    # Convert coordinate grids to torch tensors if needed (for domain setup and later use)
    if isinstance(x_grid, np.ndarray):
        x_grid = torch.from_numpy(x_grid).to(device=device, dtype=dtype)
    elif not isinstance(x_grid, torch.Tensor):
        x_grid = torch.tensor(x_grid, device=device, dtype=dtype)
    if isinstance(y_grid, np.ndarray):
        y_grid = torch.from_numpy(y_grid).to(device=device, dtype=dtype)
    elif not isinstance(y_grid, torch.Tensor):
        y_grid = torch.tensor(y_grid, device=device, dtype=dtype)
    
    Lx = (x_grid[-1] - x_grid[0] + (x_grid[1] - x_grid[0])).item()  # Approximate domain size
    Ly = (y_grid[-1] - y_grid[0] + (y_grid[1] - y_grid[0])).item()
    dx = Lx / Nx
    dy = Ly / Ny
    
    # Physical constants
    c_s = cs
    
    # Use setup_warm_start_ic_torch() helper to set up initial conditions
    # Convert inputs to torch tensors if needed (setup_warm_start_ic_torch handles this)
    provided_fields = {
        'rho': rho_ic,
        'vx': vx_ic,
        'vy': vy_ic
    }
    ic_result = setup_warm_start_ic_torch(provided_fields)
    rho0 = ic_result['rho']
    vx0 = ic_result['vx']
    vy0 = ic_result['vy']
    
    # Calculate initial potential (gravity is always True for collapse problems)
    phi0 = fft_solver_torch(const * (rho0 - rho_o), Lx, Nx, Ly, Ny)
    
    # Initialize flux terms
    Px0 = rho0 * vx0
    Py0 = rho0 * vy0
    
    # Storage for snapshots
    snapshots = {}
    
    # Time-stepping loop
    t = t_start
    k = 0
    
    # Initial dt
    dt = compute_adaptive_timestep_torch([vx0, vy0], c_s, dx, nu, include_sound_speed=True)
    
    while t < t_end:
        # Check if we should save a snapshot before this step
        for save_t in save_times:
            if t <= save_t < t + dt and save_t not in snapshots:
                # Save current state (convert to numpy for consistency)
                snapshots[save_t] = (
                    rho0.cpu().numpy().copy(),
                    vx0.cpu().numpy().copy(),
                    vy0.cpu().numpy().copy(),
                    phi0.cpu().numpy().copy(),
                    x_grid.cpu().numpy().copy(),
                    y_grid.cpu().numpy().copy()
                )
        
        # Ensure last step doesn't overshoot
        if t + dt > t_end:
            dt = t_end - t
        
        # LAX time-stepping
        mux = dt / (2 * dx)
        muy = dt / (2 * dy)
        
        # Update density
        rho1 = (0.25) * (torch.roll(rho0, -1, dims=0) + torch.roll(rho0, 1, dims=0) +
                        torch.roll(rho0, -1, dims=1) + torch.roll(rho0, 1, dims=1)) - \
               (mux * (torch.roll(rho0, -1, dims=0) * torch.roll(vx0, -1, dims=0) -
                       torch.roll(rho0, 1, dims=0) * torch.roll(vx0, 1, dims=0))) - \
               (muy * (torch.roll(rho0, -1, dims=1) * torch.roll(vy0, -1, dims=1) -
                       torch.roll(rho0, 1, dims=1) * torch.roll(vy0, 1, dims=1)))
        
        # Update momentum (with gravity)
        if gravity:
            Px1 = (0.25) * (torch.roll(Px0, -1, dims=0) + torch.roll(Px0, 1, dims=0) +
                            torch.roll(Px0, -1, dims=1) + torch.roll(Px0, 1, dims=1)) - \
                  (mux * (torch.roll(Px0, -1, dims=0) * torch.roll(vx0, -1, dims=0) -
                          torch.roll(Px0, 1, dims=0) * torch.roll(vx0, 1, dims=0))) - \
                  (muy * (torch.roll(Px0, -1, dims=1) * torch.roll(vy0, -1, dims=1) -
                          torch.roll(Px0, 1, dims=1) * torch.roll(vy0, 1, dims=1))) - \
                  ((c_s**2) * mux * (torch.roll(rho0, -1, dims=0) - torch.roll(rho0, 1, dims=0))) - \
                  (mux * rho0 * (torch.roll(phi0, -1, dims=0) - torch.roll(phi0, 1, dims=0)))
            
            Py1 = (0.25) * (torch.roll(Py0, -1, dims=0) + torch.roll(Py0, 1, dims=0) +
                            torch.roll(Py0, -1, dims=1) + torch.roll(Py0, 1, dims=1)) - \
                  (muy * (torch.roll(Py0, -1, dims=1) * torch.roll(vy0, -1, dims=1) -
                          torch.roll(Py0, 1, dims=1) * torch.roll(vy0, 1, dims=1))) - \
                  (mux * (torch.roll(Py0, -1, dims=0) * torch.roll(vx0, -1, dims=0) -
                          torch.roll(Py0, 1, dims=0) * torch.roll(vx0, 1, dims=0))) - \
                  ((c_s**2) * muy * (torch.roll(rho0, -1, dims=1) - torch.roll(rho0, 1, dims=1))) - \
                  (muy * rho0 * (torch.roll(phi0, -1, dims=1) - torch.roll(phi0, 1, dims=1)))
            
            # Update potential
            phi1 = fft_solver_torch(const * (rho1 - rho_o), Lx, Nx, Ly, Ny)
        else:
            # Without gravity (shouldn't happen for collapse problems, but included for completeness)
            Px1 = (0.25) * (torch.roll(Px0, -1, dims=0) + torch.roll(Px0, 1, dims=0) +
                            torch.roll(Px0, -1, dims=1) + torch.roll(Px0, 1, dims=1)) - \
                  (mux * (torch.roll(Px0, -1, dims=0) * torch.roll(vx0, -1, dims=0) -
                          torch.roll(Px0, 1, dims=0) * torch.roll(vx0, 1, dims=0))) - \
                  (muy * (torch.roll(Px0, -1, dims=1) * torch.roll(vy0, -1, dims=1) -
                          torch.roll(Px0, 1, dims=1) * torch.roll(vy0, 1, dims=1))) - \
                  ((c_s**2) * mux * (torch.roll(rho0, -1, dims=0) - torch.roll(rho0, 1, dims=0)))
            
            Py1 = (0.25) * (torch.roll(Py0, -1, dims=0) + torch.roll(Py0, 1, dims=0) +
                            torch.roll(Py0, -1, dims=1) + torch.roll(Py0, 1, dims=1)) - \
                  (muy * (torch.roll(Py0, -1, dims=1) * torch.roll(vy0, -1, dims=1) -
                          torch.roll(Py0, 1, dims=1) * torch.roll(vy0, 1, dims=1))) - \
                  (mux * (torch.roll(Py0, -1, dims=0) * torch.roll(vx0, -1, dims=0) -
                          torch.roll(Py0, 1, dims=0) * torch.roll(vx0, 1, dims=0))) - \
                  ((c_s**2) * muy * (torch.roll(rho0, -1, dims=1) - torch.roll(rho0, 1, dims=1)))
            
            phi1 = torch.zeros_like(rho1)
        
        # Update velocities
        vx1 = Px1 / rho1
        vy1 = Py1 / rho1
        
        # Update state
        rho0 = rho1
        vx0 = vx1
        vy0 = vy1
        Px0 = Px1
        Py0 = Py1
        if gravity:
            phi0 = phi1
        
        t += dt
        k += 1
        
        # Calculate dt for next step
        vmax = max(torch.max(torch.abs(vx0)).item(), torch.max(torch.abs(vy0)).item())
        dt1 = nu * dx / vmax if vmax > 1e-9 else float('inf')
        dt2 = nu * dx / c_s
        dt = min(dt1, dt2)
    
    # Save final snapshot if not already saved
    if t_end not in snapshots:
        snapshots[t_end] = (
            rho0.cpu().numpy().copy(),
            vx0.cpu().numpy().copy(),
            vy0.cpu().numpy().copy(),
            phi0.cpu().numpy().copy(),
            x_grid.cpu().numpy().copy(),
            y_grid.cpu().numpy().copy()
        )
    
    return snapshots
_register_module('numerical_solvers.LAX_torch', ['SimulationResult', 'build_k_space_grid_torch', 'compute_adaptive_timestep_torch', 'compute_jeans_length_torch', 'compute_lax_density_update_torch', 'compute_lax_momentum_update_torch', 'compute_perturbation_velocity_torch', 'device', 'dtype', 'fft_solver_torch', 'fft_solver_torch_3d', 'generate_velocity_field_power_spectrum_torch', 'has_gpu', 'initialize_arrays_torch', 'lax_solution_3d_sinusoidal_torch', 'lax_solution_torch', 'lax_solution_warm_start_torch', 'lax_solver_torch', 'lax_time_step_torch', 'setup_power_spectrum_ic_torch', 'setup_sinusoidal_ic_torch', 'setup_warm_start_ic_torch'])

# ==== Module: core.initial_conditions (core/initial_conditions.py) ====
"""
Initial Conditions Module

This module contains all initial condition functions and power spectrum generation
for PINNs training. Extracted from solver.py for better code organization.

Functions:
- initialize_shared_velocity_fields: Setup shared velocity fields for PINN/FD consistency
- generate_power_spectrum_field: Generate vx component using power spectrum
- generate_power_spectrum_field_vy: Generate vy component using power spectrum
- fun_rho_0: Initial density condition
- fun_vx_0: Initial x-velocity condition
- fun_vy_0: Initial y-velocity condition
- func: Placeholder function for phi initial condition
"""

import numpy as np
import torch
from config import (cs, rho_o, N_GRID, POWER_EXPONENT, 
                    PERTURBATION_TYPE, KX, KY, KZ, RANDOM_SEED,
                    DIMENSION, N_GRID_3D)
from numerical_solvers.LAX import generate_shared_velocity_field

# Global shared velocity fields for consistent initial conditions
_shared_vx_interp = None
_shared_vy_interp = None
_shared_vz_interp = None

def _ensure_column_tensor(tensor):
    return tensor if tensor.dim() > 1 else tensor.unsqueeze(-1)

def _extract_spatial_coords(coords):
    return coords[:-1] if len(coords) > 1 else coords


def _detect_dimension(x):
    """
    Detect spatial dimension from coordinate tensor.
    
    Args:
        x: Collocation coordinates [x, y, t] or [x, y, z, t] or [x, t]
    
    Returns:
        int: Spatial dimension (2 or 3)
    """
    # Extract spatial coordinates (exclude time dimension)
    spatial_coords = _extract_spatial_coords(x)
    
    # Check if z coordinate exists and has non-zero variation
    if len(spatial_coords) >= 3:
        z_coord = spatial_coords[2]
        # Check if z has meaningful variation (not all zeros or constant)
        if z_coord.numel() > 0:
            z_range = torch.max(z_coord) - torch.min(z_coord)
            if z_range > 1e-6:  # Non-trivial z variation indicates 3D
                return 3
    
    # Default to 2D if z is missing or constant
    return 2


def initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=None, dimension=None):
    """
    Initialize shared velocity fields for consistent PINN/FD initial conditions.
    This should be called once at the beginning of training.
    
    IMPORTANT: The parameters used here (POWER_EXPONENT, v_1=a*cs, seed) MUST match
    the parameters used in FD plotting functions to ensure identical initial conditions.
    All FD visualization functions should use the same defaults.
    
    Args:
        lam: Wavelength
        num_of_waves: Number of waves in domain
        v_1: Velocity amplitude
        seed: Random seed for reproducibility
        dimension: Spatial dimension (2 or 3). If None, uses DIMENSION from config.
    
    Returns:
        For 2D: Tuple (vx_np, vy_np): Velocity field arrays
        For 3D: Tuple (vx_np, vy_np, vz_np): Velocity field arrays
    """
    global _shared_vx_interp, _shared_vy_interp, _shared_vz_interp
    
    if seed is None:
        seed = RANDOM_SEED
    
    # Determine dimension
    if dimension is None:
        dimension = DIMENSION
    
    # Calculate domain size to match FD solver
    Lx = lam * num_of_waves
    Ly = lam * num_of_waves
    
    if dimension == 2:
        # 2D case
        Lz = None
        nz = None
        grid_size = N_GRID
        
        # Generate shared velocity fields for 2D
        vx_np, vy_np, vx_interp, vy_interp = generate_shared_velocity_field(
            grid_size, grid_size, Lx, Ly, 
            power_index=POWER_EXPONENT, 
            amplitude=v_1, 
            DIMENSION=2,
            random_seed=seed
        )
        
        # Store interpolation functions globally
        _shared_vx_interp = vx_interp
        _shared_vy_interp = vy_interp
        _shared_vz_interp = None
        
        return vx_np, vy_np
    
    elif dimension == 3:
        # 3D case - use N_GRID_3D for all dimensions for consistency
        Lz = lam * num_of_waves
        nx = ny = nz = N_GRID_3D  # Use N_GRID_3D for all dimensions in 3D
        
        # Generate shared velocity fields for 3D
        vx_np, vy_np, vz_np, vx_interp, vy_interp, vz_interp = generate_shared_velocity_field(
            nx, ny, Lx, Ly,
            power_index=POWER_EXPONENT,
            amplitude=v_1,
            DIMENSION=3,
            random_seed=seed,
            nz=nz,
            Lz=Lz
        )
        
        # Store interpolation functions globally
        _shared_vx_interp = vx_interp
        _shared_vy_interp = vy_interp
        _shared_vz_interp = vz_interp
        
        return vx_np, vy_np, vz_np
    
    else:
        raise ValueError(f"Unsupported dimension={dimension}. Use 2 or 3.")


def _interpolate_shared_field(x, field_interp):
    """
    Helper function to interpolate shared velocity field to collocation points.
    Supports both 2D and 3D coordinates.
    
    Args:
        x: Collocation coordinates [x, y, ...] for 2D or [x, y, z, ...] for 3D
        field_interp: Interpolation function from shared fields
    
    Returns:
        Interpolated field values as torch tensor
    """
    # Detect dimension from coordinates
    dim = _detect_dimension(x)
    
    # Convert tensor coordinates to numpy for interpolation
    x_np = x[0].detach().cpu().numpy()
    y_np = x[1].detach().cpu().numpy()
    
    if dim == 2:
        # 2D case: create coordinate pairs for interpolation
        coords = np.stack([x_np.flatten(), y_np.flatten()], axis=1)
    else:
        # 3D case: create coordinate triplets for interpolation
        z_np = x[2].detach().cpu().numpy()
        coords = np.stack([x_np.flatten(), y_np.flatten(), z_np.flatten()], axis=1)
    
    # Interpolate shared velocity field
    field_interp_values = field_interp(coords)
    
    # Convert back to tensor and reshape
    field_tensor = torch.from_numpy(field_interp_values).float().to(x[0].device)
    
    # Ensure correct shape [N, 1]
    if field_tensor.dim() == 1:
        return field_tensor.unsqueeze(-1)
    else:
        return field_tensor


def _generate_power_spectrum_fallback(lam, v_1, x, seed=None):
    """
    Fallback power spectrum generation when shared fields are not available.
    Supports both 2D and 3D FFT.
    
    Args:
        lam: Wavelength (unused for domain sizing in fallback)
        v_1: Velocity amplitude
        x: Collocation coordinates [x, y, ...] for 2D or [x, y, z, ...] for 3D
        seed: Random seed
    
    Returns:
        Generated power spectrum field
    """
    if seed is None:
        seed = RANDOM_SEED
    
    # Detect dimension from coordinates
    dim = _detect_dimension(x)
    
    # Infer domain extents directly from the collocation coordinates to support arbitrary num_of_waves
    # Use conservative defaults if tensors are degenerate (e.g., single point during a unit test)
    x_coords = x[0].detach()
    y_coords = x[1].detach() if len(x) > 1 else x[0].detach()
    z_coords = x[2].detach() if len(x) > 2 and dim == 3 else None

    xmin_val = torch.min(x_coords).item() if x_coords.numel() > 0 else 0.0
    xmax_val = torch.max(x_coords).item() if x_coords.numel() > 0 else float(lam * 2.0)
    ymin_val = torch.min(y_coords).item() if y_coords.numel() > 0 else 0.0
    ymax_val = torch.max(y_coords).item() if y_coords.numel() > 0 else float(lam * 2.0)
    
    if dim == 3 and z_coords is not None:
        zmin_val = torch.min(z_coords).item() if z_coords.numel() > 0 else 0.0
        zmax_val = torch.max(z_coords).item() if z_coords.numel() > 0 else float(lam * 2.0)
    else:
        zmin_val = 0.0
        zmax_val = float(lam * 2.0)

    # Ensure positive lengths; fall back to 2*lam if bounds collapse
    Lx = float(max(xmax_val - xmin_val, 1e-6))
    Ly = float(max(ymax_val - ymin_val, 1e-6))
    Lz = float(max(zmax_val - zmin_val, 1e-6)) if dim == 3 else None
    
    if not torch.isfinite(torch.tensor(Lx)) or Lx < 1e-6:
        Lx = float(lam * 2.0)
    if not torch.isfinite(torch.tensor(Ly)) or Ly < 1e-6:
        Ly = float(lam * 2.0)
    if dim == 3 and (Lz is None or not torch.isfinite(torch.tensor(Lz)) or Lz < 1e-6):
        Lz = float(lam * 2.0)

    # Select grid size based on dimension
    if dim == 2:
        nx, ny = N_GRID, N_GRID
        nz = None
        dx = Lx / nx
        dy = Ly / ny
        dz = None
    else:  # dim == 3
        # Use N_GRID_3D for all dimensions in 3D for consistency
        nx, ny, nz = N_GRID_3D, N_GRID_3D, N_GRID_3D
        dx = Lx / nx
        dy = Ly / ny
        dz = Lz / nz
    
    # Calculate wave numbers
    kx = 2 * np.pi * torch.fft.fftfreq(nx, dx, device=x[0].device)
    ky = 2 * np.pi * torch.fft.fftfreq(ny, dy, device=x[0].device)
    
    if dim == 2:
        KX_grid, KY_grid = torch.meshgrid(kx, ky, indexing='ij')
        
        # Calculate magnitude of wave number
        K = torch.sqrt(KX_grid**2 + KY_grid**2)
        
        # Power spectrum: P(k) ~ k^expon
        K_safe = torch.where(K == 0, torch.tensor(1e-10, device=x[0].device), K)
        power_spectrum = K_safe**POWER_EXPONENT
        
        # Remove DC (uniform) mode to avoid bulk drift
        power_spectrum[K == 0] = 0.0
        
        # Safety check: limit extreme values
        power_spectrum = torch.clamp(power_spectrum, 0, 1e6)
        
        # Generate random phases
        torch.manual_seed(seed)
        random_phases = torch.randn(nx, ny, device=x[0].device) + 1j * torch.randn(nx, ny, device=x[0].device)
        
        # Create complex field in Fourier space and transform to real space
        field_fourier = torch.sqrt(power_spectrum) * random_phases
        field_real = torch.real(torch.fft.ifft2(field_fourier))
        
        # Remove any residual mean (bulk flow) and normalize rms to v_1
        field_real = field_real - torch.mean(field_real)
        field_real = field_real / torch.std(field_real) * v_1
        
        # Interpolate to the actual collocation points
        x_norm = torch.clamp(((x[0] - xmin_val) / Lx) * (nx - 1), 0, nx - 1)
        if len(x) > 1:
            y_norm = torch.clamp(((x[1] - ymin_val) / Ly) * (ny - 1), 0, ny - 1)
        else:
            # 1D fallback: mirror x for y to preserve shape
            y_norm = x_norm.clone()
        
        x_idx = torch.round(x_norm).long()
        y_idx = torch.round(y_norm).long()
        
        # Ensure correct tensor shape [N, 1]
        result = field_real[x_idx, y_idx]
        if result.dim() == 1:
            return result.unsqueeze(-1)
        else:
            return result
    
    else:  # dim == 3
        kz = 2 * np.pi * torch.fft.fftfreq(nz, dz, device=x[0].device)
        KX_grid, KY_grid, KZ_grid = torch.meshgrid(kx, ky, kz, indexing='ij')
        
        # Calculate magnitude of wave number
        K = torch.sqrt(KX_grid**2 + KY_grid**2 + KZ_grid**2)
        
        # Power spectrum: P(k) ~ k^expon
        K_safe = torch.where(K == 0, torch.tensor(1e-10, device=x[0].device), K)
        power_spectrum = K_safe**POWER_EXPONENT
        
        # Remove DC (uniform) mode to avoid bulk drift
        power_spectrum[K == 0] = 0.0
        
        # Safety check: limit extreme values
        power_spectrum = torch.clamp(power_spectrum, 0, 1e6)
        
        # Generate random phases
        torch.manual_seed(seed)
        random_phases = torch.randn(nx, ny, nz, device=x[0].device) + 1j * torch.randn(nx, ny, nz, device=x[0].device)
        
        # Create complex field in Fourier space and transform to real space
        field_fourier = torch.sqrt(power_spectrum) * random_phases
        field_real = torch.real(torch.fft.ifftn(field_fourier))
        
        # Remove any residual mean (bulk flow) and normalize rms to v_1
        field_real = field_real - torch.mean(field_real)
        field_real = field_real / torch.std(field_real) * v_1
        
        # Interpolate to the actual collocation points
        x_norm = torch.clamp(((x[0] - xmin_val) / Lx) * (nx - 1), 0, nx - 1)
        y_norm = torch.clamp(((x[1] - ymin_val) / Ly) * (ny - 1), 0, ny - 1)
        z_norm = torch.clamp(((x[2] - zmin_val) / Lz) * (nz - 1), 0, nz - 1)
        
        x_idx = torch.round(x_norm).long()
        y_idx = torch.round(y_norm).long()
        z_idx = torch.round(z_norm).long()
        
        # Ensure correct tensor shape [N, 1]
        result = field_real[x_idx, y_idx, z_idx]
        if result.dim() == 1:
            return result.unsqueeze(-1)
        else:
            return result


def generate_power_spectrum_field(lam, v_1, x, seed=None):
    """
    Generate 2D Gaussian random field with power spectrum using shared fields if available.
    
    Args:
        lam: Wavelength
        v_1: Velocity amplitude
        x: Collocation coordinates [x, y, ...]
        seed: Random seed for reproducibility
    
    Returns:
        vx component of velocity field
    """
    if seed is None:
        seed = RANDOM_SEED
    
    # Use shared velocity fields if available
    if _shared_vx_interp is not None:
        return _interpolate_shared_field(x, _shared_vx_interp)
    
    # Fallback to original method if shared fields not available
    return _generate_power_spectrum_fallback(lam, v_1, x, seed)


def generate_power_spectrum_field_vy(lam, v_1, x, seed=None):
    """
    Generate vy component using shared fields if available.
    
    Args:
        lam: Wavelength
        v_1: Velocity amplitude
        x: Collocation coordinates [x, y, ...] for 2D or [x, y, z, ...] for 3D
        seed: Random seed for reproducibility
    
    Returns:
        vy component of velocity field
    """
    if seed is None:
        seed = RANDOM_SEED
    
    # Use shared velocity fields if available
    if _shared_vy_interp is not None:
        return _interpolate_shared_field(x, _shared_vy_interp)
    
    # Fallback to original method if shared fields not available
    return _generate_power_spectrum_fallback(lam, v_1, x, seed)


def generate_power_spectrum_field_vz(lam, v_1, x, seed=None):
    """
    Generate vz component using shared fields if available.
    
    Args:
        lam: Wavelength
        v_1: Velocity amplitude
        x: Collocation coordinates [x, y, z, ...] for 3D
        seed: Random seed for reproducibility
    
    Returns:
        vz component of velocity field
    """
    if seed is None:
        seed = RANDOM_SEED
    
    # Use shared velocity fields if available
    if _shared_vz_interp is not None:
        return _interpolate_shared_field(x, _shared_vz_interp)
    
    # Fallback to original method if shared fields not available
    return _generate_power_spectrum_fallback(lam, v_1, x, seed)


def _compute_wave_phase(spatial_coords, lam):
    """
    Compute wave phase and wave-vector components for sinusoidal perturbations.
    """
    if not spatial_coords:
        raise ValueError("Spatial coordinates are required to compute wave phase.")
    
    x_coord = _ensure_column_tensor(spatial_coords[0])
    zeros = torch.zeros_like(x_coord)
    y_coord = _ensure_column_tensor(spatial_coords[1]) if len(spatial_coords) >= 2 else zeros
    z_coord = _ensure_column_tensor(spatial_coords[2]) if len(spatial_coords) >= 3 else zeros
    
    device = x_coord.device
    dtype = x_coord.dtype
    kx = torch.as_tensor(float(KX), device=device, dtype=dtype)
    ky = torch.as_tensor(float(KY), device=device, dtype=dtype)
    kz = torch.as_tensor(float(KZ), device=device, dtype=dtype)
    
    phase = kx * x_coord + ky * y_coord + kz * z_coord
    
    # Fallback to fundamental wavelength if wave-vector is zero (e.g., user-specified)
    if torch.allclose(kx.abs() + ky.abs() + kz.abs(), torch.tensor(0.0, device=device, dtype=dtype)):
        fundamental = torch.as_tensor(2 * np.pi / lam, device=device, dtype=dtype)
        phase = fundamental * x_coord
        kx, ky, kz = fundamental, torch.zeros_like(fundamental), torch.zeros_like(fundamental)
    
    return phase, kx, ky, kz, x_coord, y_coord, z_coord


def _coupled_velocity_components(coords, lam, jeans, v_1):
    """
    Generate coupled velocity components from the same wave pattern (supports 1D/2D/3D).
    """
    spatial_coords = _extract_spatial_coords(coords)
    phase, kx, ky, kz, _, _, _ = _compute_wave_phase(spatial_coords, lam)
    dtype = phase.dtype
    device = phase.device
    v_scale = torch.as_tensor(float(v_1), device=device, dtype=dtype)
    
    if lam > jeans:
        wave_field = -v_scale * torch.sin(phase)
    else:
        wave_field = v_scale * torch.cos(phase)
    
    k_mag = torch.sqrt(kx**2 + ky**2 + kz**2)
    if k_mag <= torch.tensor(1e-12, device=device, dtype=dtype):
        vx = wave_field
        vy = torch.zeros_like(wave_field)
        vz = torch.zeros_like(wave_field)
    else:
        inv_mag = 1.0 / k_mag
        vx = wave_field * (kx * inv_mag)
        vy = wave_field * (ky * inv_mag)
        vz = wave_field * (kz * inv_mag)
    
    return vx, vy, vz


def fun_rho_0(rho_1, lam, x):
    """
    Define initial condition for density.
    
    Args:
        rho_1: Perturbation amplitude
        lam: Wavelength
        x: Spatial coordinates [x, y, t] or [x, t]
    
    Returns:
        rho_0: Initial density field
    """
    if str(PERTURBATION_TYPE).lower() == "sinusoidal":
        spatial_coords = _extract_spatial_coords(x)
        phase, *_ = _compute_wave_phase(spatial_coords, lam)
        rho_0 = rho_o + rho_1 * torch.cos(phase)
    else:
        # Power spectrum: uniform initial density
        rho_0 = torch.full_like(x[0], rho_o)
        # Ensure correct shape [N, 1]
        if rho_0.dim() == 1:
            rho_0 = rho_0.unsqueeze(-1)
    
    return rho_0


def fun_vx_0(lam, jeans, v_1, x):
    """
    Initial condition for x-velocity.
    
    Args:
        lam: Wavelength
        jeans: Jeans length
        v_1: Velocity amplitude
        x: Spatial coordinates
    
    Returns:
        vx_0: Initial x-velocity field
    """
    if str(PERTURBATION_TYPE).lower() == "sinusoidal":
        vx, _, _ = _coupled_velocity_components(x, lam, jeans, v_1)
        return vx
    else:
        # Power spectrum case
        return generate_power_spectrum_field(lam, v_1, x, seed=RANDOM_SEED)


def fun_vy_0(lam, jeans, v_1, x):
    """
    Initial condition for y-velocity.
    
    Args:
        lam: Wavelength
        jeans: Jeans length
        v_1: Velocity amplitude
        x: Spatial coordinates
    
    Returns:
        vy_0: Initial y-velocity field
    """
    if str(PERTURBATION_TYPE).lower() == "sinusoidal":
        _, vy, _ = _coupled_velocity_components(x, lam, jeans, v_1)
        return vy
    else:
        # Power spectrum case
        return generate_power_spectrum_field_vy(lam, v_1, x, seed=RANDOM_SEED)


def fun_vz_0(lam, jeans, v_1, x):
    """
    Initial condition for z-velocity (used in 3D runs).
    
    Args:
        lam: Wavelength
        jeans: Jeans length
        v_1: Velocity amplitude
        x: Spatial coordinates
    
    Returns:
        vz_0: Initial z-velocity field
    """
    if str(PERTURBATION_TYPE).lower() == "sinusoidal":
        _, _, vz = _coupled_velocity_components(x, lam, jeans, v_1)
        return vz
    else:
        # Power spectrum case: use shared fields or fallback
        return generate_power_spectrum_field_vz(lam, v_1, x, seed=RANDOM_SEED)


def func(x):
    """
    Placeholder function for phi initial condition (zero potential).
    
    Args:
        x: Spatial coordinates
    
    Returns:
        Zero tensor matching the shape of x[0]
    """
    return x[0] * 0
_register_module('core.initial_conditions', ['_compute_wave_phase', '_coupled_velocity_components', '_detect_dimension', '_ensure_column_tensor', '_extract_spatial_coords', '_generate_power_spectrum_fallback', '_interpolate_shared_field', '_shared_vx_interp', '_shared_vy_interp', '_shared_vz_interp', 'fun_rho_0', 'fun_vx_0', 'fun_vy_0', 'fun_vz_0', 'func', 'generate_power_spectrum_field', 'generate_power_spectrum_field_vy', 'generate_power_spectrum_field_vz', 'initialize_shared_velocity_fields'])

# ==== Module: core.losses (core/losses.py) ====
from core.data_generator import col_gen
from core.data_generator import diff

import numpy as np

import torch
import torch.nn as nn
from torch.autograd import Variable
from config import cs, const, G, rho_o

class ASTPN(col_gen):
    
    def __init__(self, rmin=[0,0,0,0], rmax=[1,1,1,1], N_0 = 1000, N_b=1000, N_r=3000, dimension=1):
        super().__init__(rmin,rmax, N_0,0,N_r, dimension)  # N_b set to 0 due to hard constraints
        
       
        self.coord_Lx, self.coord_Rx = self.geo_time_coord(option="BC",coordinate=1)
        
        if dimension == 2:
            self.coord_Ly, self.coord_Ry = self.geo_time_coord(option="BC",coordinate=2)

        if dimension == 3:
            self.coord_Ly, self.coord_Ry = self.geo_time_coord(option="BC",coordinate=2)
            self.coord_Lz, self.coord_Rz = self.geo_time_coord(option="BC",coordinate=3)


def pde_residue(colloc, net, dimension = 1):
    
    '''
    This is the main function that returns all the PDE residue
    
    Args:
        colloc: Collocation points
        net: Neural network
        dimension: Spatial dimension (1, 2, or 3)
    '''
    
    return pde_residue_standard(colloc, net, dimension)


def pde_residue_standard(colloc, net, dimension = 1):
    
    '''
    Standard PDE residues (network predicts rho directly)
    '''
    net_outputs = net(colloc)
    
    x = colloc[0]
    
    if dimension == 1:
        t = colloc[1]

    elif dimension == 2:
        y = colloc[1]
        t = colloc[2]

    elif dimension == 3:
        y = colloc[1]
        z = colloc[2]
        t = colloc[3]
    
    rho, vx = net_outputs[:,0:1], net_outputs[:,1:2]

    if dimension == 1:

        phi = net_outputs[:,2:3]

        rho_t = diff(rho,t,order=1)  
        rho_x = diff(rho,x,order=1)

        vx_t = diff(vx, t,order=1)
        vx_x = diff(vx, x,order=1)
        
        phi_x = diff(phi,x,order=1)
        phi_x_x = diff(phi,x,order=2)

    elif dimension == 2:

        vy = net_outputs[:,2:3]
        phi = net_outputs[:,3:4]

        rho_t = diff(rho,t,order=1)  
        rho_x = diff(rho,x,order=1)
        rho_y = diff(rho,y,order=1)

        vx_t = diff(vx, t,order=1)
        vy_t = diff(vy, t,order=1)

        vx_x = diff(vx, x,order=1)
        vx_y = diff(vx, y,order=1)
        vy_x = diff(vy, x,order=1)
        vy_y = diff(vy, y,order=1)
        
        phi_x = diff(phi,x,order=1)
        phi_x_x = diff(phi,x,order=2)

        phi_y = diff(phi,y,order=1)
        phi_y_y = diff(phi,y,order=2)

    elif dimension == 3:
        vy = net_outputs[:,2:3]
        vz = net_outputs[:,3:4]
        phi = net_outputs[:,4:5]

        rho_t = diff(rho,t,order=1)  
        rho_x = diff(rho,x,order=1)
        rho_y = diff(rho,y,order=1)
        rho_z = diff(rho,z,order=1)

        vx_t = diff(vx, t,order=1)
        vy_t = diff(vy, t,order=1)
        vz_t = diff(vz, t,order=1)

        vx_x = diff(vx, x,order=1)
        vy_x = diff(vy, x,order=1)
        vz_x = diff(vz, x,order=1)

        vx_y = diff(vx, y,order=1)
        vy_y = diff(vy, y,order=1)
        vz_y = diff(vz, y,order=1)
        
        vx_z = diff(vx, z,order=1)
        vy_z = diff(vy, z,order=1)
        vz_z = diff(vz, z,order=1)
        
        phi_x = diff(phi,x,order=1)
        phi_x_x = diff(phi,x,order=2)

        phi_y = diff(phi,y,order=1)
        phi_y_y = diff(phi,y,order=2)
    
        phi_z = diff(phi,z,order=1)
        phi_z_z = diff(phi,z,order=2)

    
    ## The residues from the equations

    if dimension == 1:
        rho_r = rho_t + vx * rho_x + rho * vx_x
        vx_r = rho*vx_t + rho*(vx*vx_x) + cs*cs*rho_x +rho*phi_x
        phi_r = phi_x_x - const*(rho - rho_o)

        return rho_r, vx_r, phi_r

    elif dimension == 2:
        rho_r = rho_t + vx * rho_x + vy * rho_y + rho * vx_x + rho * vy_y
        vx_r = rho*vx_t + rho*(vx*vx_x + vy*vx_y) + cs*cs*rho_x + rho*phi_x
        vy_r = rho*vy_t + rho*(vy*vy_y + vx*vy_x) + cs*cs*rho_y + rho*phi_y
        phi_r = phi_x_x + phi_y_y - const*(rho - rho_o)

        return rho_r, vx_r, vy_r, phi_r
    
    elif dimension == 3:
        rho_r = rho_t + vx * rho_x + rho * vx_x + vy *rho_y + rho * vy_y + vz *rho_z +rho * vz_z
        vx_r = rho*vx_t + rho*(vx*vx_x + vy*vx_y+vz*vx_z) + cs*cs*rho_x + rho*phi_x
        vy_r = rho*vy_t + rho*(vy*vy_y + vx*vy_x+vz*vy_z) + cs*cs*rho_y + rho*phi_y
        vz_r = rho*vz_t + rho*(vz*vz_z + vx*vz_x+vy*vz_y) + cs*cs*rho_z + rho*phi_z
        phi_r = phi_x_x + phi_y_y +phi_z_z - const*(rho - rho_o)
        
        return rho_r,vx_r,vy_r,vz_r,phi_r


def poisson_residue_only(colloc, net, dimension=1):
    """
    Compute only the Poisson equation residual: ∇²φ - const*(ρ - ρ₀)
    
    This is used for extra enforcement at t=0 (Option 3: Pure ML approach).
    By evaluating Poisson residual on many t=0 points, we ensure φ is 
    correctly initialized without using numerical solvers.
    
    Args:
        colloc: Collocation points [x, (y), (z), t]
        net: Neural network
        dimension: Spatial dimension (1, 2, or 3)
    
    Returns:
        phi_r: Poisson residual tensor
    """
    net_outputs = net(colloc)
    
    x = colloc[0]
    
    if dimension == 1:
        t = colloc[1]
        phi = net_outputs[:, 2:3]
        phi_x_x = diff(phi, x, order=2)
        
    elif dimension == 2:
        y = colloc[1]
        t = colloc[2]
        phi = net_outputs[:, 3:4]
        phi_x_x = diff(phi, x, order=2)
        phi_y_y = diff(phi, y, order=2)
        
    elif dimension == 3:
        y = colloc[1]
        z = colloc[2]
        t = colloc[3]
        phi = net_outputs[:, 4:5]
        phi_x_x = diff(phi, x, order=2)
        phi_y_y = diff(phi, y, order=2)
        phi_z_z = diff(phi, z, order=2)
    
    # Get density
    rho = net_outputs[:, 0:1]
    
    # Compute Poisson residual
    if dimension == 1:
        phi_r = phi_x_x - const*(rho - rho_o)
    elif dimension == 2:
        phi_r = phi_x_x + phi_y_y - const*(rho - rho_o)
    elif dimension == 3:
        phi_r = phi_x_x + phi_y_y + phi_z_z - const*(rho - rho_o)
    
    return phi_r
_register_module('core.losses', ['ASTPN', 'pde_residue', 'pde_residue_standard', 'poisson_residue_only'])

# ==== Module: core.model_architecture (core/model_architecture.py) ====
import numpy as np

import torch
import torch.nn as nn
#from torch.autograd import Variable
from config import rho_o, num_neurons, num_layers, PERTURBATION_TYPE, DEFAULT_ACTIVATION, STARTUP_DT, USE_PARAMETERIZATION

class Sin(nn.Module):
    def forward(self, input):
        return torch.sin(input)


def get_activation(activation_type):
    """
    Factory function to create activation function instances.
    
    Args:
        activation_type: String identifier ('sin', 'tanh', 'relu', 'elu')
    
    Returns:
        nn.Module activation function
    """
    activation_type = activation_type.lower()
    if activation_type == 'sin':
        return Sin()
    elif activation_type == 'tanh':
        return nn.Tanh()
    elif activation_type == 'relu':
        return nn.ReLU()
    elif activation_type == 'elu':
        return nn.ELU()
    else:
        raise ValueError(f"Unknown activation type: {activation_type}. Choose from 'sin', 'tanh', 'relu', 'elu'.")

class PINN(nn.Module):
    def __init__(self, num_neurons=num_neurons, num_layers=num_layers, n_harmonics=1, activation_type=DEFAULT_ACTIVATION):
        super(PINN, self).__init__()
        self.num_neurons = num_neurons
        self.n_harmonics = n_harmonics
        self.num_layers = max(2, int(num_layers))  # total Linear layers including output
        self.activation_type = activation_type
        
        # Domain extents for periodic embeddings (set via set_domain)
        self.xmin = None
        self.xmax = None
        self.ymin = None
        self.ymax = None
        self.zmin = None
        self.zmax = None
    
    # Helper to build a branch with dynamic depth
        def _make_branch(in_dim, out_dim):
            layers = []
            # First layer
            layers.append(nn.Linear(in_dim, self.num_neurons))
            # Hidden layers: total linear layers = self.num_layers; we already added 1; 
            # add (self.num_layers - 2) hidden Linear blocks with activations after each
            for _ in range(self.num_layers - 2):
                layers.append(get_activation(self.activation_type))
                layers.append(nn.Linear(self.num_neurons, self.num_neurons))
            # Activation before output if there is at least one hidden block
            if self.num_layers > 2:
                layers.append(get_activation(self.activation_type))
            # Output layer
            layers.append(nn.Linear(self.num_neurons, out_dim))
            return nn.Sequential(*layers)

    # 1D branch (periodic x features + t)
        in_dim_1d = 2*self.n_harmonics + 1
        self.branch_1d = _make_branch(in_dim_1d, 3)
        
    # 2D branch (periodic x,y features + t)
        in_dim_2d = 4*self.n_harmonics + 1
        self.branch_2d = _make_branch(in_dim_2d, 4)
        
    # 3D branch (periodic x,y,z features + t)
        in_dim_3d = 6*self.n_harmonics + 1
        self.branch_3d = _make_branch(in_dim_3d, 5)


    def set_domain(self, rmin, rmax, dimension):
        # rmin/rmax exclude time; follow ASTPN usage
        if dimension >= 1:
            self.xmin, self.xmax = float(rmin[0]), float(rmax[0])
        if dimension >= 2:
            self.ymin, self.ymax = float(rmin[1]), float(rmax[1])
        if dimension >= 3:
            self.zmin, self.zmax = float(rmin[2]), float(rmax[2])

    def _periodic_features(self, u, umin, umax):
        # u is [N,1]
        L = umax - umin
        theta = 2*np.pi*(u - umin)/L
        features = []

        for k in range(1, self.n_harmonics+1):
            
            scale = 1.0 / np.sqrt(k)

            features.append(scale * torch.sin(k*theta))
            features.append(scale * torch.cos(k*theta))

        return torch.cat(features, dim=1) if len(features) > 0 else u

    def _prepare_coordinate_features(self, X):
        """
        Prepare periodic features for all spatial coordinates.
        
        Args:
            X: List of coordinates [x, ...spatial..., t]
        
        Returns:
            Tuple (features, t_tensor, dimension)
        """
        x, t = X[0], X[-1]
        x = x.unsqueeze(-1) if x.dim() == 1 else x
        t = t.unsqueeze(-1) if t.dim() == 1 else t
        dimension = len(X)
        
        if dimension == 2:
            if self.xmin is None or self.xmax is None:
                raise RuntimeError("Domain not set: call net.set_domain for dimension=1")
            x_feat = self._periodic_features(x, self.xmin, self.xmax)
            features = torch.cat([x_feat, t], dim=1)
        
        elif dimension == 3:
            if self.xmin is None or self.xmax is None or self.ymin is None or self.ymax is None:
                raise RuntimeError("Domain not set: call net.set_domain for dimension=2")
            y = X[1].unsqueeze(-1) if X[1].dim() == 1 else X[1]
            x_feat = self._periodic_features(x, self.xmin, self.xmax)
            y_feat = self._periodic_features(y, self.ymin, self.ymax)
            features = torch.cat([x_feat, y_feat, t], dim=1)
        
        elif dimension == 4:
            if (self.xmin is None or self.xmax is None or
                self.ymin is None or self.ymax is None or
                self.zmin is None or self.zmax is None):
                raise RuntimeError("Domain not set: call net.set_domain for dimension=3")
            y = X[1].unsqueeze(-1) if X[1].dim() == 1 else X[1]
            z = X[2].unsqueeze(-1) if X[2].dim() == 1 else X[2]
            x_feat = self._periodic_features(x, self.xmin, self.xmax)
            y_feat = self._periodic_features(y, self.ymin, self.ymax)
            z_feat = self._periodic_features(z, self.zmin, self.zmax)
            features = torch.cat([x_feat, y_feat, z_feat, t], dim=1)
        
        else:
            raise ValueError(f"Expected len(X) in [2, 3, 4] but got {dimension}")
        
        return features, t, dimension
    
    def _apply_density_constraint(self, outputs, t):
        """
        Apply hard density constraint with causality enforcement for power spectrum perturbations.
        
        For power spectrum (non-sinusoidal):
        - For t < STARTUP_DT: Density is frozen at ρ₀ (causality - information hasn't propagated)
        - For t >= STARTUP_DT: Density evolves based on USE_PARAMETERIZATION:
          * "exponential": ρ = ρ₀ × exp(clamp((t - STARTUP_DT) × ρ̂, -10, 10))
            (ensures strictly positive density)
          * "linear": ρ = ρ₀ + (t - STARTUP_DT) × ρ̂
            (linear growth from initial condition)
          * "none": ρ = ρ̂
            (direct network prediction, no transformation)
        
        The causality constraint (STARTUP_DT) is enforced for all parameterizations.
        This ensures density remains at initial conditions until information has had time
        to propagate across the domain (finite signal speed).
        
        For sinusoidal: No constraint (returns as-is).
        
        Args:
            outputs: Raw network outputs [ρ̂, vx, vy?, vz?, phi]
            t: Time tensor
        
        Returns:
            Modified outputs with density constraint applied [ρ, vx, vy?, vz?, phi]
        """
        if str(PERTURBATION_TYPE).lower() == "sinusoidal":
            return outputs
        
        # Causality constraint for power spectrum:
        # Density frozen at ρ₀ for t < STARTUP_DT (information propagation delay)
        rho_hat = outputs[:, 0:1]
        other = outputs[:, 1:]
        
        # Create mask for causality: 1.0 where t >= STARTUP_DT, 0.0 where t < STARTUP_DT
        causal_mask = (t >= STARTUP_DT).float()
        
        # Effective time: zero for t < STARTUP_DT, (t - STARTUP_DT) for t >= STARTUP_DT
        t_effective = torch.clamp(t - STARTUP_DT, min=0.0)
        
        # Apply parameterization based on config
        parameterization = str(USE_PARAMETERIZATION).lower()
        
        if parameterization == "exponential":
            # Exponential parameterization: ρ = ρ₀ * exp(t_eff * ρ̂)
            # For t < STARTUP_DT: t_eff = 0, so exp(0) = 1, thus ρ = ρ₀
            # For t >= STARTUP_DT: ρ evolves exponentially
            rho = rho_o * torch.exp(torch.clamp(t_effective * rho_hat, min=-10, max=10))
            
        elif parameterization == "linear":
            # Linear parameterization: ρ = ρ₀ + t_eff * ρ̂
            # For t < STARTUP_DT: t_eff = 0, so ρ = ρ₀
            # For t >= STARTUP_DT: ρ grows linearly
            rho = rho_o + t_effective * rho_hat
            
        elif parameterization == "none":
            # No parameterization: direct prediction with causality enforcement
            # For t < STARTUP_DT: ρ = ρ₀ (frozen at initial condition)
            # For t >= STARTUP_DT: ρ = ρ̂ (network output directly)
            rho = causal_mask * rho_hat + (1 - causal_mask) * rho_o
            
        else:
            raise ValueError(f"Invalid USE_PARAMETERIZATION: '{USE_PARAMETERIZATION}'. "
                           f"Choose from: 'exponential', 'linear', 'none'")
        
        return torch.cat([rho, other], dim=1)
    
    def forward(self, X):
        """
        Forward pass of PINN.
        
        Args:
            X: List of coordinates [x, ...spatial..., t]
        
        Returns:
            Network predictions [rho, vx, vy?, vz?, phi]
        """
        features, t, dimension = self._prepare_coordinate_features(X)
        
        # Select appropriate branch based on dimension
        if dimension == 2:
            outputs = self.branch_1d(features)
        elif dimension == 3:
            outputs = self.branch_2d(features)
        elif dimension == 4:
            outputs = self.branch_3d(features)
        else:
            raise ValueError(f"Unexpected dimension: {dimension}")
        
        return self._apply_density_constraint(outputs, t)
        
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)
_register_module('core.model_architecture', ['PINN', 'Sin', 'get_activation', 'init_weights'])

# ==== Module: training.training_diagnostics (training/training_diagnostics.py) ====
import os
import numpy as np
import torch
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec


class TrainingDiagnostics:
    """
    Comprehensive diagnostics for PINN performance analysis.
    
    TWO DIAGNOSTIC MODES:
    
    1. LONG-TERM EVOLUTION MODE (2D, 3D sinusoidal, 3D power spectrum at t>0)
       Focus: Why does PINN fail at long times?
       Generates 6 diagnostic plots:
         - Training diagnostics: Loss convergence and balance
         - PDE residual evolution: When/where/which equations fail
         - Conservation violations: Mass and momentum drift
         - Temporal error accumulation: How errors compound over time
         - Spectral bias analysis: Frequency damping and mode evolution
         - Solution stability: Detecting unphysical behavior
    
    2. IC FITTING MODE (3D power spectrum only)
       Focus: Why can't PINN fit complex initial conditions?
       Generates 5 diagnostic plots:
         - Training diagnostics: Loss convergence with IC component breakdown
         - IC spatial comparison: Predicted vs true IC fields
         - IC power spectrum: Scale-by-scale fit quality
         - IC component convergence: Which component fails to converge
         - IC metrics summary: Correlation and RMS error statistics
    
    All diagnostics run automatically when ENABLE_TRAINING_DIAGNOSTICS = True
    """

    def __init__(self, save_dir='./diagnostics/', dimension=2, perturbation_type='power_spectrum'):
        self.save_dir = save_dir
        self.dimension = dimension
        self.perturbation_type = str(perturbation_type).lower()
        self.is_3d_power_spectrum = (dimension == 3 and self.perturbation_type == 'power_spectrum')
        
        os.makedirs(save_dir, exist_ok=True)

        # Storage for tracking metrics over iterations
        self.history = {
            'iteration': [],
            'total_loss': [],
            'pde_loss': [],
            'ic_loss': [],
            'mean_rho': [],
            'max_rho': [],
        }
        
        # Additional tracking for 3D power spectrum IC diagnostics
        if self.is_3d_power_spectrum:
            self.history.update({
                'ic_rho': [],
                'ic_vx': [],
                'ic_vy': [],
                'ic_vz': [],
                'ic_phi': [],
            })

    def _prepare_inputs(self, geomtime_col):
        """Normalize collocation inputs to the model's expected format."""
        if isinstance(geomtime_col, (list, tuple)):
            return geomtime_col
        # Single tensor [N, D] -> list of [N,1]
        return [geomtime_col[:, i:i+1] for i in range(geomtime_col.shape[1])]

    def log_iteration(self, iteration, model, loss_dict, geomtime_col, ic_component_losses=None):
        """
        Call this every N iterations during training.
        
        Args:
            iteration: Current iteration number
            model: PINN model
            loss_dict: Dictionary with 'total', 'PDE', 'IC' losses
            geomtime_col: Collocation points
            ic_component_losses: (Optional) Dict with component IC losses for 3D power spectrum
        """
        self.history['iteration'].append(iteration)
        self.history['total_loss'].append(float(loss_dict.get('total', np.nan)))
        self.history['pde_loss'].append(float(loss_dict.get('PDE', loss_dict.get('pde', np.nan))))
        self.history['ic_loss'].append(float(loss_dict.get('IC', loss_dict.get('ic', np.nan))))

        # Compute diagnostics from current model state
        with torch.no_grad():
            inputs = self._prepare_inputs(geomtime_col)
            pred = model(inputs)
            # Use first channel as density/log-density proxy
            rho = pred[:, 0].detach().cpu().numpy()

            # Density statistics
            self.history['mean_rho'].append(np.nanmean(rho))
            self.history['max_rho'].append(np.nanmax(rho))
        
        # Track component IC losses for 3D power spectrum
        if self.is_3d_power_spectrum and ic_component_losses is not None:
            self.history['ic_rho'].append(float(ic_component_losses.get('rho', np.nan)))
            self.history['ic_vx'].append(float(ic_component_losses.get('vx', np.nan)))
            self.history['ic_vy'].append(float(ic_component_losses.get('vy', np.nan)))
            self.history['ic_vz'].append(float(ic_component_losses.get('vz', np.nan)))
            self.history['ic_phi'].append(float(ic_component_losses.get('phi', np.nan)))

    def plot_diagnostics(self, iteration=None):
        """
        Generate streamlined training diagnostic plot.
        Combined loss components and balance in one figure.
        """
        if len(self.history['iteration']) == 0:
            print("No diagnostic data to plot.")
            return
        
        iters = self.history['iteration']
        suffix = f"_iter_{iteration}" if iteration is not None else "_final"

        # Combined training diagnostics plot
        fig = plt.figure(figsize=(14, 5))
        gs = GridSpec(1, 3, figure=fig)
        
        # Subplot 1: Loss evolution
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.semilogy(iters, self.history['total_loss'], 'b-', linewidth=2, label='Total')
        ax1.semilogy(iters, self.history['pde_loss'], 'r-', linewidth=2, label='PDE')
        ax1.semilogy(iters, self.history['ic_loss'], 'g-', linewidth=2, label='IC')
        ax1.set_xlabel('Iteration', fontsize=11)
        ax1.set_ylabel('Loss (log scale)', fontsize=11)
        ax1.set_title('Loss Components', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Subplot 2: Loss balance ratio
        ax2 = fig.add_subplot(gs[0, 1])
        ic_arr = np.array(self.history['ic_loss'])
        pde_arr = np.array(self.history['pde_loss'])
        ratio = pde_arr / (ic_arr + 1e-10)
        ax2.plot(iters, ratio, 'purple', linewidth=2)
        ax2.axhline(1.0, color='k', linestyle='--', linewidth=1.5, label='Balanced')
        ax2.set_xlabel('Iteration', fontsize=11)
        ax2.set_ylabel('PDE Loss / IC Loss', fontsize=11)
        ax2.set_title('Loss Balance', fontsize=12, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        # Subplot 3: Density evolution
        ax3 = fig.add_subplot(gs[0, 2])
        ax3.plot(iters, self.history['mean_rho'], 'b-', linewidth=2, label='Mean ρ')
        ax3.plot(iters, self.history['max_rho'], 'r-', linewidth=2, label='Max ρ')
        ax3.set_xlabel('Iteration', fontsize=11)
        ax3.set_ylabel('Density', fontsize=11)
        ax3.set_title('Density Statistics', fontsize=12, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/training_diagnostics{suffix}.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"Training diagnostic plot saved to {self.save_dir}")

    # ==================== POST-TRAINING DIAGNOSTICS FOR HIGH-TMAX FAILURES ====================
    
    def compute_residual_heatmap(self, model, dimension, tmax, n_spatial=None, n_temporal=60):
        """
        Compute PDE residuals on a regular grid over space-time.
        Critical for identifying WHERE and WHEN physics breaks down.
        
        Note: For power spectrum cases, uses higher resolution to match IC generation grid.
        """
        from core.losses import pde_residue
        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D
        
        device = next(model.parameters()).device
        
        # Use higher resolution for power spectrum to match IC generation
        if n_spatial is None:
            if str(PERTURBATION_TYPE).lower() == "power_spectrum":
                n_spatial = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)
            else:
                n_spatial = 80
        
        print(f"  Computing residuals on {n_spatial}x{n_spatial}x{n_temporal} grid...")
        
        if dimension == 2:
            x = torch.linspace(0, 1, n_spatial, device=device)
            y = torch.linspace(0, 1, n_spatial, device=device)
            t = torch.linspace(0, tmax, n_temporal, device=device)
            
            # Create meshgrid
            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')
            colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
            
            with torch.no_grad():
                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)
            
            # Reshape back to grid
            rho_r = rho_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
            vx_r = vx_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
            vy_r = vy_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
            phi_r = phi_r.reshape(n_spatial, n_spatial, n_temporal).cpu().numpy()
            
            return {
                'x': x.cpu().numpy(),
                'y': y.cpu().numpy(),
                't': t.cpu().numpy(),
                'rho_residual': rho_r,
                'vx_residual': vx_r,
                'vy_residual': vy_r,
                'phi_residual': phi_r
            }
        
        elif dimension == 3:
            # For 3D, use coarser grid to save memory
            n_spatial_3d = max(32, n_spatial // 2)
            x = torch.linspace(0, 1, n_spatial_3d, device=device)
            y = torch.linspace(0, 1, n_spatial_3d, device=device)
            z = torch.linspace(0, 1, n_spatial_3d, device=device)
            t = torch.linspace(0, tmax, n_temporal, device=device)
            
            # For 3D, we'll slice at z=0.5 to create 2D heatmaps
            z_slice = torch.full((n_spatial_3d * n_spatial_3d * n_temporal,), 0.5, device=device)
            X, Y, T = torch.meshgrid(x, y, t, indexing='ij')
            colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), z_slice.reshape(-1, 1), T.reshape(-1, 1)]
            
            with torch.no_grad():
                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)
            
            rho_r = rho_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
            vx_r = vx_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
            vy_r = vy_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
            phi_r = phi_r.reshape(n_spatial_3d, n_spatial_3d, n_temporal).cpu().numpy()
            
            return {
                'x': x.cpu().numpy(),
                'y': y.cpu().numpy(),
                't': t.cpu().numpy(),
                'rho_residual': rho_r,
                'vx_residual': vx_r,
                'vy_residual': vy_r,
                'phi_residual': phi_r
            }
    
    def plot_residual_heatmaps(self, residual_data, slice_idx=None):
        """
        Plot 2D heatmaps of PDE residuals over (x, t) at fixed y.
        Shows WHEN and WHERE each equation fails.
        """
        if slice_idx is None:
            slice_idx = residual_data['rho_residual'].shape[1] // 2
        
        fig = plt.figure(figsize=(16, 10))
        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
        
        residuals = [
            ('Continuity Equation', residual_data['rho_residual']),
            ('Momentum X Equation', residual_data['vx_residual']),
            ('Momentum Y Equation', residual_data['vy_residual']),
            ('Poisson Equation', residual_data['phi_residual'])
        ]
        
        for idx, (name, resid) in enumerate(residuals):
            ax = fig.add_subplot(gs[idx // 2, idx % 2])
            
            # Slice at fixed y
            resid_slice = resid[:, slice_idx, :].T  # (t, x) for imshow
            
            # Use log scale for better visualization
            resid_log = np.log10(np.abs(resid_slice) + 1e-12)
            
            im = ax.imshow(resid_log, aspect='auto', origin='lower', cmap='hot',
                          extent=[residual_data['x'][0], residual_data['x'][-1],
                                 residual_data['t'][0], residual_data['t'][-1]],
                          vmin=-10, vmax=0)
            ax.set_xlabel('x', fontsize=11)
            ax.set_ylabel('Time', fontsize=11)
            ax.set_title(f'{name} Residual (log₁₀)', fontsize=12, fontweight='bold')
            cbar = plt.colorbar(im, ax=ax)
            cbar.set_label('log₁₀|residual|', fontsize=10)
        
        plt.suptitle(f'PDE Residuals Over Space-Time (y={residual_data["y"][slice_idx]:.2f})', 
                    fontsize=14, fontweight='bold', y=0.995)
        plt.savefig(f'{self.save_dir}/residual_heatmaps.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def check_conservation_laws(self, model, dimension, tmax, n_times=30, n_grid=None):
        """
        Check if mass and momentum are conserved over time.
        Critical for physical consistency - should be flat lines!
        
        Note: Uses higher resolution for power spectrum to ensure accurate integration.
        """
        from config import PERTURBATION_TYPE, N_GRID, N_GRID_3D
        
        device = next(model.parameters()).device
        
        # Use higher resolution for power spectrum
        if n_grid is None:
            if str(PERTURBATION_TYPE).lower() == "power_spectrum":
                n_grid = min(150, N_GRID // 3) if dimension == 2 else min(100, N_GRID_3D // 3)
            else:
                n_grid = 100
        
        print(f"  Checking conservation laws at {n_times} time points on {n_grid}x{n_grid} grid...")
        
        conservation_data = {
            'times': [],
            'total_mass': [],
            'total_momentum_x': [],
            'total_momentum_y': []
        }
        
        time_points = np.linspace(0, tmax, n_times)
        
        for t_val in time_points:
            if dimension == 2:
                x = torch.linspace(0, 1, n_grid, device=device)
                y = torch.linspace(0, 1, n_grid, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                T = torch.full_like(X, t_val)
                
                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
                
                with torch.no_grad():
                    pred = model(colloc)
                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()
                    vx = pred[:, 1].reshape(n_grid, n_grid).cpu().numpy()
                    vy = pred[:, 2].reshape(n_grid, n_grid).cpu().numpy()
                
                # Integrate using trapezoidal rule
                dx = 1.0 / (n_grid - 1)
                total_mass = np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx)
                total_px = np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx)
                total_py = np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx)
                
                conservation_data['times'].append(t_val)
                conservation_data['total_mass'].append(total_mass)
                conservation_data['total_momentum_x'].append(total_px)
                conservation_data['total_momentum_y'].append(total_py)
            
            elif dimension == 3:
                # For 3D, use coarser grid
                n_grid_3d = max(40, n_grid // 2)
                x = torch.linspace(0, 1, n_grid_3d, device=device)
                y = torch.linspace(0, 1, n_grid_3d, device=device)
                z = torch.linspace(0, 1, n_grid_3d, device=device)
                X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')
                T = torch.full_like(X, t_val)
                
                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]
                
                with torch.no_grad():
                    pred = model(colloc)
                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()
                    vx = pred[:, 1].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()
                    vy = pred[:, 2].reshape(n_grid_3d, n_grid_3d, n_grid_3d).cpu().numpy()
                
                dx = 1.0 / (n_grid_3d - 1)
                total_mass = np.trapz(np.trapz(np.trapz(rho, dx=dx, axis=0), dx=dx, axis=0), dx=dx)
                total_px = np.trapz(np.trapz(np.trapz(rho * vx, dx=dx, axis=0), dx=dx, axis=0), dx=dx)
                total_py = np.trapz(np.trapz(np.trapz(rho * vy, dx=dx, axis=0), dx=dx, axis=0), dx=dx)
                
                conservation_data['times'].append(t_val)
                conservation_data['total_mass'].append(total_mass)
                conservation_data['total_momentum_x'].append(total_px)
                conservation_data['total_momentum_y'].append(total_py)
        
        return conservation_data
    
    def plot_conservation_laws(self, conservation_data):
        """Plot conserved quantities over time - should be flat lines!"""
        fig = plt.figure(figsize=(16, 5))
        gs = GridSpec(1, 3, figure=fig)
        
        times = conservation_data['times']
        
        # Mass conservation
        ax1 = fig.add_subplot(gs[0, 0])
        mass = np.array(conservation_data['total_mass'])
        mass_initial = mass[0]
        mass_drift = ((mass - mass_initial) / mass_initial) * 100  # Percent drift
        ax1.plot(times, mass_drift, 'b-', linewidth=2.5)
        ax1.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')
        ax1.set_xlabel('Time', fontsize=11)
        ax1.set_ylabel('Mass drift (%)', fontsize=11)
        ax1.set_title('Mass Conservation', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Momentum X conservation
        ax2 = fig.add_subplot(gs[0, 1])
        px = np.array(conservation_data['total_momentum_x'])
        px_initial = px[0]
        
        # Handle near-zero initial momentum properly
        if abs(px_initial) < 1e-10:
            # Plot absolute change for near-zero initial momentum
            px_drift = (px - px_initial) * 1000  # Scale to milliunit for visibility
            ylabel = 'Momentum X absolute change (×10⁻³)'
            title_suffix = ' (near-zero initial)'
        else:
            # Plot percentage change for non-zero initial momentum
            px_drift = ((px - px_initial) / abs(px_initial)) * 100
            ylabel = 'Momentum X drift (%)'
            title_suffix = ''
        
        ax2.plot(times, px_drift, 'g-', linewidth=2.5)
        ax2.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')
        ax2.set_xlabel('Time', fontsize=11)
        ax2.set_ylabel(ylabel, fontsize=11)
        ax2.set_title('Momentum X Conservation' + title_suffix, fontsize=12, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        # Momentum Y conservation
        ax3 = fig.add_subplot(gs[0, 2])
        py = np.array(conservation_data['total_momentum_y'])
        py_initial = py[0]
        
        # Handle near-zero initial momentum properly
        if abs(py_initial) < 1e-10:
            # Plot absolute change for near-zero initial momentum
            py_drift = (py - py_initial) * 1000  # Scale to milliunit for visibility
            ylabel = 'Momentum Y absolute change (×10⁻³)'
            title_suffix = ' (near-zero initial)'
        else:
            # Plot percentage change for non-zero initial momentum
            py_drift = ((py - py_initial) / abs(py_initial)) * 100
            ylabel = 'Momentum Y drift (%)'
            title_suffix = ''
        
        ax3.plot(times, py_drift, 'orange', linewidth=2.5)
        ax3.axhline(0, color='r', linestyle='--', linewidth=1.5, label='Perfect conservation')
        ax3.set_xlabel('Time', fontsize=11)
        ax3.set_ylabel(ylabel, fontsize=11)
        ax3.set_title('Momentum Y Conservation' + title_suffix, fontsize=12, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/conservation_laws.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def compute_spectral_content(self, model, dimension, tmax, n_times=5, n_grid=128):
        """
        Compute FFT of density field at different times.
        Reveals spectral bias and high-frequency mode damping.
        """
        device = next(model.parameters()).device
        
        print(f"  Computing spectral content at {n_times} time points...")
        
        time_points = np.linspace(0, tmax, n_times)
        spectra = []
        
        for t_val in time_points:
            if dimension == 2:
                x = torch.linspace(0, 1, n_grid, device=device)
                y = torch.linspace(0, 1, n_grid, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                T = torch.full_like(X, t_val)
                
                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
                
                with torch.no_grad():
                    pred = model(colloc)
                    rho = pred[:, 0].reshape(n_grid, n_grid).cpu().numpy()
                
                # 2D FFT
                fft = np.fft.fft2(rho)
                power = np.abs(np.fft.fftshift(fft))**2
                
                # Create wavenumber grid
                kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)
                ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)
                kx_shift = np.fft.fftshift(kx)
                ky_shift = np.fft.fftshift(ky)
                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')
                K = np.sqrt(KX**2 + KY**2)
                
                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})
            
            elif dimension == 3:
                # For 3D, take a 2D slice at z=0.5
                n_grid_3d = max(64, n_grid // 2)
                x = torch.linspace(0, 1, n_grid_3d, device=device)
                y = torch.linspace(0, 1, n_grid_3d, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                Z = torch.full_like(X, 0.5)
                T = torch.full_like(X, t_val)
                
                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]
                
                with torch.no_grad():
                    pred = model(colloc)
                    rho = pred[:, 0].reshape(n_grid_3d, n_grid_3d).cpu().numpy()
                
                fft = np.fft.fft2(rho)
                power = np.abs(np.fft.fftshift(fft))**2
                
                kx = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)
                ky = np.fft.fftfreq(n_grid_3d, d=1.0/n_grid_3d)
                kx_shift = np.fft.fftshift(kx)
                ky_shift = np.fft.fftshift(ky)
                KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')
                K = np.sqrt(KX**2 + KY**2)
                
                spectra.append({'time': t_val, 'power': power, 'k': K, 'rho': rho})
        
        return spectra
    
    def plot_spectral_evolution(self, spectra_data):
        """
        Plot power spectrum evolution over time.
        Shows if high-frequency modes are being damped (spectral bias).
        """
        fig = plt.figure(figsize=(16, 5))
        gs = GridSpec(1, 2, figure=fig)
        
        # Left: Radially averaged power spectra
        ax1 = fig.add_subplot(gs[0, 0])
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))
        
        for idx, spec in enumerate(spectra_data):
            # Radial binning
            k_max = np.max(spec['k'])
            k_bins = np.linspace(0, k_max, 25)
            power_avg = []
            k_centers = []
            
            for i in range(len(k_bins)-1):
                mask = (spec['k'] >= k_bins[i]) & (spec['k'] < k_bins[i+1])
                if mask.any():
                    power_avg.append(np.mean(spec['power'][mask]))
                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)
            
            if len(k_centers) > 0:
                ax1.loglog(k_centers, power_avg, '-o', color=colors[idx], 
                          linewidth=2, markersize=4, label=f"t={spec['time']:.2f}")
        
        ax1.set_xlabel('Wavenumber k', fontsize=11)
        ax1.set_ylabel('Power', fontsize=11)
        ax1.set_title('Power Spectrum Evolution', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=9, loc='best')
        ax1.grid(True, alpha=0.3, which='both')
        
        # Right: 2D power spectrum at final time
        ax2 = fig.add_subplot(gs[0, 1])
        final_spec = spectra_data[-1]
        power_log = np.log10(final_spec['power'] + 1e-12)
        
        im = ax2.imshow(power_log, origin='lower', cmap='hot', aspect='auto',
                       extent=[-np.max(final_spec['k']), np.max(final_spec['k']),
                              -np.max(final_spec['k']), np.max(final_spec['k'])])
        ax2.set_xlabel('k_x', fontsize=11)
        ax2.set_ylabel('k_y', fontsize=11)
        ax2.set_title(f'2D Power Spectrum at t={final_spec["time"]:.2f}', 
                     fontsize=12, fontweight='bold')
        cbar = plt.colorbar(im, ax=ax2)
        cbar.set_label('log₁₀(Power)', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/spectral_evolution.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def compute_temporal_statistics(self, model, dimension, tmax, n_times=50, n_spatial=100):
        """
        Compute field statistics over time to track error accumulation.
        """
        device = next(model.parameters()).device
        
        print(f"  Computing temporal statistics at {n_times} time points...")
        
        time_points = np.linspace(0, tmax, n_times)
        stats = {
            'times': [],
            'rho_mean': [],
            'rho_std': [],
            'rho_max': [],
            'grad_rho_mean': [],
            'grad_rho_max': []
        }
        
        for t_val in time_points:
            if dimension == 2:
                x = torch.linspace(0, 1, n_spatial, device=device)
                y = torch.linspace(0, 1, n_spatial, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                T = torch.full_like(X, t_val)
                
                X_flat = X.reshape(-1, 1)
                Y_flat = Y.reshape(-1, 1)
                T_flat = T.reshape(-1, 1)
                
                X_flat.requires_grad_(True)
                Y_flat.requires_grad_(True)
                
                colloc = [X_flat, Y_flat, T_flat]
                
                pred = model(colloc)
                rho = pred[:, 0:1]
                
                # Compute gradients (retain_graph=True for first call since we need multiple gradients)
                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False)[0]
                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2)
                
                rho_np = rho.detach().cpu().numpy().flatten()
                grad_np = grad_mag.detach().cpu().numpy().flatten()
                
                stats['times'].append(t_val)
                stats['rho_mean'].append(np.mean(rho_np))
                stats['rho_std'].append(np.std(rho_np))
                stats['rho_max'].append(np.max(rho_np))
                stats['grad_rho_mean'].append(np.mean(grad_np))
                stats['grad_rho_max'].append(np.max(grad_np))
            
            elif dimension == 3:
                # For 3D, take a 2D slice at z=0.5 for computational efficiency
                n_spatial_3d = max(50, n_spatial // 2)
                x = torch.linspace(0, 1, n_spatial_3d, device=device)
                y = torch.linspace(0, 1, n_spatial_3d, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                Z = torch.full_like(X, 0.5)
                T = torch.full_like(X, t_val)
                
                X_flat = X.reshape(-1, 1)
                Y_flat = Y.reshape(-1, 1)
                Z_flat = Z.reshape(-1, 1)
                T_flat = T.reshape(-1, 1)
                
                X_flat.requires_grad_(True)
                Y_flat.requires_grad_(True)
                Z_flat.requires_grad_(True)
                
                colloc = [X_flat, Y_flat, Z_flat, T_flat]
                
                pred = model(colloc)
                rho = pred[:, 0:1]
                
                # Compute gradients
                grad_rho_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                grad_rho_y = torch.autograd.grad(rho.sum(), Y_flat, create_graph=False, retain_graph=True)[0]
                grad_rho_z = torch.autograd.grad(rho.sum(), Z_flat, create_graph=False)[0]
                grad_mag = torch.sqrt(grad_rho_x**2 + grad_rho_y**2 + grad_rho_z**2)
                
                rho_np = rho.detach().cpu().numpy().flatten()
                grad_np = grad_mag.detach().cpu().numpy().flatten()
                
                stats['times'].append(t_val)
                stats['rho_mean'].append(np.mean(rho_np))
                stats['rho_std'].append(np.std(rho_np))
                stats['rho_max'].append(np.max(rho_np))
                stats['grad_rho_mean'].append(np.mean(grad_np))
                stats['grad_rho_max'].append(np.max(grad_np))
        
        return stats
    
    def plot_temporal_statistics(self, stats_data):
        """
        Plot field statistics over time.
        Shows if solution becomes unphysical or gradients explode.
        """
        fig = plt.figure(figsize=(16, 5))
        gs = GridSpec(1, 3, figure=fig)
        
        times = stats_data['times']
        
        # Density statistics
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.plot(times, stats_data['rho_mean'], 'b-', linewidth=2.5, label='Mean')
        ax1.fill_between(times,
                         np.array(stats_data['rho_mean']) - np.array(stats_data['rho_std']),
                         np.array(stats_data['rho_mean']) + np.array(stats_data['rho_std']),
                         alpha=0.3, color='b', label='±1 std')
        ax1.plot(times, stats_data['rho_max'], 'r-', linewidth=2, label='Max')
        ax1.set_xlabel('Time', fontsize=11)
        ax1.set_ylabel('Density', fontsize=11)
        ax1.set_title('Density Evolution', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Gradient statistics
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.semilogy(times, stats_data['grad_rho_mean'], 'b-', linewidth=2.5, label='Mean')
        ax2.semilogy(times, stats_data['grad_rho_max'], 'r-', linewidth=2, label='Max')
        ax2.set_xlabel('Time', fontsize=11)
        ax2.set_ylabel('|∇ρ| (log scale)', fontsize=11)
        ax2.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3, which='both')
        
        # Growth rate analysis
        ax3 = fig.add_subplot(gs[0, 2])
        rho_max = np.array(stats_data['rho_max'])
        if len(rho_max) > 5:
            # Fit exponential growth
            log_rho = np.log(rho_max + 1e-10)
            valid_idx = np.isfinite(log_rho)
            if valid_idx.sum() > 5:
                times_valid = np.array(times)[valid_idx]
                log_rho_valid = log_rho[valid_idx]
                coeffs = np.polyfit(times_valid, log_rho_valid, 1)
                growth_rate = coeffs[0]
                fit = np.exp(np.poly1d(coeffs)(times))
                
                ax3.semilogy(times, rho_max, 'b-', linewidth=2.5, label='Max ρ')
                ax3.semilogy(times, fit, 'r--', linewidth=2, 
                           label=f'Exp fit (γ={growth_rate:.4f})')
                ax3.set_xlabel('Time', fontsize=11)
                ax3.set_ylabel('Max Density (log scale)', fontsize=11)
                ax3.set_title('Density Growth Rate', fontsize=12, fontweight='bold')
                ax3.legend(fontsize=10)
                ax3.grid(True, alpha=0.3, which='both')
        
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/temporal_statistics.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def compute_temporal_error_accumulation(self, model, dimension, tmax, n_times=30):
        """
        Track how prediction errors grow and accumulate over time.
        Helps identify when the solution starts diverging from physical behavior.
        
        Now computes ACTUAL PDE residuals and gradient norms including phi.
        """
        from core.losses import pde_residue
        
        device = next(model.parameters()).device
        time_points = np.linspace(0, tmax, n_times)
        
        error_data = {
            'times': time_points,
            'pde_residuals': {'continuity': [], 'momentum_x': [], 'momentum_y': [], 'poisson': []},
            'field_ranges': {'rho': [], 'vx': [], 'vy': []},
            'gradient_norms': {'rho': [], 'vx': [], 'vy': [], 'phi': []},
            'max_residuals': []
        }
        
        if dimension == 3:
            error_data['pde_residuals']['momentum_z'] = []
            error_data['field_ranges']['vz'] = []
            error_data['gradient_norms']['vz'] = []
        
        n_spatial = 80 if dimension == 2 else 60
        
        for t_val in time_points:
            if dimension == 2:
                x = torch.linspace(0, 1, n_spatial, device=device)
                y = torch.linspace(0, 1, n_spatial, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                T = torch.full_like(X, t_val)
                
                X_flat = X.reshape(-1, 1).requires_grad_(True)
                Y_flat = Y.reshape(-1, 1).requires_grad_(True)
                T_flat = T.reshape(-1, 1).requires_grad_(True)
                
                colloc = [X_flat, Y_flat, T_flat]
            else:  # dimension == 3
                x = torch.linspace(0, 1, n_spatial, device=device)
                y = torch.linspace(0, 1, n_spatial, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                Z = torch.full_like(X, 0.5)
                T = torch.full_like(X, t_val)
                
                X_flat = X.reshape(-1, 1).requires_grad_(True)
                Y_flat = Y.reshape(-1, 1).requires_grad_(True)
                Z_flat = Z.reshape(-1, 1).requires_grad_(True)
                T_flat = T.reshape(-1, 1).requires_grad_(True)
                
                colloc = [X_flat, Y_flat, Z_flat, T_flat]
            
            # Compute ACTUAL PDE residuals using the proper residue function
            if dimension == 2:
                rho_r, vx_r, vy_r, phi_r = pde_residue(colloc, model, dimension=2)
                
                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())
                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())
                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())
                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())
                
            else:  # 3D
                rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc, model, dimension=3)
                
                error_data['pde_residuals']['continuity'].append(torch.abs(rho_r).mean().item())
                error_data['pde_residuals']['momentum_x'].append(torch.abs(vx_r).mean().item())
                error_data['pde_residuals']['momentum_y'].append(torch.abs(vy_r).mean().item())
                error_data['pde_residuals']['momentum_z'].append(torch.abs(vz_r).mean().item())
                error_data['pde_residuals']['poisson'].append(torch.abs(phi_r).mean().item())
            
            # Get predictions for field statistics
            pred = model(colloc)
            rho = pred[:, 0:1]
            vx = pred[:, 1:2]
            vy = pred[:, 2:3]
            phi = pred[:, -1:]  # Last column is phi
            
            # Track field statistics
            error_data['field_ranges']['rho'].append(torch.std(rho).item())
            error_data['field_ranges']['vx'].append(torch.std(vx).item())
            error_data['field_ranges']['vy'].append(torch.std(vy).item())
            if dimension == 3:
                vz = pred[:, 3:4]
                error_data['field_ranges']['vz'].append(torch.std(vz).item())
            
            # Track gradient norms for ALL fields including phi
            with torch.enable_grad():
                rho_grad_x = torch.autograd.grad(rho.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                vx_grad_x = torch.autograd.grad(vx.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                vy_grad_x = torch.autograd.grad(vy.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                phi_grad_x = torch.autograd.grad(phi.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                
                error_data['gradient_norms']['rho'].append(torch.norm(rho_grad_x).item())
                error_data['gradient_norms']['vx'].append(torch.norm(vx_grad_x).item())
                error_data['gradient_norms']['vy'].append(torch.norm(vy_grad_x).item())
                error_data['gradient_norms']['phi'].append(torch.norm(phi_grad_x).item())
                
                if dimension == 3:
                    vz_grad_x = torch.autograd.grad(vz.sum(), X_flat, create_graph=False, retain_graph=True)[0]
                    error_data['gradient_norms']['vz'].append(torch.norm(vz_grad_x).item())
            
            # Max residual magnitude (now including Poisson!)
            max_res = max(
                error_data['pde_residuals']['continuity'][-1],
                error_data['pde_residuals']['momentum_x'][-1],
                error_data['pde_residuals']['momentum_y'][-1],
                error_data['pde_residuals']['poisson'][-1]
            )
            if dimension == 3:
                max_res = max(max_res, error_data['pde_residuals']['momentum_z'][-1])
            
            error_data['max_residuals'].append(max_res)
        
        return error_data
    
    def plot_temporal_error_accumulation(self, error_data):
        """
        Visualize how errors accumulate and grow over time.
        Shows which equations fail first and error growth rates.
        """
        fig = plt.figure(figsize=(18, 10))
        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)
        
        times = error_data['times']
        
        # Plot 1: PDE Residual Evolution (NOW WITH POISSON!)
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.semilogy(times, error_data['pde_residuals']['continuity'], 'b-', linewidth=2, label='Continuity', marker='o', markersize=4)
        ax1.semilogy(times, error_data['pde_residuals']['momentum_x'], 'r-', linewidth=2, label='Momentum X', marker='s', markersize=4)
        ax1.semilogy(times, error_data['pde_residuals']['momentum_y'], 'g-', linewidth=2, label='Momentum Y', marker='^', markersize=4)
        # IMPORTANT: Now showing actual Poisson residuals!
        ax1.semilogy(times, error_data['pde_residuals']['poisson'], 'orange', linewidth=2.5, label='Poisson', marker='*', markersize=6)
        if 'momentum_z' in error_data['pde_residuals']:
            ax1.semilogy(times, error_data['pde_residuals']['momentum_z'], 'm-', linewidth=2, label='Momentum Z', marker='d', markersize=4)
        ax1.set_xlabel('Time', fontsize=11)
        ax1.set_ylabel('Residual Magnitude (log)', fontsize=11)
        ax1.set_title('PDE Residual Evolution', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=9, loc='best')
        ax1.grid(True, alpha=0.3, which='both')
        
        # Plot 2: Field Range Evolution (variance growth)
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.plot(times, error_data['field_ranges']['rho'], 'b-', linewidth=2.5, label='ρ std', marker='o', markersize=4)
        ax2.plot(times, error_data['field_ranges']['vx'], 'r--', linewidth=2, label='vx std', marker='s', markersize=4)
        ax2.plot(times, error_data['field_ranges']['vy'], 'g--', linewidth=2, label='vy std', marker='^', markersize=4)
        if 'vz' in error_data['field_ranges']:
            ax2.plot(times, error_data['field_ranges']['vz'], 'm--', linewidth=2, label='vz std', marker='d', markersize=4)
        ax2.set_xlabel('Time', fontsize=11)
        ax2.set_ylabel('Field Standard Deviation', fontsize=11)
        ax2.set_title('Field Variability Evolution', fontsize=12, fontweight='bold')
        ax2.legend(fontsize=9)
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Maximum Residual Growth
        ax3 = fig.add_subplot(gs[0, 2])
        ax3.semilogy(times, error_data['max_residuals'], 'k-', linewidth=2.5, marker='o', markersize=5)
        if len(times) > 5:
            # Fit exponential growth
            log_res = np.log(np.array(error_data['max_residuals']) + 1e-12)
            valid_idx = np.isfinite(log_res)
            if valid_idx.sum() > 3:
                coeffs = np.polyfit(np.array(times)[valid_idx], log_res[valid_idx], 1)
                growth_rate = coeffs[0]
                fit = np.exp(np.poly1d(coeffs)(times))
                ax3.semilogy(times, fit, 'r--', linewidth=2, label=f'Exp fit (γ={growth_rate:.4f})')
                ax3.legend(fontsize=10)
        ax3.set_xlabel('Time', fontsize=11)
        ax3.set_ylabel('Max Residual (log)', fontsize=11)
        ax3.set_title('Error Amplification Rate', fontsize=12, fontweight='bold')
        ax3.grid(True, alpha=0.3, which='both')
        
        # Plot 4: Gradient Norm Evolution (NOW WITH ALL FIELDS INCLUDING PHI!)
        ax4 = fig.add_subplot(gs[1, 0])
        ax4.semilogy(times, np.array(error_data['gradient_norms']['rho']) + 1e-12, 'b-', linewidth=2.5, label='∇ρ', marker='o', markersize=4)
        ax4.semilogy(times, np.array(error_data['gradient_norms']['phi']) + 1e-12, 'orange', linewidth=2.5, label='∇φ', marker='*', markersize=6)
        ax4.semilogy(times, np.array(error_data['gradient_norms']['vx']) + 1e-12, 'r--', linewidth=1.5, label='∇vx', marker='s', markersize=3, alpha=0.7)
        ax4.semilogy(times, np.array(error_data['gradient_norms']['vy']) + 1e-12, 'g--', linewidth=1.5, label='∇vy', marker='^', markersize=3, alpha=0.7)
        if 'vz' in error_data['gradient_norms']:
            ax4.semilogy(times, np.array(error_data['gradient_norms']['vz']) + 1e-12, 'm--', linewidth=1.5, label='∇vz', marker='d', markersize=3, alpha=0.7)
        ax4.set_xlabel('Time', fontsize=11)
        ax4.set_ylabel('Gradient Norm (log)', fontsize=11)
        ax4.set_title('Gradient Magnitude Evolution', fontsize=12, fontweight='bold')
        ax4.legend(fontsize=9, loc='best')
        ax4.grid(True, alpha=0.3, which='both')
        
        # Plot 5: Cumulative Error Metric
        ax5 = fig.add_subplot(gs[1, 1])
        cumulative_error = np.cumsum(error_data['max_residuals'])
        ax5.plot(times, cumulative_error, 'purple', linewidth=2.5, marker='o', markersize=5)
        ax5.set_xlabel('Time', fontsize=11)
        ax5.set_ylabel('Cumulative Error', fontsize=11)
        ax5.set_title('Accumulated Error Budget', fontsize=12, fontweight='bold')
        ax5.grid(True, alpha=0.3)
        
        # Plot 6: Error Growth Rate
        ax6 = fig.add_subplot(gs[1, 2])
        if len(times) > 1:
            error_rate = np.diff(error_data['max_residuals']) / np.diff(times)
            ax6.plot(times[:-1], error_rate, 'orange', linewidth=2.5, marker='o', markersize=5)
            ax6.axhline(0, color='k', linestyle='--', alpha=0.5)
            ax6.set_xlabel('Time', fontsize=11)
            ax6.set_ylabel('Error Growth Rate', fontsize=11)
            ax6.set_title('Instantaneous Error Acceleration', fontsize=12, fontweight='bold')
            ax6.grid(True, alpha=0.3)
        
        plt.suptitle('Temporal Error Accumulation Analysis', fontsize=14, fontweight='bold', y=0.995)
        plt.savefig(f'{self.save_dir}/temporal_error_accumulation.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def plot_spectral_bias_analysis(self, spectra_data):
        """
        Enhanced spectral analysis focused on detecting spectral bias.
        Shows frequency damping and mode evolution over time.
        """
        fig = plt.figure(figsize=(18, 10))
        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)
        
        # Plot 1: Radially averaged power spectra at different times
        ax1 = fig.add_subplot(gs[0, :2])
        colors = plt.cm.viridis(np.linspace(0, 1, len(spectra_data)))
        
        for idx, spec in enumerate(spectra_data):
            # Radial binning
            k_max = np.max(spec['k'])
            k_bins = np.linspace(0, k_max, 25)
            power_avg = []
            k_centers = []
            
            for i in range(len(k_bins)-1):
                mask = (spec['k'] >= k_bins[i]) & (spec['k'] < k_bins[i+1])
                if mask.sum() > 0:
                    power_avg.append(np.mean(spec['power'][mask]))
                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)
            
            if len(k_centers) > 0:
                ax1.loglog(k_centers, power_avg, color=colors[idx], linewidth=2,
                          label=f't={spec["time"]:.2f}', marker='o', markersize=3)
        
        ax1.set_xlabel('Wavenumber k', fontsize=11)
        ax1.set_ylabel('Power P(k)', fontsize=11)
        ax1.set_title('Power Spectrum Evolution (Spectral Bias Check)', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=8, ncol=2)
        ax1.grid(True, alpha=0.3, which='both')
        
        # Plot 2: Spectral Damping Rate
        ax2 = fig.add_subplot(gs[0, 2])
        if len(spectra_data) >= 2:
            # Compare first and last spectra
            spec_0 = spectra_data[0]
            spec_f = spectra_data[-1]
            
            # Compute power ratio (damping)
            k_max = min(np.max(spec_0['k']), np.max(spec_f['k']))
            k_bins = np.linspace(0, k_max, 20)
            damping_ratio = []
            k_centers = []
            
            for i in range(len(k_bins)-1):
                mask_0 = (spec_0['k'] >= k_bins[i]) & (spec_0['k'] < k_bins[i+1])
                mask_f = (spec_f['k'] >= k_bins[i]) & (spec_f['k'] < k_bins[i+1])
                if mask_0.sum() > 0 and mask_f.sum() > 0:
                    p0 = np.mean(spec_0['power'][mask_0])
                    pf = np.mean(spec_f['power'][mask_f])
                    if p0 > 0:
                        damping_ratio.append(pf / p0)
                        k_centers.append((k_bins[i] + k_bins[i+1]) / 2)
            
            if len(k_centers) > 0:
                ax2.semilogx(k_centers, damping_ratio, 'b-', linewidth=2.5, marker='o', markersize=5)
                ax2.axhline(1.0, color='r', linestyle='--', linewidth=2, label='No damping')
                ax2.set_xlabel('Wavenumber k', fontsize=11)
                ax2.set_ylabel('Power Ratio (final/initial)', fontsize=11)
                ax2.set_title('Spectral Damping by Frequency', fontsize=12, fontweight='bold')
                ax2.legend(fontsize=10)
                ax2.grid(True, alpha=0.3)
        
        # Plot 3: High-k Mode Tracking
        ax3 = fig.add_subplot(gs[1, 0])
        times = [spec['time'] for spec in spectra_data]
        
        # Track power in different k ranges
        low_k_power = []
        mid_k_power = []
        high_k_power = []
        
        for spec in spectra_data:
            k_flat = spec['k'].flatten()
            p_flat = spec['power'].flatten()
            
            k_max = np.max(k_flat)
            low_k_power.append(np.mean(p_flat[k_flat < k_max/3]))
            mid_k_power.append(np.mean(p_flat[(k_flat >= k_max/3) & (k_flat < 2*k_max/3)]))
            high_k_power.append(np.mean(p_flat[k_flat >= 2*k_max/3]))
        
        ax3.semilogy(times, low_k_power, 'b-', linewidth=2.5, label='Low-k (large scales)', marker='o', markersize=5)
        ax3.semilogy(times, mid_k_power, 'g-', linewidth=2.5, label='Mid-k', marker='s', markersize=5)
        ax3.semilogy(times, high_k_power, 'r-', linewidth=2.5, label='High-k (small scales)', marker='^', markersize=5)
        ax3.set_xlabel('Time', fontsize=11)
        ax3.set_ylabel('Average Power (log)', fontsize=11)
        ax3.set_title('Scale-Dependent Power Evolution', fontsize=12, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3, which='both')
        
        # Plot 4: Spectral Index Evolution
        ax4 = fig.add_subplot(gs[1, 1])
        spectral_indices = []
        for spec in spectra_data:
            k_flat = spec['k'].flatten()
            p_flat = spec['power'].flatten()
            
            # Fit power law in log space
            valid = (k_flat > 0) & (p_flat > 0)
            if valid.sum() > 10:
                log_k = np.log(k_flat[valid])
                log_p = np.log(p_flat[valid])
                coeffs = np.polyfit(log_k, log_p, 1)
                spectral_indices.append(coeffs[0])  # Slope = spectral index
            else:
                spectral_indices.append(np.nan)
        
        ax4.plot(times, spectral_indices, 'purple', linewidth=2.5, marker='o', markersize=6)
        ax4.axhline(0, color='k', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Time', fontsize=11)
        ax4.set_ylabel('Spectral Index (slope)', fontsize=11)
        ax4.set_title('Power Law Evolution', fontsize=12, fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # Plot 5: Density field snapshots
        ax5 = fig.add_subplot(gs[1, 2])
        # Show first, middle, last density fields
        n_show = min(3, len(spectra_data))
        indices = [0, len(spectra_data)//2, -1] if n_show == 3 else [0, -1]
        
        for i, idx in enumerate(indices[:n_show]):
            rho = spectra_data[idx]['rho']
            # Compute structure function or variance
            rho_var = np.var(rho)
            ax5.bar(i, rho_var, color=colors[idx], edgecolor='black', linewidth=1.5)
            ax5.text(i, rho_var, f't={spectra_data[idx]["time"]:.2f}', 
                    ha='center', va='bottom', fontsize=9)
        
        ax5.set_xlabel('Time Snapshot', fontsize=11)
        ax5.set_ylabel('Density Variance', fontsize=11)
        ax5.set_title('Structure Growth', fontsize=12, fontweight='bold')
        ax5.set_xticks(range(n_show))
        ax5.set_xticklabels(['Early', 'Mid', 'Late'][:n_show])
        ax5.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Spectral Bias and Mode Damping Analysis', fontsize=14, fontweight='bold', y=0.995)
        plt.savefig(f'{self.save_dir}/spectral_bias_analysis.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def compute_solution_stability(self, model, dimension, tmax, stats_data, n_check=20):
        """
        Check for signs of solution instability or unphysical behavior.
        """
        device = next(model.parameters()).device
        time_points = np.linspace(0, tmax, n_check)
        
        stability_data = {
            'times': time_points,
            'density_bounds': {'min': [], 'max': [], 'mean': []},
            'velocity_norms': [],
            'negative_density_fraction': [],
            'extreme_values': [],
            'solution_smoothness': []
        }
        
        n_spatial = 80 if dimension == 2 else 60
        
        for t_val in time_points:
            if dimension == 2:
                x = torch.linspace(0, 1, n_spatial, device=device)
                y = torch.linspace(0, 1, n_spatial, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                T = torch.full_like(X, t_val)
                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
            else:
                x = torch.linspace(0, 1, n_spatial, device=device)
                y = torch.linspace(0, 1, n_spatial, device=device)
                X, Y = torch.meshgrid(x, y, indexing='ij')
                Z = torch.full_like(X, 0.5)
                T = torch.full_like(X, t_val)
                colloc = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]
            
            with torch.no_grad():
                pred = model(colloc)
                rho = pred[:, 0].cpu().numpy()
                vx = pred[:, 1].cpu().numpy()
                vy = pred[:, 2].cpu().numpy()
                
                # Density bounds
                stability_data['density_bounds']['min'].append(np.min(rho))
                stability_data['density_bounds']['max'].append(np.max(rho))
                stability_data['density_bounds']['mean'].append(np.mean(rho))
                
                # Velocity norms
                v_norm = np.sqrt(vx**2 + vy**2)
                if dimension == 3:
                    vz = pred[:, 3].cpu().numpy()
                    v_norm = np.sqrt(vx**2 + vy**2 + vz**2)
                stability_data['velocity_norms'].append(np.max(v_norm))
                
                # Negative density fraction
                neg_frac = np.sum(rho < 0) / len(rho)
                stability_data['negative_density_fraction'].append(neg_frac)
                
                # Extreme value detection (more than 10x mean)
                extreme_frac = np.sum(np.abs(rho - np.mean(rho)) > 10 * np.std(rho)) / len(rho)
                stability_data['extreme_values'].append(extreme_frac)
                
                # Solution smoothness (Laplacian magnitude as proxy)
                if dimension == 2:
                    rho_2d = rho.reshape(n_spatial, n_spatial)
                    laplacian = np.abs(np.gradient(np.gradient(rho_2d, axis=0), axis=0) + 
                                     np.gradient(np.gradient(rho_2d, axis=1), axis=1))
                    stability_data['solution_smoothness'].append(np.mean(laplacian))
                else:
                    stability_data['solution_smoothness'].append(0.0)  # Placeholder for 3D
        
        return stability_data
    
    def plot_solution_stability(self, stability_data):
        """
        Visualize solution stability metrics to detect unphysical behavior.
        """
        fig = plt.figure(figsize=(18, 10))
        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)
        
        times = stability_data['times']
        
        # Plot 1: Density Bounds
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.plot(times, stability_data['density_bounds']['min'], 'b-', linewidth=2.5, label='Min', marker='v', markersize=5)
        ax1.plot(times, stability_data['density_bounds']['mean'], 'g-', linewidth=2.5, label='Mean', marker='o', markersize=5)
        ax1.plot(times, stability_data['density_bounds']['max'], 'r-', linewidth=2.5, label='Max', marker='^', markersize=5)
        ax1.axhline(0, color='k', linestyle='--', alpha=0.5, label='Zero')
        ax1.set_xlabel('Time', fontsize=11)
        ax1.set_ylabel('Density', fontsize=11)
        ax1.set_title('Density Bounds Check', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Velocity Norms
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.semilogy(times, np.array(stability_data['velocity_norms']) + 1e-12, 'purple', linewidth=2.5, marker='o', markersize=5)
        ax2.set_xlabel('Time', fontsize=11)
        ax2.set_ylabel('Max Velocity Norm (log)', fontsize=11)
        ax2.set_title('Velocity Magnitude Evolution', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3, which='both')
        
        # Plot 3: Negative Density Fraction
        ax3 = fig.add_subplot(gs[0, 2])
        ax3.plot(times, np.array(stability_data['negative_density_fraction']) * 100, 'red', linewidth=2.5, marker='o', markersize=5)
        ax3.axhline(0, color='k', linestyle='--', alpha=0.5)
        ax3.set_xlabel('Time', fontsize=11)
        ax3.set_ylabel('Negative Density (%)', fontsize=11)
        ax3.set_title('Unphysical Density Detection', fontsize=12, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: Extreme Values
        ax4 = fig.add_subplot(gs[1, 0])
        ax4.plot(times, np.array(stability_data['extreme_values']) * 100, 'orange', linewidth=2.5, marker='o', markersize=5)
        ax4.set_xlabel('Time', fontsize=11)
        ax4.set_ylabel('Extreme Value Fraction (%)', fontsize=11)
        ax4.set_title('Outlier Detection (>10σ)', fontsize=12, fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # Plot 5: Solution Smoothness
        ax5 = fig.add_subplot(gs[1, 1])
        ax5.semilogy(times, np.array(stability_data['solution_smoothness']) + 1e-12, 'green', linewidth=2.5, marker='o', markersize=5)
        ax5.set_xlabel('Time', fontsize=11)
        ax5.set_ylabel('Smoothness Metric (log)', fontsize=11)
        ax5.set_title('Solution Regularity', fontsize=12, fontweight='bold')
        ax5.grid(True, alpha=0.3, which='both')
        
        # Plot 6: Stability Summary
        ax6 = fig.add_subplot(gs[1, 2])
        # Compute overall stability score (0-1, higher is worse)
        neg_dens_score = np.array(stability_data['negative_density_fraction'])
        extreme_score = np.array(stability_data['extreme_values'])
        overall_score = 0.5 * neg_dens_score + 0.5 * extreme_score
        
        ax6.plot(times, overall_score * 100, 'darkred', linewidth=3, marker='o', markersize=6)
        ax6.fill_between(times, 0, overall_score * 100, alpha=0.3, color='red')
        ax6.axhline(5, color='orange', linestyle='--', linewidth=2, label='Warning (5%)')
        ax6.axhline(1, color='yellow', linestyle='--', linewidth=2, label='Acceptable (1%)')
        ax6.set_xlabel('Time', fontsize=11)
        ax6.set_ylabel('Instability Score (%)', fontsize=11)
        ax6.set_title('Overall Stability Assessment', fontsize=12, fontweight='bold')
        ax6.legend(fontsize=9)
        ax6.grid(True, alpha=0.3)
        
        plt.suptitle('Solution Stability and Physical Validity Analysis', fontsize=14, fontweight='bold', y=0.995)
        plt.savefig(f'{self.save_dir}/solution_stability.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def run_comprehensive_diagnostics(self, model, dimension, tmax):
        """
        Run all post-training diagnostics for long-term evolution analysis.
        
        Generates 6 critical diagnostic plots:
        1. Training diagnostics (loss, balance, density) - already generated
        2. PDE residual evolution - WHEN/WHERE/WHICH equations fail
        3. Conservation law violations - Mass/momentum drift over time
        4. Temporal error accumulation - How errors compound over time
        5. Spectral bias analysis - Frequency damping and mode evolution
        6. Solution stability - Detecting unphysical behavior
        
        Args:
            model: Trained PINN model
            dimension: Spatial dimension (2 or 3)
            tmax: Maximum time to analyze
        """
        print("\n" + "="*70)
        print("  Running Long-Term Evolution Diagnostics")
        print("="*70)
        
        # Plot 2: PDE Residual Evolution
        print("\n[1/5] Computing PDE residual evolution...")
        residuals = self.compute_residual_heatmap(model, dimension, tmax)
        self.plot_residual_heatmaps(residuals)
        print("      [OK] PDE residual evolution saved")
        
        # Plot 3: Conservation Laws
        print("\n[2/5] Analyzing conservation law violations...")
        conservation = self.check_conservation_laws(model, dimension, tmax)
        self.plot_conservation_laws(conservation)
        print("      [OK] Conservation violations plot saved")
        
        # Plot 4: Temporal Error Accumulation
        print("\n[3/5] Computing temporal error accumulation...")
        error_data = self.compute_temporal_error_accumulation(model, dimension, tmax)
        self.plot_temporal_error_accumulation(error_data)
        print("      [OK] Error accumulation plot saved")
        
        # Plot 5: Spectral Bias Analysis
        print("\n[4/5] Analyzing spectral bias and mode damping...")
        spectra = self.compute_spectral_content(model, dimension, tmax)
        self.plot_spectral_bias_analysis(spectra)
        print("      [OK] Spectral bias analysis saved")
        
        # Plot 6: Solution Stability
        print("\n[5/5] Checking solution stability...")
        stats = self.compute_temporal_statistics(model, dimension, tmax)
        stability_data = self.compute_solution_stability(model, dimension, tmax, stats)
        self.plot_solution_stability(stability_data)
        print("      [OK] Solution stability diagnostics saved")
        
        print("\n" + "="*70)
        print(f"  All diagnostics saved to: {self.save_dir}")
        print("="*70)
        print("\nLong-Term Evolution Diagnostic Summary:")
        print("  1. training_diagnostics.png - Training convergence and loss balance")
        print("  2. residual_heatmaps.png - Spatiotemporal PDE violation patterns")
        print("  3. conservation_laws.png - Conservation law drift over time")
        print("  4. temporal_error_accumulation.png - Error growth and compounding")
        print("  5. spectral_bias_analysis.png - Frequency damping and spectral issues")
        print("  6. solution_stability.png - Physical validity and stability metrics")
        print("="*70 + "\n")
    
    
    def run_case_specific_diagnostics(self, model, tmax=None, true_ic_data=None):
        """
        Smart dispatcher: runs appropriate diagnostics based on case type.
        
        - 2D cases (any perturbation): High-tmax temporal evolution diagnostics
        - 3D power spectrum: IC fitting diagnostics  
        - 3D sinusoidal: High-tmax temporal evolution diagnostics (fallback to 2D approach)
        
        Args:
            model: Trained PINN model
            tmax: Maximum time (required for 2D or 3D non-power-spectrum cases)
            true_ic_data: Dict with IC data (required for 3D power spectrum)
                         {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}
        """
        if self.is_3d_power_spectrum:
            # 3D power spectrum: Focus on IC fitting
            if true_ic_data is None:
                raise ValueError("true_ic_data required for 3D power spectrum diagnostics")
            self.run_3d_power_spectrum_diagnostics(model, true_ic_data)
        else:
            # All other cases: Focus on temporal evolution
            if tmax is None:
                raise ValueError("tmax required for temporal evolution diagnostics")
            self.run_comprehensive_diagnostics(model, self.dimension, tmax)
    # ==================== 3D POWER SPECTRUM IC DIAGNOSTICS ====================
    
    def compute_ic_spatial_comparison(self, model, true_ic_data, n_grid=80):
        """
        Compare predicted vs true ICs spatially for both 2D and 3D cases.
        Critical for diagnosing WHERE the network fails to fit the IC structure.
        
        Args:
            model: Trained PINN model
            true_ic_data: Dict with IC collocation points and true values
                         2D: {'colloc_IC', 'rho', 'vx', 'vy'}
                         3D: {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}
            n_grid: Grid resolution per dimension (for visualization grid)
        
        Returns:
            Dict with predicted and true fields, plus errors
        """
        device = next(model.parameters()).device
        
        print(f"  Computing IC spatial comparison on training IC points...")
        
        # Use the actual IC collocation points from training
        colloc_IC = true_ic_data['colloc_IC']
        
        # Get predictions at IC points
        with torch.no_grad():
            pred = model(colloc_IC)
            rho_pred = pred[:, 0].cpu().numpy()
            vx_pred = pred[:, 1].cpu().numpy()
            vy_pred = pred[:, 2].cpu().numpy()
            if self.dimension == 3:
                vz_pred = pred[:, 3].cpu().numpy()
                phi_pred = pred[:, 4].cpu().numpy()
            else:
                phi_pred = pred[:, 3].cpu().numpy()
        
        # Get true ICs (detach in case they have gradients)
        rho_true = true_ic_data['rho'].detach().cpu().numpy()
        vx_true = true_ic_data['vx'].detach().cpu().numpy()
        vy_true = true_ic_data['vy'].detach().cpu().numpy()
        if self.dimension == 3 and true_ic_data['vz'] is not None:
            vz_true = true_ic_data['vz'].detach().cpu().numpy()
        
        # For visualization, create a regular grid
        x = torch.linspace(0, 1, n_grid, device=device)
        y = torch.linspace(0, 1, n_grid, device=device)
        X, Y = torch.meshgrid(x, y, indexing='ij')
        T = torch.zeros_like(X)
        
        if self.dimension == 3:
            # For 3D, take z=0.5 slice
            Z = torch.full_like(X, 0.5)
            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), Z.reshape(-1, 1), T.reshape(-1, 1)]
        else:
            # For 2D
            colloc_viz = [X.reshape(-1, 1), Y.reshape(-1, 1), T.reshape(-1, 1)]
        
        # Get predictions on visualization grid
        with torch.no_grad():
            pred_viz = model(colloc_viz)
            rho_pred_viz = pred_viz[:, 0].reshape(n_grid, n_grid).cpu().numpy()
            vx_pred_viz = pred_viz[:, 1].reshape(n_grid, n_grid).cpu().numpy()
            vy_pred_viz = pred_viz[:, 2].reshape(n_grid, n_grid).cpu().numpy()
            if self.dimension == 3:
                vz_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()
                phi_pred_viz = pred_viz[:, 4].reshape(n_grid, n_grid).cpu().numpy()
            else:
                phi_pred_viz = pred_viz[:, 3].reshape(n_grid, n_grid).cpu().numpy()
        
        # For "true" visualization, we need to interpolate from IC points to regular grid
        # Extract spatial coordinates from IC collocation points (detach first)
        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()
        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()
        
        if self.dimension == 3:
            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()
            # Find points near z=0.5 for visualization
            z_tolerance = 0.1
            mask_z = np.abs(z_ic - 0.5) < z_tolerance
        else:
            mask_z = np.ones(len(x_ic), dtype=bool)
        
        if mask_z.sum() > 100:  # Need enough points for interpolation
            from scipy.interpolate import griddata
            points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])
            X_viz = X.cpu().numpy()
            Y_viz = Y.cpu().numpy()
            points_viz = np.column_stack([X_viz.flatten(), Y_viz.flatten()])
            
            # Interpolate with nearest neighbor fallback for NaN values
            rho_true_viz = griddata(points_ic, rho_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)
            vx_true_viz = griddata(points_ic, vx_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)
            vy_true_viz = griddata(points_ic, vy_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)
            
            # For uniform density (all values the same), fill NaNs with the constant value
            # For varying fields, use nearest neighbor interpolation for NaN regions
            if np.isnan(rho_true_viz).any():
                if np.std(rho_true[mask_z]) < 1e-10:
                    # Uniform field - fill with mean
                    rho_true_viz = np.nan_to_num(rho_true_viz, nan=np.mean(rho_true[mask_z]))
                else:
                    # Varying field - use nearest neighbor for gaps
                    rho_nn = griddata(points_ic, rho_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)
                    rho_true_viz = np.where(np.isnan(rho_true_viz), rho_nn, rho_true_viz)
            
            if np.isnan(vx_true_viz).any():
                vx_nn = griddata(points_ic, vx_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)
                vx_true_viz = np.where(np.isnan(vx_true_viz), vx_nn, vx_true_viz)
            
            if np.isnan(vy_true_viz).any():
                vy_nn = griddata(points_ic, vy_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)
                vy_true_viz = np.where(np.isnan(vy_true_viz), vy_nn, vy_true_viz)
            
            if self.dimension == 3:
                vz_true_viz = griddata(points_ic, vz_true[mask_z], points_viz, method='linear', fill_value=np.nan).reshape(n_grid, n_grid)
                if np.isnan(vz_true_viz).any():
                    vz_nn = griddata(points_ic, vz_true[mask_z], points_viz, method='nearest').reshape(n_grid, n_grid)
                    vz_true_viz = np.where(np.isnan(vz_true_viz), vz_nn, vz_true_viz)
        else:
            # Not enough points for interpolation, use predicted as reference
            print("  [WARN] Not enough IC points for ground truth visualization")
            rho_true_viz = rho_pred_viz
            vx_true_viz = vx_pred_viz
            vy_true_viz = vy_pred_viz
            if self.dimension == 3:
                vz_true_viz = vz_pred_viz
        
        result = {
            'x': x.cpu().numpy(),
            'y': y.cpu().numpy(),
            'rho_pred': rho_pred_viz,
            'rho_true': rho_true_viz,
            'vx_pred': vx_pred_viz,
            'vx_true': vx_true_viz,
            'vy_pred': vy_pred_viz,
            'vy_true': vy_true_viz,
            'phi_pred': phi_pred_viz,
            # Also store scatter point data for metrics
            'rho_pred_scatter': rho_pred,
            'rho_true_scatter': rho_true,
            'vx_pred_scatter': vx_pred,
            'vx_true_scatter': vx_true,
            'vy_pred_scatter': vy_pred,
            'vy_true_scatter': vy_true,
        }
        
        if self.dimension == 3:
            result.update({
                'vz_pred': vz_pred_viz,
                'vz_true': vz_true_viz,
                'vz_pred_scatter': vz_pred,
                'vz_true_scatter': vz_true,
            })
        
        return result
    
    def plot_ic_power_spectrum_comparison(self, ps_data):
        """
        Plot power spectrum comparison for all velocity components.
        Shows which scales (k-modes) are missing or damped.
        """
        fig = plt.figure(figsize=(16, 5))
        gs = GridSpec(1, 3, figure=fig)
        
        components = [('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]
        
        for idx, (comp, label) in enumerate(components):
            ax = fig.add_subplot(gs[0, idx])
            
            # Radial binning
            K = ps_data['K']
            power_pred = ps_data[f'power_{comp}_pred']
            power_true = ps_data[f'power_{comp}_true']
            
            k_max = np.max(K)
            k_bins = np.linspace(0, k_max, 30)
            power_pred_avg = []
            power_true_avg = []
            k_centers = []
            
            for i in range(len(k_bins)-1):
                mask = (K >= k_bins[i]) & (K < k_bins[i+1])
                if mask.any():
                    power_pred_avg.append(np.mean(power_pred[mask]))
                    power_true_avg.append(np.mean(power_true[mask]))
                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)
            
            if len(k_centers) > 0:
                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)
                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)
                
                ax.set_xlabel('Wavenumber k', fontsize=11)
                ax.set_ylabel('Power', fontsize=11)
                ax.set_title(f'{label} Power Spectrum', fontsize=12, fontweight='bold')
                ax.legend(fontsize=10)
                ax.grid(True, alpha=0.3, which='both')
        
        plt.suptitle('IC Power Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def plot_ic_component_convergence(self):
        """
        Plot IC component loss convergence over training.
        Shows WHICH component (rho, vx, vy, vz, phi) is hardest to fit.
        """
        if not self.is_3d_power_spectrum or len(self.history['iteration']) == 0:
            print("IC component tracking not available.")
            return
        
        iters = self.history['iteration']
        
        # Replace zeros and very small values with a minimum threshold for log plotting
        def safe_log_data(data):
            """Replace zeros/negative values with small epsilon for log plotting."""
            data_array = np.array(data)
            data_array = np.where(data_array <= 0, 1e-12, data_array)
            return data_array
        
        fig = plt.figure(figsize=(16, 10))
        gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)
        
        # Plot 1: All IC components on same plot
        ax1 = fig.add_subplot(gs[0, :])
        
        # Check if rho is zero/constant (uniform field case)
        rho_is_uniform = np.max(self.history['ic_rho']) < 1e-10
        
        if not rho_is_uniform:
            ax1.semilogy(iters, safe_log_data(self.history['ic_rho']), linewidth=2, label='ρ', 
                        marker='o', markersize=3, markevery=max(1, len(iters)//20))
        else:
            # Plot a dashed line at the bottom to indicate uniform/zero loss
            ax1.axhline(1e-12, color='C0', linestyle='--', linewidth=2, label='ρ (uniform IC, no loss)', alpha=0.5)
        
        ax1.semilogy(iters, safe_log_data(self.history['ic_vx']), linewidth=2, label='vx', 
                    marker='s', markersize=3, markevery=max(1, len(iters)//20))
        ax1.semilogy(iters, safe_log_data(self.history['ic_vy']), linewidth=2, label='vy', 
                    marker='^', markersize=3, markevery=max(1, len(iters)//20))
        ax1.semilogy(iters, safe_log_data(self.history['ic_vz']), linewidth=2, label='vz', 
                    marker='d', markersize=3, markevery=max(1, len(iters)//20))
        ax1.semilogy(iters, safe_log_data(self.history['ic_phi']), linewidth=2, label='φ', 
                    marker='v', markersize=3, markevery=max(1, len(iters)//20))
        
        ax1.set_xlabel('Iteration', fontsize=11)
        ax1.set_ylabel('Loss (log scale)', fontsize=11)
        ax1.set_title('IC Component Losses', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=10, ncol=5)
        ax1.grid(True, alpha=0.3)
        
        # Individual component plots
        components = [
            ('ic_rho', 'ρ (Density)', gs[1, 0], rho_is_uniform),
            ('ic_vx', 'vx (X-Velocity)', gs[1, 1], False),
            ('ic_vy', 'vy (Y-Velocity)', gs[1, 2], False),
        ]
        
        for hist_key, title, grid_pos, is_uniform in components:
            ax = fig.add_subplot(grid_pos)
            if is_uniform:
                # Show a flat line at bottom with annotation
                ax.axhline(1e-12, color='steelblue', linestyle='--', linewidth=2.5, alpha=0.5)
                ax.text(0.5, 0.5, 'Uniform IC\n(no spatial variation)', 
                       transform=ax.transAxes, ha='center', va='center',
                       fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
                ax.set_ylim([1e-13, 1e-10])
            else:
                ax.semilogy(iters, safe_log_data(self.history[hist_key]), linewidth=2.5, color='steelblue')
            
            ax.set_xlabel('Iteration', fontsize=10)
            ax.set_ylabel('Loss (log scale)', fontsize=10)
            ax.set_title(title, fontsize=11, fontweight='bold')
            ax.grid(True, alpha=0.3)
        
        plt.suptitle('IC Component Convergence Analysis', fontsize=14, fontweight='bold', y=0.995)
        plt.savefig(f'{self.save_dir}/ic_component_convergence.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def compute_ic_correlation_metrics(self, model, true_ic_data):
        """
        Compute spatial correlation between predicted and true ICs.
        High correlation = good spatial structure capture.
        """
        print(f"  Computing IC correlation metrics on training IC points...")
        
        # Use IC collocation points
        colloc_IC = true_ic_data['colloc_IC']
        
        # Get predictions
        with torch.no_grad():
            pred = model(colloc_IC)
            rho_pred = pred[:, 0].cpu().numpy().flatten()
            vx_pred = pred[:, 1].cpu().numpy().flatten()
            vy_pred = pred[:, 2].cpu().numpy().flatten()
            if self.dimension == 3:
                vz_pred = pred[:, 3].cpu().numpy().flatten()
        
        # Get true ICs (detach in case they have gradients)
        rho_true = true_ic_data['rho'].detach().cpu().numpy().flatten()
        vx_true = true_ic_data['vx'].detach().cpu().numpy().flatten()
        vy_true = true_ic_data['vy'].detach().cpu().numpy().flatten()
        if self.dimension == 3 and true_ic_data['vz'] is not None:
            vz_true = true_ic_data['vz'].detach().cpu().numpy().flatten()
        
        # Compute correlations - handle uniform fields (zero variance)
        def safe_corrcoef(pred, true):
            """Compute correlation, handling constant fields gracefully."""
            # Check if either field is constant
            if np.std(pred) < 1e-10 or np.std(true) < 1e-10:
                # For uniform fields, correlation is undefined
                # If both are uniform and equal, perfect match (1.0)
                # If different constants, poor match (use normalized error)
                if np.std(pred) < 1e-10 and np.std(true) < 1e-10:
                    # Both uniform - check if they match
                    mean_diff = abs(np.mean(pred) - np.mean(true))
                    return 1.0 if mean_diff < 1e-6 else 0.0
                else:
                    # One uniform, one not - cannot capture structure
                    return 0.0
            else:
                return np.corrcoef(pred, true)[0, 1]
        
        correlations = {
            'rho': safe_corrcoef(rho_pred, rho_true),
            'vx': safe_corrcoef(vx_pred, vx_true),
            'vy': safe_corrcoef(vy_pred, vy_true),
        }
        
        # Compute RMS errors
        rms_errors = {
            'rho': np.sqrt(np.mean((rho_pred - rho_true)**2)),
            'vx': np.sqrt(np.mean((vx_pred - vx_true)**2)),
            'vy': np.sqrt(np.mean((vy_pred - vy_true)**2)),
        }
        
        if self.dimension == 3:
            correlations['vz'] = safe_corrcoef(vz_pred, vz_true)
            rms_errors['vz'] = np.sqrt(np.mean((vz_pred - vz_true)**2))
        
        return correlations, rms_errors
    
    def plot_ic_metrics_summary(self, correlations, rms_errors):
        """
        Summary plot: correlation and RMS error for each IC component.
        Quick visual diagnostic of IC fitting quality.
        """
        fig = plt.figure(figsize=(14, 5))
        gs = GridSpec(1, 2, figure=fig)
        
        components = ['rho', 'vx', 'vy', 'vz']
        labels = ['ρ', 'vx', 'vy', 'vz']
        
        # Correlation plot
        ax1 = fig.add_subplot(gs[0, 0])
        corr_vals = [correlations[c] for c in components]
        
        # Handle NaN/inf correlations from uniform fields
        corr_vals_safe = []
        for val in corr_vals:
            if np.isnan(val) or np.isinf(val):
                corr_vals_safe.append(0.0)  # Show as 0 for visualization
            else:
                corr_vals_safe.append(val)
        
        colors = ['green' if c > 0.9 else 'orange' if c > 0.7 else 'red' for c in corr_vals_safe]
        bars1 = ax1.bar(labels, corr_vals_safe, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
        ax1.axhline(1.0, color='b', linestyle='--', linewidth=1.5, label='Perfect correlation')
        ax1.axhline(0.9, color='g', linestyle=':', linewidth=1.5, label='Good threshold')
        ax1.set_ylabel('Correlation Coefficient', fontsize=11)
        ax1.set_title('Spatial Correlation: Predicted vs True IC', fontsize=12, fontweight='bold')
        ax1.set_ylim([0, 1.05])
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for bar, val, orig_val in zip(bars1, corr_vals_safe, corr_vals):
            height = bar.get_height()
            if np.isnan(orig_val) or np.isinf(orig_val):
                label_text = 'N/A\n(uniform)'
                fontsize = 8
            else:
                label_text = f'{val:.3f}'
                fontsize = 10
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    label_text, ha='center', va='bottom', fontsize=fontsize, fontweight='bold')
        
        # RMS Error plot
        ax2 = fig.add_subplot(gs[0, 1])
        rms_vals = [rms_errors[c] for c in components]
        bars2 = ax2.bar(labels, rms_vals, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)
        ax2.set_ylabel('RMS Error', fontsize=11)
        ax2.set_title('RMS Error: Predicted vs True IC', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for bar, val in zip(bars2, rms_vals):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        plt.suptitle('IC Fitting Quality Metrics', fontsize=14, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/ic_metrics_summary.png', dpi=150, bbox_inches='tight')
        plt.close()
    
    def run_3d_power_spectrum_diagnostics(self, model, true_ic_data):
        """
        Run diagnostics for 3D power spectrum case.
        Focus on IC fitting quality, not temporal evolution.
        
        Generates 5 critical diagnostic plots:
        1. Training diagnostics (loss, balance, density) - already generated
        2. IC spatial comparison (predicted vs true fields)
        3. IC power spectrum comparison (scale-by-scale)
        4. IC component convergence (which component fails)
        5. IC metrics summary (correlation and RMS error)
        
        Args:
            model: Trained PINN model
            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}
        """
        print("\n" + "="*70)
        print("  Running 3D Power Spectrum IC Diagnostics")
        print("="*70)
        
        # Plot 2: IC Spatial Comparison
        print("\n[1/4] Computing IC spatial comparison...")
        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)
        self.plot_ic_spatial_comparison(ic_data)
        print("      [OK] IC spatial comparison saved")
        
        # Plot 3: IC Power Spectrum Comparison
        print("\n[2/4] Computing IC power spectrum comparison...")
        ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)
        self.plot_ic_power_spectrum_comparison(ps_data)
        print("      [OK] IC power spectrum comparison saved")
        
        # Plot 4: IC Component Convergence
        print("\n[3/4] Plotting IC component convergence...")
        self.plot_ic_component_convergence()
        print("      [OK] IC component convergence saved")
        
        # Plot 5: IC Metrics Summary
        print("\n[4/4] Computing IC metrics summary...")
        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)
        self.plot_ic_metrics_summary(correlations, rms_errors)
        print("      [OK] IC metrics summary saved")
        
        print("\n" + "="*70)
        print(f"  All diagnostics saved to: {self.save_dir}")
        print("="*70)
        print("\nDiagnostic Summary:")
        print("  1. training_diagnostics.png - Training convergence analysis")
        print("  2. ic_spatial_comparison.png - Predicted vs true IC fields")
        print("  3. ic_power_spectrum.png - Scale-by-scale power spectrum fit")
        print("  4. ic_component_convergence.png - Which IC component fails")
        print("  5. ic_metrics_summary.png - Correlation and RMS error metrics")
        print("="*70 + "\n")
        
        # Print numerical summary
        print("\nIC Fitting Quality Summary:")
        print("-" * 50)
        print(f"{'Component':<15} {'Correlation':<15} {'RMS Error':<15}")
        print("-" * 50)
        for comp in ['rho', 'vx', 'vy', 'vz']:
            print(f"{comp:<15} {correlations[comp]:>14.4f} {rms_errors[comp]:>14.6f}")
        print("-" * 50)
        avg_corr = np.mean([correlations[c] for c in ['rho', 'vx', 'vy', 'vz']])
        print(f"{'Average':<15} {avg_corr:>14.4f}")
        print("-" * 50 + "\n")
    
    # ==================== UNIFIED DIAGNOSTICS (5 PLOTS FOR ALL CASES) ====================
    
    def run_unified_diagnostics(self, model, true_ic_data, final_iteration):
        """
        Unified diagnostics that generate exactly 5 plots for all cases (2D/3D, any perturbation).
        
        The 5 plots are:
        1. Training diagnostics (loss convergence and balance)
        2. IC spatial comparison (predicted vs true fields)
        3. IC power spectrum (for power_spectrum) or field spectrum (for sinusoidal)
        4. IC component convergence (which component fails)
        5. IC metrics summary (correlation and RMS error)
        
        Args:
            model: Trained PINN model
            true_ic_data: Dict with IC data {'colloc_IC', 'rho', 'vx', 'vy', 'vz'}
            final_iteration: Final iteration number
        """
        print("\n" + "="*70)
        print("  Running Unified Training Diagnostics (5 plots)")
        print("="*70)
        
        # Plot 1: Training diagnostics
        print("\n[1/5] Plotting training diagnostics...")
        self.plot_diagnostics(final_iteration)
        print("      [OK] Training diagnostics saved")
        
        # Plot 2: IC Spatial Comparison
        print("\n[2/5] Computing IC spatial comparison...")
        ic_data = self.compute_ic_spatial_comparison(model, true_ic_data)
        self.plot_ic_spatial_comparison(ic_data)
        print("      [OK] IC spatial comparison saved")
        
        # Plot 3: IC Power/Field Spectrum Comparison
        print("\n[3/5] Computing spectrum comparison...")
        if self.perturbation_type == 'power_spectrum':
            # For power spectrum: compare power spectra
            ps_data = self.compute_ic_power_spectrum_comparison(model, true_ic_data)
            self.plot_ic_power_spectrum_comparison(ps_data)
        else:
            # For sinusoidal: compute field spectra from predictions
            ps_data = self.compute_field_spectrum_comparison(model, true_ic_data)
            self.plot_field_spectrum_comparison(ps_data)
        print("      [OK] Spectrum comparison saved")
        
        # Plot 4: IC Component Convergence
        print("\n[4/5] Plotting IC component convergence...")
        self.plot_ic_component_convergence()
        print("      [OK] IC component convergence saved")
        
        # Plot 5: IC Metrics Summary
        print("\n[5/5] Computing IC metrics summary...")
        correlations, rms_errors = self.compute_ic_correlation_metrics(model, true_ic_data)
        self.plot_ic_metrics_summary(correlations, rms_errors)
        print("      [OK] IC metrics summary saved")
        
        print("\n" + "="*70)
        print(f"  All diagnostics saved to: {self.save_dir}")
        print("="*70)
        print("\nDiagnostic Summary:")
        print("  1. training_diagnostics.png - Training convergence analysis")
        print("  2. ic_spatial_comparison.png - Predicted vs true IC fields")
        print("  3. ic_power_spectrum.png - Spectrum comparison")
        print("  4. ic_component_convergence.png - IC component convergence")
        print("  5. ic_metrics_summary.png - Correlation and RMS error metrics")
        print("="*70 + "\n")
        
        # Print numerical summary
        print("\nIC Fitting Quality Summary:")
        print("-" * 50)
        print(f"{'Component':<15} {'Correlation':<15} {'RMS Error':<15}")
        print("-" * 50)
        comps = ['rho', 'vx', 'vy', 'vz'] if self.dimension == 3 else ['rho', 'vx', 'vy']
        for comp in comps:
            if comp in correlations:
                print(f"{comp:<15} {correlations[comp]:>14.4f} {rms_errors[comp]:>14.6f}")
        print("-" * 50)
        avg_corr = np.mean([correlations[c] for c in comps if c in correlations])
        print(f"{'Average':<15} {avg_corr:>14.4f}")
        print("-" * 50 + "\n")
    
    def compute_field_spectrum_comparison(self, model, true_ic_data, n_grid=128):
        """
        For sinusoidal cases: Compute power spectra of predicted vs true IC fields.
        Similar to power spectrum comparison but computed from actual fields.
        """
        device = next(model.parameters()).device
        
        print(f"  Computing field spectrum comparison on IC points...")
        
        # Use IC collocation points
        colloc_IC = true_ic_data['colloc_IC']
        
        # Get predictions
        with torch.no_grad():
            pred = model(colloc_IC)
            rho_pred = pred[:, 0].cpu().numpy()
            vx_pred = pred[:, 1].cpu().numpy()
            vy_pred = pred[:, 2].cpu().numpy()
            if self.dimension == 3:
                vz_pred = pred[:, 3].cpu().numpy()
        
        # Get true ICs
        rho_true = true_ic_data['rho'].detach().cpu().numpy()
        vx_true = true_ic_data['vx'].detach().cpu().numpy()
        vy_true = true_ic_data['vy'].detach().cpu().numpy()
        if self.dimension == 3:
            vz_true = true_ic_data['vz'].detach().cpu().numpy()
        
        # Need to interpolate to regular grid for FFT
        x_ic = colloc_IC[0].detach().cpu().numpy().flatten()
        y_ic = colloc_IC[1].detach().cpu().numpy().flatten()
        
        if self.dimension == 3:
            z_ic = colloc_IC[2].detach().cpu().numpy().flatten()
            # Use points near z=0.5 for 2D slice
            z_tolerance = 0.1
            mask_z = np.abs(z_ic - 0.5) < z_tolerance
            if mask_z.sum() < 100:
                mask_z = np.ones(len(z_ic), dtype=bool)
        else:
            mask_z = np.ones(len(x_ic), dtype=bool)
        
        # Create regular grid for FFT
        x_grid = np.linspace(0, 1, n_grid)
        y_grid = np.linspace(0, 1, n_grid)
        X_grid, Y_grid = np.meshgrid(x_grid, y_grid, indexing='ij')
        
        # Interpolate from scattered IC points to regular grid
        from scipy.interpolate import griddata
        points_ic = np.column_stack([x_ic[mask_z], y_ic[mask_z]])
        points_grid = np.column_stack([X_grid.flatten(), Y_grid.flatten()])
        
        # Interpolate density
        rho_pred_grid = griddata(points_ic, rho_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        rho_true_grid = griddata(points_ic, rho_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        
        # Interpolate velocities
        vx_pred_grid = griddata(points_ic, vx_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        vy_pred_grid = griddata(points_ic, vy_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        
        vx_true_grid = griddata(points_ic, vx_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        vy_true_grid = griddata(points_ic, vy_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        
        if self.dimension == 3:
            vz_pred_grid = griddata(points_ic, vz_pred[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
            vz_true_grid = griddata(points_ic, vz_true[mask_z], points_grid, method='linear', fill_value=0).reshape(n_grid, n_grid)
        
        # Compute power spectra
        def compute_power_spectrum(field):
            fft = np.fft.fft2(field)
            power = np.abs(np.fft.fftshift(fft))**2
            
            kx = np.fft.fftfreq(n_grid, d=1.0/n_grid)
            ky = np.fft.fftfreq(n_grid, d=1.0/n_grid)
            kx_shift = np.fft.fftshift(kx)
            ky_shift = np.fft.fftshift(ky)
            KX, KY = np.meshgrid(kx_shift, ky_shift, indexing='ij')
            K = np.sqrt(KX**2 + KY**2)
            
            return power, K
        
        power_rho_pred, K = compute_power_spectrum(rho_pred_grid)
        power_rho_true, _ = compute_power_spectrum(rho_true_grid)
        
        power_vx_pred, _ = compute_power_spectrum(vx_pred_grid)
        power_vx_true, _ = compute_power_spectrum(vx_true_grid)
        
        power_vy_pred, _ = compute_power_spectrum(vy_pred_grid)
        power_vy_true, _ = compute_power_spectrum(vy_true_grid)
        
        result = {
            'K': K,
            'power_rho_pred': power_rho_pred,
            'power_rho_true': power_rho_true,
            'power_vx_pred': power_vx_pred,
            'power_vx_true': power_vx_true,
            'power_vy_pred': power_vy_pred,
            'power_vy_true': power_vy_true,
        }
        
        if self.dimension == 3:
            power_vz_pred, _ = compute_power_spectrum(vz_pred_grid)
            power_vz_true, _ = compute_power_spectrum(vz_true_grid)
            result['power_vz_pred'] = power_vz_pred
            result['power_vz_true'] = power_vz_true
        
        return result
    
    def plot_field_spectrum_comparison(self, ps_data):
        """
        Plot field spectrum comparison for sinusoidal cases.
        Shows density and velocity spectra.
        """
        if self.dimension == 3:
            fig = plt.figure(figsize=(18, 5))
            gs = GridSpec(1, 4, figure=fig)
            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity'), ('vz', 'Z-Velocity')]
        else:
            fig = plt.figure(figsize=(14, 5))
            gs = GridSpec(1, 3, figure=fig)
            components = [('rho', 'Density'), ('vx', 'X-Velocity'), ('vy', 'Y-Velocity')]
        
        for idx, (comp, label) in enumerate(components):
            ax = fig.add_subplot(gs[0, idx])
            
            # Radial binning
            K = ps_data['K']
            power_pred = ps_data[f'power_{comp}_pred']
            power_true = ps_data[f'power_{comp}_true']
            
            k_max = np.max(K)
            k_bins = np.linspace(0, k_max, 30)
            power_pred_avg = []
            power_true_avg = []
            k_centers = []
            
            for i in range(len(k_bins)-1):
                mask = (K >= k_bins[i]) & (K < k_bins[i+1])
                if mask.any():
                    power_pred_avg.append(np.mean(power_pred[mask]))
                    power_true_avg.append(np.mean(power_true[mask]))
                    k_centers.append((k_bins[i] + k_bins[i+1]) / 2)
            
            if len(k_centers) > 0:
                ax.loglog(k_centers, power_true_avg, 'r-', linewidth=2.5, label='True IC', marker='o', markersize=4)
                ax.loglog(k_centers, power_pred_avg, 'b--', linewidth=2.5, label='Predicted', marker='s', markersize=4)
                
                ax.set_xlabel('Wavenumber k', fontsize=11)
                ax.set_ylabel('Power', fontsize=11)
                ax.set_title(f'{label} Spectrum', fontsize=12, fontweight='bold')
                ax.legend(fontsize=10)
                ax.grid(True, alpha=0.3, which='both')
        
        plt.suptitle('IC Field Spectrum: Predicted vs True', fontsize=14, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/ic_power_spectrum.png', dpi=150, bbox_inches='tight')
        plt.close()
_register_module('training.training_diagnostics', ['TrainingDiagnostics'])

# ==== Module: training.physics (training/physics.py) ====
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from core.losses import ASTPN, pde_residue
from training.training_diagnostics import TrainingDiagnostics
from core.data_generator import diff
from core.model_architecture import PINN
from core.initial_conditions import (initialize_shared_velocity_fields, generate_power_spectrum_field, 
                                    generate_power_spectrum_field_vy, fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0)
from config import cs, const, G, rho_o, PERTURBATION_TYPE, KX, KY, BATCH_SIZE, NUM_BATCHES, RANDOM_SEED
from config import IC_WEIGHT, ENABLE_TRAINING_DIAGNOSTICS


# ==================== Physics Calculations and Loss Functions ====================

def closure(model, net, mse_cost_function, collocation_domain, collocation_IC, optimizer, rho_1, lam, jeans, v_1, data_terms=None):

    ############## Loss based on initial conditions ###############
    rho_0 = fun_rho_0(rho_1, lam, collocation_IC)
    vx_0  = fun_vx_0(lam, jeans, v_1, collocation_IC)

    if model.dimension == 2:
        vy_0  = fun_vy_0(lam, jeans, v_1, collocation_IC)

    elif model.dimension == 3:
        vy_0  = fun_vy_0(lam, jeans, v_1, collocation_IC)
        vz_0  = fun_vz_0(lam, jeans, v_1, collocation_IC)
    
    net_ic_out = net(collocation_IC)

    rho_ic_out = net_ic_out[:,0:1]
    vx_ic_out  = net_ic_out[:,1:2]

    if model.dimension == 2:
        vy_ic_out  = net_ic_out[:,2:3]
    elif model.dimension == 3:
        vy_ic_out  = net_ic_out[:,2:3]
        vz_ic_out  = net_ic_out[:,3:4]

    # For sinusoidal: enforce only explicit sinusoidal ICs; skip continuity seeding
    is_sin = str(PERTURBATION_TYPE).lower() == "sinusoidal"
    if is_sin:
        x_ic_for_ic = collocation_IC[0]
        if len(collocation_IC) >= 2:  # 2D case
            y_ic_for_ic = collocation_IC[1]
            rho_ic_target = rho_o + rho_1 * torch.cos(KX * x_ic_for_ic + KY * y_ic_for_ic)
        else:  # 1D case
            rho_ic_target = rho_o + rho_1 * torch.cos(2*np.pi*x_ic_for_ic/lam)
        mse_rho_ic = mse_cost_function(rho_ic_out, rho_ic_target)
    else:
        mse_rho_ic = 0.0 * torch.mean(rho_ic_out*0)

    mse_vx_ic  =  mse_cost_function(vx_ic_out, vx_0)

    if model.dimension == 2:
        mse_vy_ic  =  mse_cost_function(vy_ic_out, vy_0)

    elif model.dimension == 3:
        mse_vy_ic  =  mse_cost_function(vy_ic_out, vy_0)
        mse_vz_ic  =  mse_cost_function(vz_ic_out, vz_0)

    ############## Loss based on PDE ###################################
    
    # Apply startup time offset to PDE collocation time only (IC remains at t=0)
    if isinstance(collocation_domain, (list, tuple)):
        colloc_shifted = list(collocation_domain)
    else:
        # Single tensor format: split into [x, y, t]
        colloc_shifted = [collocation_domain[:, i:i+1] for i in range(collocation_domain.shape[1])]

    if model.dimension == 1:
        # time is at index 1
        # Note: Domain collocation points now start from STARTUP_DT (set in data_generator.py)
        rho_r,vx_r,phi_r = pde_residue(colloc_shifted, net, dimension = 1)

    elif model.dimension == 2:
        # time is at index 2
        # Note: Domain collocation points now start from STARTUP_DT (set in data_generator.py)
        rho_r,vx_r,vy_r,phi_r = pde_residue(colloc_shifted, net, dimension = 2)

    elif model.dimension == 3:
        # time is at index 3
        # Note: Domain collocation points now start from STARTUP_DT (set in data_generator.py)
        rho_r,vx_r,vy_r,vz_r,phi_r = pde_residue(colloc_shifted, net, dimension = 3)
    

    mse_rho  = torch.mean(rho_r ** 2)
    mse_velx = torch.mean(vx_r  ** 2)

    if model.dimension == 2:
        mse_vely = torch.mean(vy_r  ** 2)

    elif model.dimension == 3:
        mse_vely = torch.mean(vy_r  ** 2)
        mse_velz = torch.mean(vz_r  ** 2)
    
    mse_phi  = torch.mean(phi_r ** 2)

    ic_weight = IC_WEIGHT

    ################### Combining the loss functions ####################
    if model.dimension == 1:
        base = ic_weight * mse_vx_ic + mse_rho + mse_velx + mse_phi
        loss = base + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)

    elif model.dimension == 2:
        base = ic_weight * (mse_vx_ic + mse_vy_ic) + mse_rho + mse_velx + mse_vely + mse_phi
        loss = base + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)

    elif model.dimension == 3:
        base = ic_weight * (mse_vx_ic + mse_vy_ic + mse_vz_ic) + mse_rho + mse_velx + mse_vely + mse_velz + mse_phi
        loss = base + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)

    
        #loss = mse_rho_ic + mse_vx_ic + mse_vy_ic + mse_vz_ic + \
        #rhox_b + rhoy_b + rhoz_b + vx_xb + vx_yb + vx_zb +  vy_xb + vy_yb + vy_zb + vz_xb + vz_yb + vz_zb + \
        #phi_xb + phi_xx_b + phi_yb + phi_yy_b +  phi_zb + phi_zz_b + mse_rho + mse_velx +  mse_vely + mse_velz + mse_phi 

    optimizer.zero_grad()
    loss.backward(retain_graph=True)
    
    # Create loss breakdown dictionary
    loss_breakdown = {}
    
    # IC losses (grouped together)
    ic_loss = mse_vx_ic.item()
    if isinstance(mse_rho_ic, torch.Tensor) and mse_rho_ic.item() > 0:
        ic_loss += mse_rho_ic.item()
    
    if model.dimension == 2:
        ic_loss += mse_vy_ic.item()
    elif model.dimension == 3:
        ic_loss += mse_vy_ic.item()
        ic_loss += mse_vz_ic.item()
    
    loss_breakdown['IC'] = ic_loss
    
    # PDE losses (grouped together)
    pde_loss = mse_rho.item() + mse_velx.item()
    
    if model.dimension == 2:
        pde_loss += mse_vely.item()
    elif model.dimension == 3:
        pde_loss += mse_vely.item()
        pde_loss += mse_velz.item()
    
    pde_loss += mse_phi.item()
    loss_breakdown['PDE'] = pde_loss

    data_loss_tensor, data_breakdown = _evaluate_data_terms(net, mse_cost_function, data_terms if data_terms else [])
    if data_loss_tensor is not None:
        loss = loss + data_loss_tensor
        for label, value in data_breakdown.items():
            loss_breakdown[label] = value

    return loss, loss_breakdown

def _random_batch_indices(total_count, batch_size, device):
    actual = int(min(batch_size, total_count))
    return torch.randperm(total_count, device=device)[:actual]


def _make_batch_tensors(tensors_list, indices):
    """Create batch tensors by indexing. No cloning for speed."""
    return [t[indices] for t in tensors_list]


def _sample_data_term(dataset, batch_size):
    if dataset is None or dataset.get('x') is None or dataset.get('t') is None:
        return None
    count = int(dataset.get('count', dataset['x'].shape[0]))
    if count <= 0:
        return None
    device = dataset['x'].device
    bs = batch_size if batch_size is not None else count
    bs = max(1, min(int(bs), count))
    indices = _random_batch_indices(count, bs, device)

    def _slice(key):
        tensor = dataset.get(key)
        if tensor is None:
            return None
        return tensor[indices]

    return {
        'x': _slice('x'),
        'y': _slice('y'),
        'z': _slice('z'),
        't': _slice('t'),
        'rho': _slice('rho'),
        'vx': _slice('vx'),
        'vy': _slice('vy'),
        'phi': _slice('phi'),
    }


def _compute_data_loss_unweighted(net, batch, mse_cost_function):
    if batch is None or batch['x'] is None or batch['t'] is None:
        return None

    inputs = [batch['x']]
    if batch.get('y') is not None:
        inputs.append(batch['y'])
    if batch.get('z') is not None:
        inputs.append(batch['z'])
    inputs.append(batch['t'])

    outputs = net(inputs)
    num_outputs = outputs.shape[1]

    component_specs = [
        ('rho', 0),
        ('vx', 1),
        ('vy', 2),
        ('phi', 3),
    ]

    losses = []
    for key, idx in component_specs:
        target = batch.get(key)
        if target is None or idx >= num_outputs:
            continue
        pred = outputs[:, idx:idx+1]
        losses.append(mse_cost_function(pred, target))

    if not losses:
        return None
    return sum(losses) / len(losses)


def _evaluate_data_terms(net, mse_cost_function, data_terms):
    if not data_terms:
        return None, {}

    total_loss = None
    breakdown = {}

    for term in data_terms:
        dataset = term.get('dataset')
        weight = float(term.get('weight', 0.0) or 0.0)
        if dataset is None or weight <= 0:
            continue
        batch = _sample_data_term(dataset, term.get('batch_size'))
        data_loss = _compute_data_loss_unweighted(net, batch, mse_cost_function)
        if data_loss is None:
            continue
        weighted_loss = weight * data_loss
        total_loss = weighted_loss if total_loss is None else total_loss + weighted_loss
        label = term.get('label', 'DATA')
        breakdown[label] = breakdown.get(label, 0.0) + float(weighted_loss.detach().cpu())

    return total_loss, breakdown


def closure_batched(model, net, mse_cost_function, collocation_domain, collocation_IC, optimizer,
                    rho_1, lam, jeans, v_1, batch_size, num_batches, update_tracker=True, iteration=0, use_fft_poisson=None, data_terms=None, collocation_poisson_ic=None):
    """
    Batched closure function for training.
    
    This function computes losses over mini-batches.
    """
    # Aggregate losses across mini-batches
    total_loss = 0.0
    num_effective_batches = 0

    # Determine counts and devices
    # Handle both formats: list of tensors [x, y, t] or single tensor [N, 3]
    if isinstance(collocation_domain, (list, tuple)):
        dom_n = collocation_domain[0].size(0)
        device = collocation_domain[0].device
    else:
        dom_n = collocation_domain.size(0)
        device = collocation_domain.device
    
    ic_n = collocation_IC[0].size(0)

    last_data_breakdown = {}

    for _ in range(int(max(1, num_batches))):
        dom_idx = _random_batch_indices(dom_n, batch_size, device)
        ic_idx = _random_batch_indices(ic_n, batch_size, device)

        # Handle both formats for collocation_domain
        if isinstance(collocation_domain, (list, tuple)):
            batch_dom = _make_batch_tensors(collocation_domain, dom_idx)
        else:
            # Single tensor format: split into [x, y, t]
            batch_dom = [collocation_domain[dom_idx, i:i+1] for i in range(collocation_domain.shape[1])]
        
        batch_ic = _make_batch_tensors(collocation_IC, ic_idx)

        # IC loss terms
        rho_0 = fun_rho_0(rho_1, lam, batch_ic)
        vx_0  = fun_vx_0(lam, jeans, v_1, batch_ic)

        net_ic_out = net(batch_ic)
        rho_ic_out = net_ic_out[:,0:1]
        vx_ic_out  = net_ic_out[:,1:2]

        if model.dimension == 2:
            vy_0 = fun_vy_0(lam, jeans, v_1, batch_ic)
            vy_ic_out = net_ic_out[:,2:3]
        elif model.dimension == 3:
            vy_0 = fun_vy_0(lam, jeans, v_1, batch_ic)
            vz_0 = fun_vz_0(lam, jeans, v_1, batch_ic)
            vy_ic_out = net_ic_out[:,2:3]
            vz_ic_out = net_ic_out[:,3:4]

        is_sin = str(PERTURBATION_TYPE).lower() == "sinusoidal"
        if is_sin:
            x_ic_for_ic = batch_ic[0]
            if len(batch_ic) >= 2:
                y_ic_for_ic = batch_ic[1]
                rho_ic_target = rho_o + rho_1 * torch.cos(KX * x_ic_for_ic + KY * y_ic_for_ic)
            else:
                rho_ic_target = rho_o + rho_1 * torch.cos(2*np.pi*x_ic_for_ic/lam)
            mse_rho_ic = mse_cost_function(rho_ic_out, rho_ic_target)
        else:
            mse_rho_ic = 0.0 * torch.mean(rho_ic_out*0)

        mse_vx_ic  = mse_cost_function(vx_ic_out, vx_0)
        if model.dimension == 2:
            mse_vy_ic = mse_cost_function(vy_ic_out, vy_0)
        elif model.dimension == 3:
            mse_vy_ic = mse_cost_function(vy_ic_out, vy_0)
            mse_vz_ic = mse_cost_function(vz_ic_out, vz_0)

        # PDE residuals on batched domain with startup shift
        if isinstance(batch_dom, (list, tuple)):
            colloc_shifted = list(batch_dom)
        else:
            # Single tensor format: split into [x, y, t]
            colloc_shifted = [batch_dom[:, i:i+1] for i in range(batch_dom.shape[1])]
        if model.dimension == 1:
            # Note: Domain collocation points already start from STARTUP_DT, no need to shift further
            rho_r, vx_r, phi_r = pde_residue(colloc_shifted, net, dimension=1)
        elif model.dimension == 2:
            # Note: Domain collocation points already start from STARTUP_DT, no need to shift further
            rho_r, vx_r, vy_r, phi_r = pde_residue(colloc_shifted, net, dimension=2)
        else:
            # Note: Domain collocation points already start from STARTUP_DT, no need to shift further
            rho_r, vx_r, vy_r, vz_r, phi_r = pde_residue(colloc_shifted, net, dimension=3)

        # Extract time values from batch_dom
        if model.dimension == 1:
            t_dom = batch_dom[1]
        elif model.dimension == 2:
            t_dom = batch_dom[2]
        elif model.dimension == 3:
            t_dom = batch_dom[3]

        # Standard uniform weighting for PDE residuals
        mse_rho  = torch.mean(rho_r ** 2)
        mse_velx = torch.mean(vx_r  ** 2)
        if model.dimension == 2:
            mse_vely = torch.mean(vy_r  ** 2)
        elif model.dimension == 3:
            mse_vely = torch.mean(vy_r  ** 2)
            mse_velz = torch.mean(vz_r  ** 2)
        mse_phi  = torch.mean(phi_r ** 2)

        if model.dimension == 1:
            base = mse_vx_ic + mse_rho + mse_velx + mse_phi
            loss = base + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)
        elif model.dimension == 2:
            base = mse_vx_ic + mse_vy_ic + mse_rho + mse_velx + mse_vely + mse_phi
            loss = base + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)
        else:
            base = mse_vx_ic + mse_vy_ic + mse_vz_ic + mse_rho + mse_velx + mse_vely + mse_velz + mse_phi
            loss = base + (mse_rho_ic if isinstance(mse_rho_ic, torch.Tensor) else 0.0)

        if data_terms:
            data_loss_batch, batch_breakdown = _evaluate_data_terms(net, mse_cost_function, data_terms)
            if data_loss_batch is not None:
                loss = loss + data_loss_batch
                last_data_breakdown = batch_breakdown

        total_loss = total_loss + loss
        num_effective_batches += 1
    
    # Add Poisson IC loss (Option 3: Pure ML approach for initial phi)
    # This enforces Poisson equation at t=0 with extra collocation points
    mse_poisson_ic = 0.0
    mse_phi_mean = 0.0
    if collocation_poisson_ic is not None:
        from core.losses import poisson_residue_only
        from config import POISSON_IC_WEIGHT, PHI_MEAN_CONSTRAINT_WEIGHT
        
        # Only compute and add Poisson IC loss if weight is non-zero
        if POISSON_IC_WEIGHT > 0:
            # Enforce Poisson equation: ∇²φ = const*(ρ-ρ₀)
            phi_r_ic = poisson_residue_only(collocation_poisson_ic, net, dimension=model.dimension)
            mse_poisson_ic = torch.mean(phi_r_ic ** 2)
            total_loss = total_loss + POISSON_IC_WEIGHT * mse_poisson_ic
        
        # Only compute and add phi mean constraint if weight is non-zero
        if PHI_MEAN_CONSTRAINT_WEIGHT > 0:
            # Enforce mean(φ) = 0 at t=0 to fix gauge freedom (Option A)
            # This removes the arbitrary constant offset in φ
            net_output_ic = net(collocation_poisson_ic)
            phi_ic = net_output_ic[:, -1]  # Last output is phi
            mean_phi = torch.mean(phi_ic)
            mse_phi_mean = mean_phi ** 2
            total_loss = total_loss + PHI_MEAN_CONSTRAINT_WEIGHT * mse_phi_mean

    optimizer.zero_grad()
    avg_loss = total_loss / max(1, num_effective_batches)
    avg_loss.backward(retain_graph=True)
    
    # Aggressive memory cleanup after backward pass
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # Create loss breakdown dictionary (averaged across batches)
    loss_breakdown = {}
    
    # For batched version, we need to compute breakdown from the last batch
    # This is an approximation since we can't easily track individual terms across batches
    # IC losses (grouped together)
    ic_loss = mse_vx_ic.item()
    if isinstance(mse_rho_ic, torch.Tensor) and mse_rho_ic.item() > 0:
        ic_loss += mse_rho_ic.item()
    
    if model.dimension == 2:
        ic_loss += mse_vy_ic.item()
    elif model.dimension == 3:
        ic_loss += mse_vy_ic.item()
        ic_loss += mse_vz_ic.item()
    
    loss_breakdown['IC'] = ic_loss
    
    # PDE losses (grouped together)
    pde_loss = mse_rho.item() + mse_velx.item()
    
    if model.dimension == 2:
        pde_loss += mse_vely.item()
    elif model.dimension == 3:
        pde_loss += mse_vely.item()
        pde_loss += mse_velz.item()
    
    pde_loss += mse_phi.item()
    loss_breakdown['PDE'] = pde_loss
    
    # Add Poisson IC loss to breakdown if it was computed
    if isinstance(mse_poisson_ic, torch.Tensor):
        loss_breakdown['Poisson_IC'] = mse_poisson_ic.item()
    if isinstance(mse_phi_mean, torch.Tensor):
        loss_breakdown['Phi_Mean'] = mse_phi_mean.item()
    
    for label, value in last_data_breakdown.items():
        loss_breakdown[label] = value

    return avg_loss, loss_breakdown
_register_module('training.physics', ['_compute_data_loss_unweighted', '_evaluate_data_terms', '_make_batch_tensors', '_random_batch_indices', '_sample_data_term', 'closure', 'closure_batched'])

# ==== Module: training.trainer (training/trainer.py) ====
"""
Training loop management for PINN and XPINN.

This module handles the optimization loops (Adam and L-BFGS) for both
single PINN and XPINN decomposition training.
"""
import numpy as np
import torch
from training.training_diagnostics import TrainingDiagnostics
from config import BATCH_SIZE, NUM_BATCHES, ENABLE_TRAINING_DIAGNOSTICS
from training.physics import closure_batched


def _compute_ic_component_losses(net, collocation_IC, rho_1, lam, jeans, v_1, dimension, mse_cost_function):
    """
    Compute individual IC component losses for 3D power spectrum diagnostics.
    
    Returns:
        Dict with component-wise losses: {'rho', 'vx', 'vy', 'vz', 'phi'}
    """
    from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0
    from config import rho_o
    
    with torch.no_grad():
        # Get true ICs
        rho_0 = fun_rho_0(rho_1, lam, collocation_IC)
        vx_0 = fun_vx_0(lam, jeans, v_1, collocation_IC)
        vy_0 = fun_vy_0(lam, jeans, v_1, collocation_IC)
        vz_0 = fun_vz_0(lam, jeans, v_1, collocation_IC)
        
        # Get predictions
        net_ic_out = net(collocation_IC)
        rho_ic_out = net_ic_out[:, 0:1]
        vx_ic_out = net_ic_out[:, 1:2]
        vy_ic_out = net_ic_out[:, 2:3]
        vz_ic_out = net_ic_out[:, 3:4]
        phi_ic_out = net_ic_out[:, 4:5]
        
        # Compute component losses
        ic_losses = {
            'rho': mse_cost_function(rho_ic_out, rho_0).item(),
            'vx': mse_cost_function(vx_ic_out, vx_0).item(),
            'vy': mse_cost_function(vy_ic_out, vy_0).item(),
            'vz': mse_cost_function(vz_ic_out, vz_0).item(),
            'phi': torch.mean(phi_ic_out ** 2).item(),  # phi should be close to 0 ideally
        }
    
    return ic_losses


def train(model, net, collocation_domain, collocation_IC, optimizer, optimizerL, iteration_adam, iterationL, mse_cost_function, closure, rho_1, lam, jeans, v_1, device, data_terms=None, collocation_poisson_ic=None):
    """
    Standard training loop for single PINN.
    
    Manages Adam and L-BFGS optimization phases with cosine scheduling for
    continuity weight and startup_dt parameters.
    
    Args:
        model: Collocation model
        net: Neural network
        collocation_domain: Domain collocation points
        collocation_IC: Initial condition points
        optimizer: Adam optimizer
        optimizerL: L-BFGS optimizer
        iteration_adam: Number of Adam iterations
        iterationL: Number of L-BFGS iterations
        mse_cost_function: MSE loss function
        closure: Closure function (unused, kept for compatibility)
        rho_1: Perturbation amplitude
        lam: Wavelength
        jeans: Jeans length
        v_1: Velocity amplitude
        device: PyTorch device
        data_terms: Optional list of additional supervised datasets with weights
        collocation_poisson_ic: Extra collocation points at t=0 for Poisson enforcement
    """
    bs = int(BATCH_SIZE)
    nb = int(NUM_BATCHES)

    # Import config values for diagnostics
    from config import PERTURBATION_TYPE, tmax
    
    # Create diagnostics with correct dimension and perturbation type
    diagnostics = TrainingDiagnostics(
        save_dir='./diagnostics/',
        dimension=model.dimension,
        perturbation_type=PERTURBATION_TYPE
    ) if ENABLE_TRAINING_DIAGNOSTICS else None

    for i in range(iteration_adam):
        optimizer.zero_grad()

        loss, loss_breakdown = optimizer.step(lambda: closure_batched(model, net, mse_cost_function, collocation_domain, collocation_IC, optimizer, rho_1, lam, jeans, v_1, bs, nb, update_tracker=True, iteration=i, use_fft_poisson=True, data_terms=data_terms, collocation_poisson_ic=collocation_poisson_ic))

        with torch.autograd.no_grad():
            # Diagnostics logging every 50 iterations
            if i % 50 == 0 and diagnostics is not None:
                try:
                    # Compute IC component losses for 3D power spectrum tracking
                    ic_component_losses = None
                    if diagnostics.is_3d_power_spectrum:
                        ic_component_losses = _compute_ic_component_losses(
                            net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function
                        )
                    
                    diagnostics.log_iteration(
                        iteration=i,
                        model=net,
                        loss_dict={
                            'total': loss.item(),
                            'pde': float(loss_breakdown.get('PDE', 0.0)),
                            'ic': float(loss_breakdown.get('IC', 0.0))
                        },
                        geomtime_col=collocation_domain,
                        ic_component_losses=ic_component_losses
                    )
                except Exception as _diag_err:
                    # Keep training robust if diagnostics fail
                    print(f"[WARN] Diagnostics logging failed at {i}: {_diag_err}")

            if i % 100 == 0:
                print(f"Training Loss at {i} for Adam (batched) in {model.dimension}D system = {loss.item():.2e}", flush=True)
                # Print loss breakdown
                breakdown_str = " | ".join([f"{k}: {v:.2e}" for k, v in loss_breakdown.items() if v > 0])
                if breakdown_str:
                    print(f"  Loss breakdown: {breakdown_str}", flush=True)

    for i in range(iterationL):
        optimizer.zero_grad()
        global_step = iteration_adam + i

        # L-BFGS expects a closure that returns only scalar loss
        # Store loss_breakdown in a list so we can access it after the step
        loss_breakdown_holder = [None]
        
        def lbfgs_closure():
            loss, loss_breakdown = closure_batched(model, net, mse_cost_function, collocation_domain, collocation_IC, optimizerL, rho_1, lam, jeans, v_1, bs, nb, update_tracker=False, iteration=global_step, use_fft_poisson=False, data_terms=data_terms, collocation_poisson_ic=collocation_poisson_ic)
            loss_breakdown_holder[0] = loss_breakdown
            return loss
        
        loss = optimizerL.step(lbfgs_closure)
        loss_breakdown = loss_breakdown_holder[0]

        with torch.autograd.no_grad():
            # Diagnostics logging every 50 iterations in LBFGS too
            if i % 50 == 0 and loss_breakdown is not None and diagnostics is not None:
                try:
                    # Compute IC component losses for 3D power spectrum tracking
                    ic_component_losses = None
                    if diagnostics.is_3d_power_spectrum:
                        ic_component_losses = _compute_ic_component_losses(
                            net, collocation_IC, rho_1, lam, jeans, v_1, model.dimension, mse_cost_function
                        )
                    
                    diagnostics.log_iteration(
                        iteration=iteration_adam + i,
                        model=net,
                        loss_dict={
                            'total': loss.item() if hasattr(loss, 'item') else float(loss),
                            'pde': float(loss_breakdown.get('PDE', 0.0)),
                            'ic': float(loss_breakdown.get('IC', 0.0))
                        },
                        geomtime_col=collocation_domain,
                        ic_component_losses=ic_component_losses
                    )
                except Exception as _diag_err:
                    print(f"[WARN] Diagnostics logging (LBFGS) failed at {i}: {_diag_err}")
            if i % 50 == 0:
                print(f"Training Loss at {i} for LBGFS (batched) in {model.dimension}D system = {loss.item():.2e}", flush=True)
                # Print loss breakdown
                breakdown_str = " | ".join([f"{k}: {v:.2e}" for k, v in loss_breakdown.items() if v > 0])
                if breakdown_str:
                    print(f"  Loss breakdown: {breakdown_str}", flush=True)
    
    # Generate diagnostic plots at the end of training
    if diagnostics is not None:
        try:
            final_iteration = iteration_adam + iterationL - 1
            
            # Generate unified diagnostics (5 plots for all cases)
            print("\n[INFO] Generating unified training diagnostics...")
            from core.initial_conditions import fun_rho_0, fun_vx_0, fun_vy_0, fun_vz_0
            
            # Evaluate true ICs on the IC collocation points for comparison
            with torch.no_grad():
                true_ic_data = {
                    'colloc_IC': collocation_IC,
                    'rho': fun_rho_0(rho_1, lam, collocation_IC),
                    'vx': fun_vx_0(lam, jeans, v_1, collocation_IC),
                    'vy': fun_vy_0(lam, jeans, v_1, collocation_IC),
                    'vz': fun_vz_0(lam, jeans, v_1, collocation_IC) if model.dimension == 3 else None,
                }
            
            diagnostics.run_unified_diagnostics(
                model=net, 
                true_ic_data=true_ic_data,
                final_iteration=final_iteration
            )
            
            print(f"\n[INFO] All diagnostics saved to ./diagnostics/")
        except Exception as _diag_err:
            import traceback
            print(f"[WARN] Final diagnostics plotting failed: {_diag_err}")
            traceback.print_exc()
    
    # Return diagnostics object for potential reuse in post-training analysis
    return diagnostics
_register_module('training.trainer', ['_compute_ic_component_losses', 'train'])

# ==== Module: visualization.Plotting (visualization/Plotting.py) ====
from scipy import signal
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from torch.autograd import Variable
import torch
import scipy
import os
import time
from numerical_solvers.LAX import lax_solution, lax_solution_3d_sinusoidal, lax_solver
from numerical_solvers.LAX import lax_solution1D_sinusoidal as lax_solution1D_sin
from numerical_solvers.LAX_torch import lax_solution_torch, lax_solution_3d_sinusoidal_torch, lax_solver_torch
def _clear_cuda_cache():
    """
    Comprehensive GPU cache clearing before FD solver runs.
    
    This function does more than just empty_cache() - it also:
    1. Forces garbage collection
    2. Synchronizes CUDA operations
    3. Clears the cache allocator
    
    This helps prevent OOM errors when FD solver needs GPU memory.
    """
    if torch.cuda.is_available():
        import gc
        # Force garbage collection first to free Python objects
        gc.collect()
        # Clear PyTorch's cache allocator
        torch.cuda.empty_cache()
        # Synchronize to ensure all operations complete
        torch.cuda.synchronize()

def find_max_contrast_slice(rho_volume, z_coords):
    """
    Find z-slice with maximum density contrast (range).
    
    Best for showing interesting physics - regions with both 
    high density (collapse) and low density (rarefaction).
    
    Args:
        rho_volume: (Nx, Ny, Nz) 3D density field
        z_coords: (Nz,) z-coordinate array
    
    Returns:
        z_idx: Index of optimal z-slice
        z_val: Value of z at that slice
        contrast: Density contrast at that slice
    """
    contrast_per_z = []
    for z_idx in range(rho_volume.shape[2]):
        # np.ptp = peak-to-peak (max - min)
        contrast = np.ptp(rho_volume[:, :, z_idx])
        contrast_per_z.append(contrast)
    
    best_z_idx = np.argmax(contrast_per_z)
    best_z_val = z_coords[best_z_idx]
    max_contrast = contrast_per_z[best_z_idx]
    
    return best_z_idx, best_z_val, max_contrast


def find_max_density_slice(rho_volume, z_coords):
    """
    Find z-slice with maximum integrated density.
    
    Good for showing where collapse is strongest.
    
    Args:
        rho_volume: (Nx, Ny, Nz) 3D density field
        z_coords: (Nz,) z-coordinate array
    
    Returns:
        z_idx: Index of optimal z-slice
        z_val: Value of z at that slice
        total_density: Integrated density at that slice
    """
    density_per_z = np.sum(rho_volume, axis=(0, 1))
    
    best_z_idx = np.argmax(density_per_z)
    best_z_val = z_coords[best_z_idx]
    max_density = density_per_z[best_z_idx]
    
    return best_z_idx, best_z_val, max_density


def analyze_z_variation(rho_volume, z_coords, t, save_dir=None):
    """
    Diagnostic tool: visualize how density varies across z.
    
    Call this once to understand your 3D data before choosing
    which selection method to use.
    
    Args:
        rho_volume: (Nx, Ny, Nz) 3D density field
        z_coords: (Nz,) z-coordinate array
        t: Current time (for labeling)
        save_dir: Optional directory to save figure
    
    Returns:
        fig: matplotlib figure
    """
    # Compute metrics for each z-slice
    density_per_z = np.sum(rho_volume, axis=(0, 1))
    contrast_per_z = [np.ptp(rho_volume[:, :, i]) for i in range(len(z_coords))]
    
    # Find optimal slices
    z_max_dens = z_coords[np.argmax(density_per_z)]
    z_max_cont = z_coords[np.argmax(contrast_per_z)]
    z_median = z_coords[np.argmin(np.abs(density_per_z - np.median(density_per_z)))]
    
    # Create diagnostic plot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Left: Integrated density vs z
    ax1.plot(z_coords, density_per_z, 'o-', linewidth=2, markersize=4)
    ax1.axvline(z_max_dens, color='red', linestyle='--', alpha=0.7, 
                label=f'Max density: z={z_max_dens:.3f}')
    ax1.axvline(z_median, color='green', linestyle='--', alpha=0.7,
                label=f'Median: z={z_median:.3f}')
    ax1.set_xlabel('z', fontsize=12)
    ax1.set_ylabel('Integrated Density', fontsize=12)
    ax1.set_title(f'Density Distribution at t={t:.2f}', fontsize=13)
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # Right: Contrast vs z
    ax2.plot(z_coords, contrast_per_z, 's-', color='orange', linewidth=2, markersize=4)
    ax2.axvline(z_max_cont, color='red', linestyle='--', alpha=0.7,
                label=f'Max contrast: z={z_max_cont:.3f}')
    ax2.set_xlabel('z', fontsize=12)
    ax2.set_ylabel('Density Contrast (max - min)', fontsize=12)
    ax2.set_title(f'Structure Distribution at t={t:.2f}', fontsize=13)
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    
    # Print summary
    print(f"\\n{'='*60}")
    print(f"Z-Slice Analysis at t={t:.2f}")
    print(f"{'='*60}")
    print(f"Max density slice:   z = {z_max_dens:.3f} (density = {np.max(density_per_z):.2e})")
    print(f"Max contrast slice:  z = {z_max_cont:.3f} (contrast = {np.max(contrast_per_z):.2e})")
    print(f"Median density slice: z = {z_median:.3f}")
    print(f"Density range across z: [{np.min(density_per_z):.2e}, {np.max(density_per_z):.2e}]")
    print(f"Contrast range across z: [{np.min(contrast_per_z):.2e}, {np.max(contrast_per_z):.2e}]")
    print(f"{'='*60}\\n")
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        savepath = os.path.join(save_dir, f'z_variation_analysis_t{t:.2f}.png')
        plt.savefig(savepath, dpi=300, bbox_inches='tight')
        print(f"Saved z-variation analysis to {savepath}")
    
    return fig


def _timed_call(label, fn, *args, **kwargs):
    start = time.perf_counter()
    result = fn(*args, **kwargs)
    elapsed = time.perf_counter() - start
    print(f"[Timing] {label} took {elapsed:.2f}s")
    return result

from config import (SAVE_STATIC_SNAPSHOTS, SNAPSHOT_DIR, PERTURBATION_TYPE, cs, const, G, rho_o, 
                    TIMES_1D, a, KX, KY, KZ, FD_N_1D, FD_N_2D, FD_N_3D, POWER_EXPONENT, 
                    N_GRID, N_GRID_3D, DIMENSION, SLICE_Y, SLICE_Z)
from config import RANDOM_SEED, SHOW_LINEAR_THEORY

# Global variable to store shared velocity fields for plotting
_shared_vx_np = None
_shared_vy_np = None
_shared_vz_np = None

def _build_input_list(x_tensor, t_tensor, y_tensor=None, z_tensor=None):
    coords = [x_tensor]
    if DIMENSION >= 2:
        if y_tensor is None:
            y_tensor = torch.full_like(x_tensor, SLICE_Y)
        coords.append(y_tensor)
    if DIMENSION >= 3:
        if z_tensor is None:
            z_tensor = torch.full_like(x_tensor, SLICE_Z)
        coords.append(z_tensor)
    coords.append(t_tensor)
    return coords

def _split_outputs(outputs):
    rho = outputs[:, 0:1]
    vx = outputs[:, 1:2]
    vy = outputs[:, 2:3] if DIMENSION >= 2 else None
    if DIMENSION == 1:
        phi = outputs[:, 2:3]
        vz = None
    elif DIMENSION == 2:
        phi = outputs[:, 3:4]
        vz = None
    else:
        vz = outputs[:, 3:4]
        phi = outputs[:, 4:5]
    return rho, vx, vy, vz, phi

def set_shared_velocity_fields(vx_np, vy_np, vz_np=None):
    """
    Set shared velocity fields for consistent FD plotting.
    
    Args:
        vx_np: X-velocity field (2D or 3D numpy array)
        vy_np: Y-velocity field (2D or 3D numpy array)
        vz_np: Z-velocity field (3D numpy array, optional for 2D)
    """
    global _shared_vx_np, _shared_vy_np, _shared_vz_np
    _shared_vx_np = vx_np
    _shared_vy_np = vy_np
    _shared_vz_np = vz_np

def _call_unified_3d_solver(time, lam, num_of_waves, rho_1, nu=0.5, 
                            use_velocity_ps=None, ps_index=None, vel_rms=None, random_seed=None,
                            save_times=None):
    """
    Helper function to call unified 3D LAX solver with proper IC type.
    
    Args:
        time: Final simulation time
        lam: Wavelength
        num_of_waves: Number of waves in domain
        rho_1: Density perturbation amplitude
        nu: Courant number (default: 0.5)
        use_velocity_ps: Whether to use power spectrum IC (defaults to config)
        ps_index: Power spectrum index (defaults to POWER_EXPONENT)
        vel_rms: Velocity RMS amplitude (defaults to a*cs)
        random_seed: Random seed (defaults to RANDOM_SEED)
        save_times: Optional list of times to save snapshots (default: None)
    
    Returns:
        If save_times is None: SimulationResult object
        If save_times is provided: Dict mapping time -> SimulationResult
    """
    # Use config defaults if not specified
    if use_velocity_ps is None:
        use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
    if ps_index is None:
        ps_index = POWER_EXPONENT
    if vel_rms is None:
        vel_rms = a * cs
    if random_seed is None:
        random_seed = RANDOM_SEED
    
    # Set up domain parameters
    Lx = Ly = Lz = lam * num_of_waves
    Nx = Ny = Nz = FD_N_3D
    
    domain_params = {
        'Lx': Lx, 'Ly': Ly, 'Lz': Lz,
        'nx': Nx, 'ny': Ny, 'nz': Nz
    }
    
    # Set up physics parameters
    physics_params = {
        'c_s': cs,
        'rho_o': rho_o,
        'const': const,
        'G': G,
        'rho_1': rho_1,
        'lam': lam
    }
    
    # Set up options
    options = {
        'gravity': True,
        'nu': nu,
        'comparison': False,
        'isplot': False
    }
    
    # Set up IC parameters based on perturbation type
    if use_velocity_ps:
        ic_type = 'power_spectrum'
        ic_params = {
            'power_index': ps_index,
            'amplitude': vel_rms,
            'random_seed': random_seed,
            'vx0_shared': _shared_vx_np,
            'vy0_shared': _shared_vy_np,
            'vz0_shared': _shared_vz_np
        }
    else:
        ic_type = 'sinusoidal'
        ic_params = {
            'KX': KX,
            'KY': KY,
            'KZ': KZ
        }
    
    # Call unified solver (GPU or CPU)
    if torch.cuda.is_available():
        _clear_cuda_cache()
        result = _timed_call(
            "LAX 3D (unified torch)",
            lax_solver_torch,
            time=time, domain_params=domain_params,
            physics_params=physics_params,
            ic_type=ic_type, ic_params=ic_params,
            options=options, save_times=save_times
        )
    else:
        # CPU solver doesn't support save_times yet, so we'll need to handle it differently
        # For now, if save_times is provided, we'll use the torch version even if on CPU
        if save_times is not None:
            result = _timed_call(
                "LAX 3D (unified torch cpu)",
                lax_solver_torch,
                time=time, domain_params=domain_params,
                physics_params=physics_params,
                ic_type=ic_type, ic_params=ic_params,
                options=options, save_times=save_times
            )
        else:
            result = _timed_call(
                "LAX 3D (unified cpu)",
                lax_solver,
                time=time, domain_params=domain_params,
                physics_params=physics_params,
                ic_type=ic_type, ic_params=ic_params,
                options=options
            )
    
    return result

def get_fd_default_params():
    """
    Get default FD parameters that match PINN training configuration.
    This ensures consistency between PINN and FD initial conditions.
    
    Returns:
        dict with keys: use_velocity_ps, ps_index, vel_rms, random_seed
    """
    return {
        'use_velocity_ps': (str(PERTURBATION_TYPE).lower() == "power_spectrum"),
        'ps_index': POWER_EXPONENT,
        'vel_rms': a * cs,
        'random_seed': RANDOM_SEED
    }

has_gpu = torch.cuda.is_available()
has_mps = torch.backends.mps.is_built()
device = "mps" if torch.backends.mps.is_built() \
    else "cuda:0" if torch.cuda.is_available() else "cpu"


def predict_xpinn(nets, x, y, t, xmin, xmax, ymin, ymax):
    """
    Legacy compatibility wrapper - XPINN no longer supported.
    Now simply calls the single network (handles list with one network).
    
    Args:
        nets: Single network or list with single network
        x, y, t: Coordinate tensors [N, 1]
        xmin, xmax, ymin, ymax: Domain bounds (ignored)
    
    Returns:
        Predictions [N, 4] (rho, vx, vy, phi)
    """
    if isinstance(nets, list):
        if len(nets) != 1:
            raise ValueError("Multi-network XPINN is no longer supported")
        net = nets[0]
    else:
        net = nets
    return net([x, y, t])


def add_interface_lines(ax, xmin, xmax, ymin, ymax):
    """
    Legacy compatibility function - does nothing now that XPINN is removed.
    
    Args:
        ax: Matplotlib axis (unused)
        xmin, xmax, ymin, ymax: Domain bounds (unused)
    """
    # XPINN removed - this function is now a no-op for compatibility
    pass


def plot_function(net, time_array, initial_params, velocity=False, isplot=False, animation=False):
    """
    Plot function for 1D slices through 2D domain
    
    Args:
        net: Trained neural network OR list of networks (for XPINN)
        time_array: Array of times to plot
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        velocity: Whether to plot velocity
        isplot: Whether to save plots
        animation: Whether this is for animation
    """
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax = initial_params
    
    # Handle both single network and list of networks
    if isinstance(net, list):
        nets = net
        use_xpinn = len(nets) > 1
    else:
        nets = [net]
        use_xpinn = False  
    # rho_o imported from config.py
    num_of_waves_x = (xmax-xmin)/lam
    num_of_waves_y = (ymax-ymin)/lam
    if animation:
        ## Converting the float (time-input) to an numpy array for animation
        ## Ignore this when the function is called in isolation
        time_array = np.array([time_array])
        # print("time",np.asarray(time_array))
    
    rho_max_Pinns = []    
    peak_lst=[]
    pert_xscale=[]
    for t in time_array:
        print("Plotting at t=", t)
        
        # Create 1D slice through the domain (like in notebook: Y = 0.6)
        X = np.linspace(xmin, xmax, 1000).reshape(1000, 1)
        Y = SLICE_Y * np.ones(1000).reshape(1000, 1)  # Fixed Y slice
        if DIMENSION >= 3:
            Z = SLICE_Z * np.ones(1000).reshape(1000, 1)
        t_ = t * np.ones(1000).reshape(1000, 1)
        
        pt_x_collocation = Variable(torch.from_numpy(X).float(), requires_grad=True).to(device)
        pt_y_collocation = Variable(torch.from_numpy(Y).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
        pt_z_collocation = Variable(torch.from_numpy(Z).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
        pt_t_collocation = Variable(torch.from_numpy(t_).float(), requires_grad=True).to(device)
        
        # Evaluate network(s)
        if use_xpinn:
            if DIMENSION >= 3:
                raise NotImplementedError("XPINN visualizations currently support up to 2D.")
            output_0 = predict_xpinn(nets, pt_x_collocation, pt_y_collocation, pt_t_collocation, xmin, xmax, ymin, ymax)
        else:
            # Ensure inputs are on the same device as the model
            net_device = next(nets[0].parameters()).device
            pt_x_collocation = pt_x_collocation.to(net_device)
            pt_t_collocation = pt_t_collocation.to(net_device)
            if pt_y_collocation is not None:
                pt_y_collocation = pt_y_collocation.to(net_device)
            if pt_z_collocation is not None:
                pt_z_collocation = pt_z_collocation.to(net_device)
            inputs = _build_input_list(
                pt_x_collocation,
                pt_t_collocation,
                pt_y_collocation,
                pt_z_collocation
            )
            output_0 = nets[0](inputs)
        
        rho_tensor, vx_tensor, vy_tensor, vz_tensor, phi_tensor = _split_outputs(output_0)
        rho_pred0 = rho_tensor.detach().cpu().numpy()
        v_pred_x0 = vx_tensor.detach().cpu().numpy()
        v_pred_y0 = vy_tensor.detach().cpu().numpy() if vy_tensor is not None else None
        phi_pred0 = phi_tensor.detach().cpu().numpy()
 
        rho_max_PN = np.max(rho_pred0)
        
        ## Theoretical Values
        #rho_theory = np.max(rho_o + rho_1*np.exp(alpha * t)*np.cos(2*np.pi*X[:, 0:1]/lam))
        #rho_theory0 = np.max(rho_o + rho_1*np.exp(alpha * 0)*np.cos(2*np.pi*X[:, 0:1]/lam)) ## at t =0 
        
        #diff=abs(rho_max_PN-rho_theory)/abs(rho_max_PN+rho_theory) * 2  ## since the den is rhomax+rhotheory

        
#         ### Difference between peaks for the PINNs solution
        
#         rho_pred0Flat=rho_pred0.reshape(-1)
#         peaks,_=scipy.signal.find_peaks(rho_pred0Flat)
#         peak_lst.append(peaks)
        
#         growth_pert=(rho_theory-rho_theory0)/rho_theory0*100 ## growth percentage
        
#         peak_diff=(rho_pred0Flat[peaks[1]]-rho_pred0Flat[peaks[0]])/(rho_pred0Flat[peaks[1]]+rho_pred0Flat[peaks[0]])

        #g_pred0=phi_x = dde.grad.jacobian(phi_pred0, X, i=0, j=0)
        if isplot:              
            # Create output folder if it doesn't exist
            os.makedirs(output_folder, exist_ok=True)
            
            # Density plot
            plt.figure(1)
            plt.plot(X, rho_pred0, label="t={}".format(round(t,2)))
            plt.ylabel(r"$\rho$")
            plt.xlabel("x")
            plt.grid()
            plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
            plt.title(r"PINNs Solution for $\lambda$ = {} $\lambda_J$".format(round(lam/(2*np.pi),2)))
            #plt.savefig(output_folder+'/PINNS_density'+str(lam)+'_'+str(int(num_of_waves_x))+'_'+str(tmax)+'.png', dpi=300)

            if velocity == True:
                # Velocity plots
                plt.figure(2)
                plt.plot(X, v_pred_x0, '--', label="t={}".format(round(t,2)))
                plt.ylabel("$v_x$")
                plt.xlabel("x")
                plt.title("PINNs Solution Velocity")
                plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
                #plt.savefig(output_folder+'/PINNS_velocity'+str(lam)+'_'+str(int(num_of_waves_x))+'_'+str(tmax)+'.png', dpi=300)

            # Potential plot
            plt.figure(3)
            plt.plot(X, phi_pred0, '--', label="t={}".format(round(t,2)))
            plt.ylabel(r"$\phi$")
            plt.xlabel("x")
            plt.title("PINNs Solution Potential")
            plt.legend(numpoints=1, loc='upper right', fancybox=True, shadow=True)
            #plt.savefig(output_folder+'/phi'+str(lam)+'_'+str(int(num_of_waves_x))+'_'+str(tmax)+'.png', dpi=300)

        
        else:  
            if animation:
                return X, rho_pred0, v_pred_x0, v_pred_y0, phi_pred0, rho_max_PN
            else:
                return X, rho_pred0, rho_max_PN


def Two_D_surface_plots(net, time, initial_params, ax=None, which="density"):
    """
    Create 2D surface plots with velocity vectors

    Args:
        net: Trained neural network OR list of networks (for XPINN)
        time: Time to plot
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        ax: Optional axis to plot on
        which: "density" or "velocity"
    """
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax = initial_params
    
    # Handle both single network and list of networks
    if isinstance(net, list):
        nets = net
        use_xpinn = len(nets) > 1
    else:
        nets = [net]
        use_xpinn = False
    
    # Use N_GRID for consistency with FD solver resolution
    # Exclude right boundary for periodic domains to avoid double-counting
    Q = N_GRID
    xs = np.linspace(xmin, xmax, Q, endpoint=False)
    ys = np.linspace(ymin, ymax, Q, endpoint=False)
    tau, phi = np.meshgrid(xs, ys) 
    Xgrid = np.vstack([tau.flatten(), phi.flatten()]).T
    t_00 = time * np.ones(Q**2).reshape(Q**2, 1)
    
    # Convert to tensors
    pt_x_collocation = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
    pt_y_collocation = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
    pt_z_collocation = Variable(torch.from_numpy(np.full((Q**2,1), SLICE_Z)).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
    pt_t_collocation = Variable(torch.from_numpy(t_00).float(), requires_grad=True).to(device)
    
    # Evaluate network(s)
    if use_xpinn:
        if DIMENSION >= 3:
            raise NotImplementedError("XPINN visualizations currently support up to 2D.")
        output_00 = predict_xpinn(nets, pt_x_collocation, pt_y_collocation, pt_t_collocation, xmin, xmax, ymin, ymax)
    else:
        # Ensure inputs are on the same device as the model
        net_device = next(nets[0].parameters()).device
        pt_x_collocation = pt_x_collocation.to(net_device)
        pt_t_collocation = pt_t_collocation.to(net_device)
        if pt_y_collocation is not None:
            pt_y_collocation = pt_y_collocation.to(net_device)
        if pt_z_collocation is not None:
            pt_z_collocation = pt_z_collocation.to(net_device)
        output_00 = nets[0](_build_input_list(pt_x_collocation, pt_t_collocation, pt_y_collocation, pt_z_collocation))
    
    rho_tensor, vx_tensor, vy_tensor, _, _ = _split_outputs(output_00)
    rho = rho_tensor.detach().cpu().numpy().reshape(Q, Q)
    U = vx_tensor.detach().cpu().numpy().reshape(Q, Q)
    if vy_tensor is not None:
        V = vy_tensor.detach().cpu().numpy().reshape(Q, Q)
    else:
        V = np.zeros_like(U)

    if ax is None:  # for single plot
        plt.figure(figsize=(5, 5))
        ax = plt.gca() 

    # Clean velocity fields and avoid zero-length arrows
    U_clean = np.nan_to_num(U, nan=0.0, posinf=0.0, neginf=0.0)
    V_clean = np.nan_to_num(V, nan=0.0, posinf=0.0, neginf=0.0)
    Vmag = np.sqrt(U_clean**2 + V_clean**2)
    mask = Vmag > 1e-12

    if which == "density":
        pc = ax.pcolormesh(tau, phi, rho, shading='auto', cmap='YlOrBr', vmin=np.min(rho), vmax=np.max(rho))
        skip = (slice(None, None, 5), slice(None, None, 5))
        ax.quiver(
            tau[skip][mask[skip]], phi[skip][mask[skip]],
            U_clean[skip][mask[skip]], V_clean[skip][mask[skip]],
            color='k', headwidth=3.0, width=0.003,
            scale_units='xy', angles='xy', scale=1.0, minlength=0.0, pivot='mid'
        )
        ax.set_title("Density, t={}".format(round(time, 2)))
        cbar = plt.colorbar(pc, shrink=0.6, location='right')
        cbar.formatter.set_powerlimits((0, 0))
        cbar.ax.set_title(r"$\rho$", fontsize=14)
    else:  # velocity magnitude surface plot
        pc = ax.pcolormesh(tau, phi, Vmag, shading='auto', cmap='viridis', vmin=np.min(Vmag), vmax=np.max(Vmag))
        skip = (slice(None, None, 5), slice(None, None, 5))
        ax.quiver(
            tau[skip][mask[skip]], phi[skip][mask[skip]],
            U_clean[skip][mask[skip]], V_clean[skip][mask[skip]],
            color='k', headwidth=3.0, width=0.003,
            scale_units='xy', angles='xy', scale=1.0, minlength=0.0, pivot='mid'
        )
        ax.set_title("Velocity, t={}".format(round(time, 2)))
        cbar = plt.colorbar(pc, shrink=0.6, location='right')
        cbar.ax.set_title(r" $|v|$", fontsize=14)

    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    
    # Add interface lines if using XPINN
    if use_xpinn:
        add_interface_lines(ax, xmin, xmax, ymin, ymax)
    
    return pc


def create_2d_animation(net, initial_params, time_points=None, which="density", fps=2, save_path=None, fixed_colorbar=True, verbose=False):
    """
    Create an animated 2D surface plot showing evolution over time

    Args:
        net: Trained neural network OR list of networks (for XPINN)
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_points: Array of time points for animation (default: 50 points from 0 to 2.0)
        which: "density" or "velocity"
        fps: Frames per second for animation
        save_path: Optional path to save the animation (e.g., 'animation.mp4')
    """
    if time_points is None:
        # Use the provided training tmax from initial_params to bound animation time
        _xmin, _xmax, _ymin, _ymax, _rho_1, _alpha, _lam, _output_folder, tmax = initial_params
        time_points = np.linspace(0.0, float(tmax), 80)
    
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax = initial_params
    
    # Handle both single network and list of networks
    if isinstance(net, list):
        nets = net
        use_xpinn = len(nets) > 1
    else:
        nets = [net]
        use_xpinn = False
    
    if verbose:
        print(f"Creating 2D animation with {len(time_points)} frames...")
    
    # Create output directory for saving plots: always use config.SNAPSHOT_DIR/GRINN
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(8, 8), constrained_layout=True)
    
    # Get data for first frame to set up colorbar limits
    # Use N_GRID for consistency with FD solver resolution
    # Exclude right boundary for periodic domains to avoid double-counting
    Q = N_GRID
    xs = np.linspace(xmin, xmax, Q, endpoint=False)
    ys = np.linspace(ymin, ymax, Q, endpoint=False)
    tau, phi = np.meshgrid(xs, ys)
    if DIMENSION >= 3:
        zeta = np.full_like(tau, SLICE_Z)
    Xgrid = np.vstack([tau.flatten(), phi.flatten()]).T
    t_00 = time_points[0] * np.ones(Q**2).reshape(Q**2, 1)
    
    # Convert to tensors for first frame
    pt_x_collocation = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
    pt_y_collocation = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
    pt_z_collocation = Variable(torch.from_numpy(zeta.reshape(-1, 1)).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
    pt_t_collocation = Variable(torch.from_numpy(t_00).float(), requires_grad=True).to(device)
    
    # Get first frame data to set colorbar limits
    if use_xpinn:
        if DIMENSION >= 3:
            raise NotImplementedError("XPINN animations currently support up to 2D.")
        output_00 = predict_xpinn(nets, pt_x_collocation, pt_y_collocation, pt_t_collocation, xmin, xmax, ymin, ymax)
    else:
        # Ensure inputs are on the same device as the model
        net_device = next(nets[0].parameters()).device
        pt_x_collocation = pt_x_collocation.to(net_device)
        pt_t_collocation = pt_t_collocation.to(net_device)
        if pt_y_collocation is not None:
            pt_y_collocation = pt_y_collocation.to(net_device)
        if pt_z_collocation is not None:
            pt_z_collocation = pt_z_collocation.to(net_device)
        output_00 = nets[0](_build_input_list(pt_x_collocation, pt_t_collocation, pt_y_collocation, pt_z_collocation))
    rho_first_tensor, vx_first_tensor, vy_first_tensor, _, _ = _split_outputs(output_00)
    rho_first = rho_first_tensor.detach().cpu().numpy().reshape(Q, Q)
    U_first = vx_first_tensor.detach().cpu().numpy().reshape(Q, Q)
    if vy_first_tensor is not None:
        V_first = vy_first_tensor.detach().cpu().numpy().reshape(Q, Q)
    else:
        V_first = np.zeros_like(U_first)
    
    # (removed temporary quick-check print of mean(U), mean(V))
    
    # Optionally precompute fixed color limits using first and last frames
    fixed_vmin = None
    fixed_vmax = None
    if which == "density" and fixed_colorbar:
        # Use N_GRID for consistency with FD solver resolution
        # Exclude right boundary for periodic domains to avoid double-counting
        Q = N_GRID
        xs = np.linspace(xmin, xmax, Q, endpoint=False)
        ys = np.linspace(ymin, ymax, Q, endpoint=False)
        tau, phi = np.meshgrid(xs, ys)
        Xgrid = np.vstack([tau.flatten(), phi.flatten()]).T
        # First frame
        t_first = time_points[0] * np.ones(Q**2).reshape(Q**2, 1)
        pt_x = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
        pt_y = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
        pt_z = Variable(torch.from_numpy(zeta.reshape(-1, 1)).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
        pt_t = Variable(torch.from_numpy(t_first).float(), requires_grad=True).to(device)
        if use_xpinn:
            if DIMENSION >= 3:
                raise NotImplementedError("XPINN animations currently support up to 2D.")
            pred_first = predict_xpinn(nets, pt_x, pt_y, pt_t, xmin, xmax, ymin, ymax)
            rho_first = pred_first[:, 0].data.cpu().numpy().reshape(Q, Q)
        else:
            # Ensure inputs are on the same device as the model
            net_device = next(nets[0].parameters()).device
            pt_x = pt_x.to(net_device)
            pt_t = pt_t.to(net_device)
            if pt_y is not None:
                pt_y = pt_y.to(net_device)
            if pt_z is not None:
                pt_z = pt_z.to(net_device)
            rho_first = nets[0](_build_input_list(pt_x, pt_t, pt_y, pt_z))[:, 0].data.cpu().numpy().reshape(Q, Q)
        # Last frame
        t_last = time_points[-1] * np.ones(Q**2).reshape(Q**2, 1)
        pt_t_last = Variable(torch.from_numpy(t_last).float(), requires_grad=True).to(device)
        if use_xpinn:
            pred_last = predict_xpinn(nets, pt_x, pt_y, pt_t_last, xmin, xmax, ymin, ymax)
            rho_last = pred_last[:, 0].data.cpu().numpy().reshape(Q, Q)
        else:
            # Ensure inputs are on the same device as the model
            net_device = next(nets[0].parameters()).device
            pt_t_last = pt_t_last.to(net_device)
            rho_last = nets[0](_build_input_list(pt_x, pt_t_last, pt_y, pt_z))[:, 0].data.cpu().numpy().reshape(Q, Q)
        fixed_vmin = min(np.min(rho_first), np.min(rho_last))
        fixed_vmax = max(np.max(rho_first), np.max(rho_last))
        if fixed_vmin == fixed_vmax:
            eps = 1e-6 if fixed_vmin == 0 else 1e-6 * abs(fixed_vmin)
            fixed_vmin, fixed_vmax = fixed_vmin - eps, fixed_vmax + eps

    # Set up initial plot
    if which == "density":
        # Handle flat initial frame by expanding color limits slightly
        if fixed_colorbar and fixed_vmin is not None:
            vmin_use, vmax_use = fixed_vmin, fixed_vmax
        else:
            rmin, rmax = np.min(rho_first), np.max(rho_first)
            if not np.isfinite(rmin) or not np.isfinite(rmax):
                rmin, rmax = 0.0, 1.0
            if rmin == rmax:
                eps = 1e-6 if rmin == 0 else 1e-6 * abs(rmin)
                rmin, rmax = rmin - eps, rmax + eps
            vmin_use, vmax_use = rmin, rmax
        pc = ax.pcolormesh(tau, phi, rho_first, shading='auto', cmap='YlOrBr', vmin=vmin_use, vmax=vmax_use)
        pert_str = "Sinusoidal" if str(PERTURBATION_TYPE).lower() == "sinusoidal" else "Power Spectrum"
        ax.set_title(f"{pert_str} Density, t={time_points[0]:.2f}")
        cbar = plt.colorbar(pc, shrink=0.6, location='right')
        cbar.formatter.set_powerlimits((0, 0))
        cbar.ax.set_title(r"$\rho$", fontsize=14)
    else:  # velocity magnitude surface plot
        Vmag_first = np.sqrt(U_first**2 + V_first**2)
        pc = ax.pcolormesh(tau, phi, Vmag_first, shading='auto', cmap='viridis')
        pert_str = "Sinusoidal" if str(PERTURBATION_TYPE).lower() == "sinusoidal" else "Power Spectrum"
        ax.set_title(f"{pert_str} Velocity, t={time_points[0]:.2f}")
        cbar = plt.colorbar(pc, shrink=0.6, location='right')
        cbar.ax.set_title(r" $|v|$", fontsize=14)
    
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    
    # Do not save the initial frame by default; animation frames and optional snapshots cover needs
    
    def animate(frame):
        t = time_points[frame]
        if verbose:
            print(f"Animating frame {frame+1}/{len(time_points)} at t={t:.2f}")
        
        # Get data for current time
        t_00 = t * np.ones(Q**2).reshape(Q**2, 1)
        
        # Convert to tensors
        pt_x_collocation = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
        pt_y_collocation = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
        pt_z_collocation = Variable(torch.from_numpy(zeta.reshape(-1, 1)).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
        pt_t_collocation = Variable(torch.from_numpy(t_00).float(), requires_grad=True).to(device)
        
        # Evaluate network(s)
        if use_xpinn:
            output_00 = predict_xpinn(nets, pt_x_collocation, pt_y_collocation, pt_t_collocation, xmin, xmax, ymin, ymax)
        else:
            # Ensure inputs are on the same device as the model
            net_device = next(nets[0].parameters()).device
            pt_x_collocation = pt_x_collocation.to(net_device)
            pt_t_collocation = pt_t_collocation.to(net_device)
            if pt_y_collocation is not None:
                pt_y_collocation = pt_y_collocation.to(net_device)
            if pt_z_collocation is not None:
                pt_z_collocation = pt_z_collocation.to(net_device)
            output_00 = nets[0](_build_input_list(pt_x_collocation, pt_t_collocation, pt_y_collocation, pt_z_collocation))
        
        rho_tensor, vx_tensor, vy_tensor, _, _ = _split_outputs(output_00)
        rho = rho_tensor.detach().cpu().numpy().reshape(Q, Q)
        U = vx_tensor.detach().cpu().numpy().reshape(Q, Q)
        if vy_tensor is not None:
            V = vy_tensor.detach().cpu().numpy().reshape(Q, Q)
        else:
            V = np.zeros_like(U)
        
        # Update plot data
        if which == "density":
            pc.set_array(rho.ravel())
            # Update color limits only if not fixed
            if not fixed_colorbar:
                rmin, rmax = np.min(rho), np.max(rho)
                if rmin == rmax:
                    eps = 1e-6 if rmin == 0 else 1e-6 * abs(rmin)
                    rmin, rmax = rmin - eps, rmax + eps
                pc.set_clim(vmin=rmin, vmax=rmax)
            # Add interface lines if using XPINN
            if use_xpinn:
                add_interface_lines(ax, xmin, xmax, ymin, ymax)
            pert_str = "Sinusoidal" if str(PERTURBATION_TYPE).lower() == "sinusoidal" else "Power Spectrum"
            ax.set_title(f"{pert_str} Density, t={t:.2f}")
        else:  # velocity magnitude surface plot
            Vmag = np.sqrt(U**2 + V**2)
            pc.set_array(Vmag.ravel())
            # Add interface lines if using XPINN
            if use_xpinn:
                add_interface_lines(ax, xmin, xmax, ymin, ymax)
            pert_str = "Sinusoidal" if str(PERTURBATION_TYPE).lower() == "sinusoidal" else "Power Spectrum"
            ax.set_title(f"{pert_str} Velocity, t={t:.2f}")
        
        # Save every 10th frame for static snapshots (disabled to avoid extra plots)
        # if frame % 10 == 0:
        #     save_path_frame = os.path.join(output_dir, f"{which}_t_{t:.2f}.png")
        #     plt.savefig(save_path_frame, dpi=300, bbox_inches='tight')
        #     if verbose:
        #         print(f"Saved frame to {save_path_frame}")
        
        return pc
    
    # Create animation
    anim = animation.FuncAnimation(fig, animate, frames=len(time_points), 
                                 interval=1000/fps, blit=False, repeat=True)
    
    # Save animation with appropriate writer/extension
    try:
        if animation.writers.is_available('ffmpeg'):
            animation_path = os.path.join(output_dir, f"{which}_animation.mp4")
            if verbose:
                print(f"Saving animation to {animation_path} with ffmpeg...")
            anim.save(animation_path, writer='ffmpeg', fps=fps)
            saved_format = 'mp4'
        else:
            animation_path = os.path.join(output_dir, f"{which}_animation.gif")
            if verbose:
                print(f"ffmpeg not available. Saving animation to {animation_path} with Pillow...")
            anim.save(animation_path, writer='pillow', fps=fps)
            saved_format = 'gif'
        if verbose:
            print("Animation saved successfully!")
    except Exception as e:
        print(f"Animation save failed: {e}")
        raise
    
    # Optional static snapshots uniformly over [0, tmax]
    # Static snapshots disabled to prevent extra plots
    # if SAVE_STATIC_SNAPSHOTS:
    #     snapshot_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    #     os.makedirs(snapshot_dir, exist_ok=True)
    #     # Determine tmax from initial_params
    #     _xmin, _xmax, _ymin, _ymax, _rho_1, _alpha, _lam, _output_folder, tmax_val = initial_params
    #     times_static = np.linspace(0.0, float(tmax_val), 5)
    #     if verbose:
    #         print(f"Saving {len(times_static)} static snapshots to {snapshot_dir} over [0, {tmax_val}]...")
    #     for t in times_static:
    #         t_00 = t * np.ones(Q**2).reshape(Q**2, 1)
    #         pt_x_collocation = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
    #         pt_y_collocation = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device)
    #         pt_t_collocation = Variable(torch.from_numpy(t_00).float(), requires_grad=True).to(device)
    #         output_00 = net([pt_x_collocation, pt_y_collocation, pt_t_collocation])
    #         rho = output_00[:, 0].data.cpu().numpy().reshape(Q, Q)
    #         U = output_00[:, 1].data.cpu().numpy().reshape(Q, Q)
    #         V = output_00[:, 2].data.cpu().numpy().reshape(Q, Q)
    #
    #         fig_static, ax_static = plt.subplots(figsize=(8, 8))
    #         if which == "density":
    #             pc_static = ax_static.pcolormesh(tau, phi, rho, shading='auto', cmap='YlOrBr')
    #             cbar_static = plt.colorbar(pc_static, shrink=0.6, location='right')
    #             cbar_static.formatter.set_powerlimits((0, 0))
    #             cbar_static.ax.set_title(r"$\rho$", fontsize=14)
    #         else:
    #             Vmag = np.sqrt(U**2 + V**2)
    #             pc_static = ax_static.pcolormesh(tau, phi, Vmag, shading='auto', cmap='viridis')
    #             cbar_static = plt.colorbar(pc_static, shrink=0.6, location='right')
    #             cbar_static.ax.set_title(r" $|v|$", fontsize=14)
    #         ax_static.set_xlim(xmin, xmax)
    #         ax_static.set_ylim(ymin, ymax)
    #         ax_static.set_xlabel("x")
    #         ax_static.set_ylabel("y")
    #         plt.tight_layout()
    #         static_save_path = os.path.join(snapshot_dir, f"{which}_static_t_{t:.2f}.png")
    #         plt.savefig(static_save_path, dpi=300, bbox_inches='tight')
    #         plt.close(fig_static)
    #         if verbose:
    #             print(f"Saved static snapshot to {static_save_path}")
    
    # Generate 5x3 comparison tables for both density and velocity (only once per animation call)
    if verbose:
        print("Generating 5x3 comparison tables...")
    
    # Only generate comparison tables if this is the density animation call
    # This prevents duplicate generation when both density and velocity animations are created
    if which == "density":
        # Compute cache for 5x3 comparison table time points (5 points uniformly over [0, tmax])
        comparison_time_points = np.linspace(0.0, float(tmax), 5)
        print(f"Computing FD cache for 5x3 comparison table ({len(comparison_time_points)} time points)...")
        fd_cache_5x3 = compute_fd_data_cache(
            initial_params, comparison_time_points,
            N=N_GRID, nu=0.5
        )
        
        print("Generating density comparison table...")
        # Create comparison table for density - use cached data
        create_5x3_comparison_table(net, initial_params, which="density", N=N_GRID, nu=0.5, fd_cache=fd_cache_5x3)
        
        print("Generating velocity comparison table...")
        # Create comparison table for velocity - use cached data
        create_5x3_comparison_table(net, initial_params, which="velocity", N=N_GRID, nu=0.5, fd_cache=fd_cache_5x3)
    
    # Display animation inline if in a notebook
    try:
        from IPython.display import HTML, display
        import base64
        with open(animation_path, 'rb') as f:
            data = f.read()
        data_base64 = base64.b64encode(data).decode('utf-8')
        if saved_format == 'mp4':
            video_html = f'''
    <div style="text-align: center;">
        <h3>{which.title()} Evolution Animation</h3>
        <video width="600" height="600" controls autoplay loop>
            <source src="data:video/mp4;base64,{data_base64}" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </div>
    '''
            display(HTML(video_html))
        else:
            img_html = f'''
    <div style="text-align: center;">
        <h3>{which.title()} Evolution Animation</h3>
        <img src="data:image/gif;base64,{data_base64}" width="600" height="600" />
    </div>
    '''
            display(HTML(img_html))
    except Exception as _:
        pass
    
    # Avoid displaying the figure in non-notebook runs
    plt.close(fig)
    
    return anim


def create_2d_surface_plots(net, initial_params, time_points=None, which="density"):
    """
    Create 2D surface plots at multiple time points

    Args:
        net: Trained neural network
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_points: List of time points to plot (default: [0.0, 0.5, 1.0, 1.5, 2.0])
        which: "density" or "velocity"
    """
    if time_points is None:
        time_points = [0.0, 0.5, 1.0, 1.5, 2.0]
    
    print("Creating 2D surface plots...")
    
    # Create subplots for multiple time points
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    for i, t in enumerate(time_points):
        if i < len(axes):
            print(f"Plotting at t = {t}")
            Two_D_surface_plots(net, t, initial_params, ax=axes[i], which=which)

    # Remove the last empty subplot if needed
    if len(time_points) < len(axes):
        fig.delaxes(axes[-1])

    plt.tight_layout()
    
    # Save the figure to output folder
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, f"2d_surface_plots_{which}.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved 2D surface plots ({which}) to {save_path}")
    
    return fig, axes


def create_1d_comparison_plots(net, initial_params, time_array_1d=None):
    """
    Create 1D cross section comparison plots between PINN, Linear Theory, and Finite Difference
    
    Args:
        net: Trained neural network
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_array_1d: List of time points for 1D plots (default: [0.5, 1.0, 1.5])
    """
    if time_array_1d is None:
        time_array_1d = [0.5, 1.0, 1.5]
    
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax = initial_params
    # rho_o imported from config.py
    num_of_waves = (xmax - xmin) / lam
    
    print("Creating 1D cross section comparison plots...")
    
    # Create comparison plots
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    axes = axes.flatten()

    # Collect all velocity data to determine appropriate y-axis limits
    all_velocity_data = []
    
    for i, time in enumerate(time_array_1d):
        print(f"Creating 1D comparison plots at t = {time}")
        
        # Get LAX solution (Finite Difference)
        x, rho, v, phi, n, rho_LT, rho_LT_max, rho_max_FD, v_LT = _timed_call(
            "LAX comparison slice (cpu)",
            lax_solution,
            time, FD_N_2D, 0.5, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=True, animation=True
        )
        
        # Get PINN solution
        X, rho_pred0, v_pred_x0, v_pred_y0, phi_pred0, rho_max_PN = plot_function(
            net, [time], initial_params, velocity=True, isplot=False, animation=True
        )
        
        # Interpolate LAX solutions to match PINN grid
        from scipy.interpolate import interp1d
        X_flat = X.flatten()
        
        # Interpolate Linear Theory and Finite Difference to PINN grid
        rho_LT_interp = interp1d(x, rho_LT, kind='linear', bounds_error=False, fill_value='extrapolate')(X_flat)
        rho_FD_interp = interp1d(x, rho[n-1,:], kind='linear', bounds_error=False, fill_value='extrapolate')(X_flat)
        v_LT_interp = interp1d(x, v_LT, kind='linear', bounds_error=False, fill_value='extrapolate')(X_flat)
        v_FD_interp = interp1d(x, v[n-1,:], kind='linear', bounds_error=False, fill_value='extrapolate')(X_flat)
        phi_FD_interp = interp1d(x, phi[n-1,:], kind='linear', bounds_error=False, fill_value='extrapolate')(X_flat)
        
        # Collect velocity data for limit calculation
        all_velocity_data.append(v_pred_x0)
        all_velocity_data.append(v_FD_interp)
        if (np.isclose(KY, 0.0)) and (a < 0.1):
            all_velocity_data.append(v_LT_interp)
        
        # Density comparison plots
        axes[i*3].plot(X, rho_pred0, color='c', linewidth=3, label="PINN")
        # Only plot Linear Theory when KY == 0 and amplitude is small
        if (np.isclose(KY, 0.0)) and (a < 0.1):
            axes[i*3].plot(X, rho_LT_interp, linestyle='dashed', color='firebrick', linewidth=2, label="Linear Theory")
        axes[i*3].plot(X, rho_FD_interp, linestyle='solid', color='black', linewidth=1, label="Finite Difference")
        axes[i*3].set_xlim(xmin, xmax)
        axes[i*3].set_title(f"Density at t={time:.1f}")
        axes[i*3].set_ylabel(r"$\rho$")
        axes[i*3].grid(True)
        axes[i*3].legend()
        axes[i*3].set_ylim(0.5*rho_o, 1.5*rho_o)
        
        # Velocity comparison plots
        axes[i*3+1].plot(X, v_pred_x0, color='c', linewidth=3, label="PINN")
        # Only plot Linear Theory when KY == 0 and amplitude is small
        if (np.isclose(KY, 0.0)) and (a < 0.1):
            axes[i*3+1].plot(X, v_LT_interp, linestyle='dashed', color='firebrick', linewidth=2, label="Linear Theory")
        axes[i*3+1].plot(X, v_FD_interp, linestyle='solid', color='black', linewidth=1, label="Finite Difference")
        axes[i*3+1].set_xlim(xmin, xmax)
        axes[i*3+1].set_title(f"Velocity at t={time:.1f}")
        axes[i*3+1].set_ylabel("$v_x$")
        axes[i*3+1].grid(True)
        axes[i*3+1].legend()
        
        # Potential comparison plots
        axes[i*3+2].plot(X, phi_pred0, color='c', linewidth=3, label="PINN")
        axes[i*3+2].plot(X, phi_FD_interp, linestyle='solid', color='black', linewidth=1, label="Finite Difference")
        axes[i*3+2].set_xlim(xmin, xmax)
        axes[i*3+2].set_title(f"Potential at t={time:.1f}")
        axes[i*3+2].set_ylabel(r"$\phi$")
        axes[i*3+2].set_xlabel("x")
        axes[i*3+2].grid(True)
        axes[i*3+2].legend()

    # Set velocity y-axis limits based on amplitude (same logic as create_1d_cross_sections_sinusoidal)
    # Calculate limits from all collected velocity data and apply consistently to all velocity plots
    if a < 0.1:
        # Default limits for small amplitude cases
        v_limu_default = 0.055
        v_liml_default = -0.055
    else:
        # Default limits for large amplitude cases
        v_limu_default = 0.6
        v_liml_default = -0.6
    
    # Calculate actual data range across all velocity data with padding
    if all_velocity_data:
        v_min = np.min([np.min(v_data) for v_data in all_velocity_data])
        v_max = np.max([np.max(v_data) for v_data in all_velocity_data])
        v_range = v_max - v_min
        # Add 10% padding to ensure all data stays within limits
        padding = max(0.01, 0.1 * v_range)
        v_liml_actual = v_min - padding
        v_limu_actual = v_max + padding
        
        # Ensure limits are at least as wide as default, but expand if data exceeds them
        v_liml_actual = min(v_liml_actual, v_liml_default)
        v_limu_actual = max(v_limu_actual, v_limu_default)
    else:
        v_liml_actual = v_liml_default
        v_limu_actual = v_limu_default
    
    # Apply consistent limits to all velocity plots
    for i in range(len(time_array_1d)):
        axes[i*3+1].set_ylim(v_liml_actual, v_limu_actual)

    plt.tight_layout()
    
    # Save the figure to output folder
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "1d_comparison_plots.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved 1D comparison plots to {save_path}")
    
    plt.show()


def create_growth_comparison_plot(net, initial_params, time_array_growth=None):
    """
    Create growth comparison plot showing density maximum evolution over time
    
    Args:
        net: Trained neural network
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_array_growth: Array of time points for growth analysis (default: 10 points from 0.1 to tmax)
    """
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax = initial_params
    # rho_o imported from config.py
    num_of_waves = (xmax - xmin) / lam
    
    if time_array_growth is None:
        time_array_growth = np.linspace(0.1, tmax, 5)  # Reduced from 10 to 5 points
    
    print("Creating growth comparison plot...")
    
    Growth_LT_list = []
    Growth_FD_list = []
    Growth_PN_list = []

    for i, time in enumerate(time_array_growth):
        print(f"Processing growth point {i+1}/{len(time_array_growth)} at t={time:.2f}")
        # Get LAX solution with configured grid resolution
        x, rho, v, phi, n, rho_LT, rho_LT_max, rho_max_FD, v_LT = _timed_call(
            "LAX comparison slice (cpu)",
            lax_solution,
            time, FD_N_2D, 0.5, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=True, animation=True
        )
        
        # Get PINN solution
        X, rho_pred0, v_pred_x0, v_pred_y0, phi_pred0, rho_max_PN = plot_function(
            net, [time], initial_params, velocity=True, isplot=False, animation=True
        )
        
        Growth_LT = rho_LT_max - rho_o
        Growth_FD = rho_max_FD - rho_o  
        Growth_PN = rho_max_PN - rho_o
        
        Growth_LT_list.append(Growth_LT)
        Growth_FD_list.append(Growth_FD)
        Growth_PN_list.append(Growth_PN)

    # Plot growth comparison
    plt.figure(figsize=(8, 6))
    # Only plot Linear Theory when KY == 0 and amplitude is small
    if (np.isclose(KY, 0.0)) and (a < 0.1):
        plt.plot(time_array_growth, np.log(Growth_LT_list), marker='o', color='b', linewidth=2, label="Linear Theory")
    plt.plot(time_array_growth, np.log(Growth_FD_list), '--', marker='*', color='k', linewidth=3, label="Finite Difference")
    plt.plot(time_array_growth, np.log(Growth_PN_list), marker='^', markersize=8, linewidth=2, color='r', label="PINN")
    plt.xlabel("t", fontsize=14)
    plt.ylabel(r"$\log (\rho_{\rm max} - \rho_{0})$", fontsize=14)
    plt.grid(True)
    plt.legend(fontsize=12)
    plt.title("Growth Comparison: PINN vs Linear Theory vs Finite Difference")
    plt.tight_layout()
    
    # Save the figure to output folder
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "growth_comparison_plot.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved growth comparison plot to {save_path}")
    
    plt.show()


def compute_fd_data_cache(initial_params, time_points, N=200, nu=0.5,
                          use_velocity_ps=None, ps_index=None, vel_rms=None, random_seed=None):
    """
    Compute and cache FD solver data for all time points to avoid redundant solver calls.
    
    Args:
        initial_params: (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_points: List of time points to compute
        N: Grid resolution for LAX solver
        nu: Courant number for LAX solver
        use_velocity_ps: Whether to use velocity power spectrum (defaults to config)
        ps_index: Power spectrum index (defaults to POWER_EXPONENT)
        vel_rms: Velocity RMS amplitude (defaults to a*cs)
        random_seed: Random seed (defaults to RANDOM_SEED)
    
    Returns:
        Dictionary mapping time -> FD data: {time: {'x': x_fd, 'y': y_fd, 'z': z_fd (if 3D),
                                                    'rho': rho_fd, 'vx': vx_fd, 'vy': vy_fd, 'phi': phi_fd}}
    """
    xmin, xmax, ymin, ymax, rho_1, _alpha, lam, _output_folder, _tmax = initial_params
    
    # Use config defaults if not specified
    if use_velocity_ps is None:
        use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
    if ps_index is None:
        ps_index = POWER_EXPONENT
    if vel_rms is None:
        vel_rms = a * cs
    if random_seed is None:
        random_seed = RANDOM_SEED
    
    num_of_waves = (xmax - xmin) / lam
    
    # Decide grid resolution policy
    if str(PERTURBATION_TYPE).lower() == "power_spectrum":
        N_use = N_GRID
    else:
        N_use = FD_N_2D if N is None else N
    
    # Convert time_points to numpy array and sort
    time_points = np.array(time_points)
    time_points = np.sort(np.unique(time_points))
    max_time = float(np.max(time_points))
    
    print(f"Computing FD data cache for {len(time_points)} time points (optimized: single run to t={max_time:.2f})...")
    fd_cache = {}
    
    # OPTIMIZED: Run solver once to max_time with snapshots at all time points
    if DIMENSION == 3:
        # Use unified solver with snapshot support
        results_dict = _call_unified_3d_solver(
            time=max_time, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1, nu=nu,
            use_velocity_ps=use_velocity_ps, ps_index=ps_index,
            vel_rms=vel_rms, random_seed=random_seed,
            save_times=time_points.tolist()
        )
        
        # Process each snapshot
        for t in time_points:
            if t not in results_dict:
                print(f"  Warning: snapshot at t={t:.2f} not found, skipping...")
                continue
            
            print(f"  Processing snapshot at t = {t:.2f}")
            result = results_dict[t]
            
            # Extract results from unified solver
            x_fd = result.coordinates['x']
            y_fd = result.coordinates['y']
            z_fd = result.coordinates['z']
            rho_vol = result.density
            vx_vol, vy_vol, vz_vol = result.velocity_components
            phi_vol = result.potential
            
            # Adaptive z-slice selection based on perturbation type
            if str(PERTURBATION_TYPE).lower() == 'power_spectrum':
                # ADAPTIVE Z-SELECTION: Choose slice with most structure
                z_idx, z_val, contrast = find_max_contrast_slice(rho_vol, z_fd)
                print(f"  📊 Auto-selected z={z_val:.3f} (contrast={contrast:.2e})")
                
                # Optional: Save diagnostic plot first time
                if t == time_points[0]:  # Only for first timepoint
                    analyze_z_variation(rho_vol, z_fd, t, save_dir=SNAPSHOT_DIR)
            else:  # sinusoidal
                # FIXED Z-SELECTION: Use config value for sinusoidal
                z_idx = np.argmin(np.abs(z_fd - SLICE_Z))
                z_val = z_fd[z_idx]
                print(f"  Using fixed z={z_val:.3f} (sinusoidal)")
            
            # Extract 2D slices
            rho_fd = rho_vol[:, :, z_idx]
            vx_fd = vx_vol[:, :, z_idx]
            vy_fd = vy_vol[:, :, z_idx]
            phi_fd = phi_vol[:, :, z_idx] if phi_vol is not None else np.zeros_like(rho_fd)
            
            # Store selected z for later use
            fd_cache[t] = {
                'x': x_fd,
                'y': y_fd,
                'z': z_fd,
                'rho': rho_fd,
                'vx': vx_fd,
                'vy': vy_fd,
                'phi': phi_fd,
                'z_idx': z_idx,     # Store index
                'z_val': z_val,     # Store value
                'rho_vol': rho_vol,  # Keep volume for reference (optional)
                'vx_vol': vx_vol,
                'vy_vol': vy_vol,
                'phi_vol': phi_vol,
            }
    else:
        # 2D case - use optimized single-run approach
        # Set up domain and physics parameters for unified solver
        Lx = Ly = lam * num_of_waves
        domain_params = {'Lx': Lx, 'Ly': Ly, 'nx': N_use, 'ny': N_use}
        physics_params = {
            'c_s': cs,
            'rho_o': rho_o,
            'const': const,
            'G': G,
            'rho_1': rho_1,
            'lam': lam
        }
        options = {'gravity': True, 'nu': nu, 'comparison': False, 'isplot': False}
        
        # Set up IC parameters
        if use_velocity_ps:
            ic_type = 'power_spectrum'
            ic_params = {
                'power_index': ps_index,
                'amplitude': vel_rms,
                'random_seed': random_seed,
                'vx0_shared': _shared_vx_np,
                'vy0_shared': _shared_vy_np
            }
        else:
            ic_type = 'sinusoidal'
            ic_params = {'KX': KX, 'KY': KY}
        
        # Run solver once with snapshots
        if torch.cuda.is_available() or (use_velocity_ps and _shared_vx_np is None):
            # Use torch solver (supports save_times)
            _clear_cuda_cache()
            results_dict = _timed_call(
                "LAX 2D (optimized torch)",
                lax_solver_torch,
                time=max_time, domain_params=domain_params,
                physics_params=physics_params,
                ic_type=ic_type, ic_params=ic_params,
                options=options, save_times=time_points.tolist()
            )
        else:
            # Fallback: use old approach for shared-field CPU case
            # (lax_solution doesn't support save_times yet)
            results_dict = {}
            for t in time_points:
                print(f"  Computing FD data at t = {t:.2f}")
                n_fd_use = int(_shared_vx_np.shape[0])
                x_fd, rho_fd, vx_fd, vy_fd, phi_fd, _n, _rho_max = _timed_call(
                    "LAX 2D (shared-field cpu)",
                    lax_solution,
                    t, n_fd_use, nu, lam, num_of_waves, rho_1,
                    gravity=True, isplot=False, comparison=False, animation=True,
                    vx0_shared=_shared_vx_np, vy0_shared=_shared_vy_np
                )
                Lx = lam * num_of_waves
                Nx = x_fd.shape[0]
                Ny = rho_fd.shape[1]
                y_fd = np.linspace(0.0, Lx, Ny, endpoint=False)
                # Create a simple result object
                class SimpleResult:
                    def __init__(self):
                        self.coordinates = {'x': x_fd, 'y': y_fd}
                        self.density = rho_fd
                        self.velocity_components = [vx_fd, vy_fd]
                        self.potential = phi_fd
                results_dict[t] = SimpleResult()
        
        # Process each snapshot for 2D
        for t in time_points:
            if t not in results_dict:
                print(f"  Warning: snapshot at t={t:.2f} not found, skipping...")
                continue
            
            print(f"  Processing snapshot at t = {t:.2f}")
            result = results_dict[t]
            
            x_fd = result.coordinates['x']
            y_fd = result.coordinates['y']
            rho_fd = result.density
            vx_fd, vy_fd = result.velocity_components
            phi_fd = result.potential if hasattr(result, 'potential') and result.potential is not None else np.zeros_like(rho_fd)
            
            fd_cache[t] = {
                'x': x_fd,
                'y': y_fd,
                'rho': rho_fd,
                'vx': vx_fd,
                'vy': vy_fd,
                'phi': phi_fd
            }
    
    print(f"FD data cache computed for {len(fd_cache)} time points.")
    return fd_cache


def create_all_plots(net, initial_params, include_growth=False,
                     fd_use_velocity_ps=None, fd_ps_index=None, fd_vel_rms=None, fd_random_seed=None):
    """
    Create only 2D surface plot grids (density and velocity). Optionally create FD grids and return figures.
    Uses cached FD data to avoid redundant solver calls.

    Args:
        net: Trained neural network
        initial_params: (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        include_growth: Unused now; kept for API compatibility.
        fd_use_velocity_ps: Override for FD velocity power spectrum flag (defaults to config)
        fd_ps_index: Override for FD power spectrum index (defaults to POWER_EXPONENT)
        fd_vel_rms: Override for FD velocity RMS (defaults to a*cs)
        fd_random_seed: Override for FD random seed (defaults to 1234)
    Returns:
        dict with figures/axes: {"pinn_density": (fig, axes), "pinn_velocity": (fig, axes),
                                 "fd_density": (fig, axes), "fd_velocity": (fig, axes)}
    """
    print("="*60)
    print("CREATING GRID VISUALIZATION PLOTS")
    print("="*60)
    
    # Use config defaults if not overridden to ensure consistency with PINN training
    if fd_use_velocity_ps is None:
        fd_use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
    if fd_ps_index is None:
        fd_ps_index = POWER_EXPONENT
    if fd_vel_rms is None:
        fd_vel_rms = a * cs
    if fd_random_seed is None:
        fd_random_seed = RANDOM_SEED

    # Default time points for 2D surface plots
    time_points_2d = [0.0, 0.5, 1.0, 1.5, 2.0]
    
    # Get time points for 1D cross-sections (if used)
    time_points_1d = TIMES_1D if isinstance(TIMES_1D, (list, tuple)) and len(TIMES_1D) > 0 else [0.5, 1.0, 1.5]
    
    # Combine all time points and remove duplicates
    all_time_points = sorted(list(set(time_points_2d + time_points_1d)))
    
    # Compute FD data cache once for all plots (2D and 1D)
    print(f"Computing FD cache for {len(all_time_points)} unique time points...")
    fd_cache = compute_fd_data_cache(
        initial_params, all_time_points,
        use_velocity_ps=fd_use_velocity_ps, ps_index=fd_ps_index,
        vel_rms=fd_vel_rms, random_seed=fd_random_seed
    )

    # 1. PINN density grid
    fig_den, axes_den = create_2d_surface_plots(net, initial_params, which="density")

    # 2. PINN velocity grid
    fig_vel, axes_vel = create_2d_surface_plots(net, initial_params, which="velocity")

    # 3. FD density grid - uses cached data
    fig_fd_den, axes_fd_den = create_2d_surface_plots_FD(initial_params, which="density",
                                                         use_velocity_ps=fd_use_velocity_ps, ps_index=fd_ps_index,
                                                         vel_rms=fd_vel_rms, random_seed=fd_random_seed,
                                                         fd_cache=fd_cache)

    # 4. FD velocity grid - uses cached data
    fig_fd_vel, axes_fd_vel = create_2d_surface_plots_FD(initial_params, which="velocity",
                                                         use_velocity_ps=fd_use_velocity_ps, ps_index=fd_ps_index,
                                                         vel_rms=fd_vel_rms, random_seed=fd_random_seed,
                                                         fd_cache=fd_cache)

    print("="*60)
    print("ALL GRID PLOTS COMPLETED!")
    print("="*60)

    # Display all created figures so they appear when called from train.py
    plt.show()

    result = {
        "pinn_density": (fig_den, axes_den),
        "pinn_velocity": (fig_vel, axes_vel),
        "fd_density": (fig_fd_den, axes_fd_den),
        "fd_velocity": (fig_fd_vel, axes_fd_vel),
        "fd_cache": fd_cache,  # Return cache for reuse in other plotting functions
    }
    
    return result


def create_density_growth_plot(net, initial_params, tmax, dt=0.1):
    """
    Create a PINN vs LAX density growth comparison plot.

    Plots over time: (1) maximum density, (2) log(rho_max - rho_o + eps).

    Args:
        net: trained PINN model
        initial_params: (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax_train)
        tmax: maximum time to plot (independent of training tmax)
        dt: temporal spacing (default 0.1)
    """
    xmin, xmax, ymin, ymax, rho_1, _alpha, lam, _output_folder, _tmax_train = initial_params
    
    # Handle both single network and list of networks (XPINN)
    if isinstance(net, list):
        nets = net
        use_xpinn = len(nets) > 1
    else:
        nets = [net]
        use_xpinn = False
    num_of_waves = (xmax - xmin) / lam

    # Time grid (inclusive of tmax)
    time_points = np.arange(0.0, float(tmax) + 1e-9, float(dt))

    # Pre-compute FD cache for all time points to avoid redundant solver calls
    # This dramatically speeds up the plot generation, especially for 3D
    print(f"Pre-computing FD data cache for {len(time_points)} time points (this may take a moment)...")
    fd_cache = compute_fd_data_cache(
        initial_params, time_points, N=None, nu=0.5,
        use_velocity_ps=None, ps_index=None, vel_rms=None, random_seed=None
    )
    print("FD cache computed. Generating density growth plot...")

    pinn_max_list = []
    fd_max_list = []

    # PINN grid sampling settings (use N_GRID for consistency with FD solver)
    # Exclude right boundary for periodic domains to avoid double-counting
    Q = N_GRID
    xs = np.linspace(xmin, xmax, Q, endpoint=False)
    ys = np.linspace(ymin, ymax, Q, endpoint=False)
    TAU, PHI = np.meshgrid(xs, ys)
    if DIMENSION >= 3:
        ZETA = np.full_like(TAU, SLICE_Z)
    Xgrid = np.vstack([TAU.flatten(), PHI.flatten()]).T

    for idx, t in enumerate(time_points):
        # PINN evaluation on QxQ grid
        t_vec = t * np.ones(Q**2).reshape(Q**2, 1)
        pt_x = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
        pt_y = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
        pt_z = Variable(torch.from_numpy(ZETA.reshape(-1, 1)).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
        pt_t = Variable(torch.from_numpy(t_vec).float(), requires_grad=True).to(device)
        if use_xpinn:
            if DIMENSION >= 3:
                raise NotImplementedError("XPINN density-growth visualization not supported for DIMENSION=3.")
            out = predict_xpinn(nets, pt_x, pt_y, pt_t, xmin, xmax, ymin, ymax)
        else:
            out = nets[0](_build_input_list(pt_x, pt_t, pt_y, pt_z))
        rho_tensor, *_ = _split_outputs(out)
        rho_pinn = rho_tensor.detach().cpu().numpy().reshape(Q, Q)
        pinn_max_list.append(np.max(rho_pinn))

        # LAX/FD evaluation using cached data
        if DIMENSION == 3:
            # Use cached FD data
            if t in fd_cache:
                cache_data = fd_cache[t]
                rho_fd = cache_data['rho']
                fd_max_list.append(np.max(rho_fd))
            else:
                # Fallback: compute on the fly if cache miss (shouldn't happen)
                use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
                result = _call_unified_3d_solver(
                    time=t, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1, nu=0.5,
                    use_velocity_ps=use_velocity_ps, ps_index=POWER_EXPONENT,
                    vel_rms=a*cs, random_seed=RANDOM_SEED
                )
                rho_fd = result.density
                z_grid = result.coordinates['z']
                z_idx = np.argmin(np.abs(z_grid - SLICE_Z))
                fd_max_list.append(np.max(rho_fd[:, :, z_idx]))
        else:
            # Use cached FD data for 2D
            if t in fd_cache:
                cache_data = fd_cache[t]
                rho_fd = cache_data['rho']
                fd_max_list.append(np.max(rho_fd))
            else:
                # Fallback: compute on the fly if cache miss (shouldn't happen)
                if torch.cuda.is_available():
                    _clear_cuda_cache()
                    use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
                    if idx == 0:
                        print(f"Using GPU solver for density growth plot (CUDA available)")
                    x_fd, rho_fd, _vx_fd, _vy_fd, _phi_fd, _n, _rho_max = _timed_call(
                        "LAX 2D (torch)",
                        lax_solution_torch,
                        time_val=t, N=N_GRID, nu=0.5, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1,
                        gravity=True, use_velocity_ps=use_velocity_ps, ps_index=POWER_EXPONENT, 
                        vel_rms=a*cs, random_seed=RANDOM_SEED
                    )
                else:
                    if str(PERTURBATION_TYPE).lower() == "power_spectrum":
                        if _shared_vx_np is not None and _shared_vy_np is not None:
                            n_fd_use = int(_shared_vx_np.shape[0])
                            x_fd, rho_fd, _vx_fd, _vy_fd, _phi_fd, _n, _rho_max = _timed_call(
                                "LAX 2D (shared-field cpu)",
                                lax_solution,
                                t, n_fd_use, 0.5, lam, num_of_waves, rho_1,
                                gravity=True, isplot=False, comparison=False, animation=True,
                                vx0_shared=_shared_vx_np, vy0_shared=_shared_vy_np
                            )
                        else:
                            x_fd, rho_fd, _vx_fd, _vy_fd, _phi_fd, _n, _rho_max = _timed_call(
                                "LAX 2D (power cpu)",
                                lax_solution,
                                t, N_GRID, 0.5, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=False, animation=True,
                                use_velocity_ps=True, ps_index=POWER_EXPONENT, vel_rms=a*cs, random_seed=RANDOM_SEED
                            )
                    else:
                        x_fd, rho_fd, _vx_fd, _vy_fd, _phi_fd, _n, _rho_max = _timed_call(
                            "LAX 2D (sinusoidal cpu)",
                            lax_solution,
                            t, FD_N_2D, 0.5, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=False, animation=True,
                            use_velocity_ps=False
                        )
                fd_max_list.append(np.max(rho_fd))

    # Build figure with two subplots
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    # (1) Max density vs time
    axes[0].plot(time_points, fd_max_list, label="LAX", color='k', linewidth=2)
    axes[0].plot(time_points, pinn_max_list, label="PINN", color='c', linewidth=2)
    axes[0].set_xlabel("t")
    axes[0].set_ylabel(r"$\rho_{\max}$")
    axes[0].set_title("Maximum Density vs Time")
    axes[0].grid(True)
    axes[0].legend()
    # Annotate parameters on the plot
    try:
        param_str = f"a={a}, power_index={POWER_EXPONENT}"
        axes[0].text(0.02, 0.95, param_str, transform=axes[0].transAxes,
                     fontsize=9, va='top', bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='gray', alpha=0.6))
    except Exception:
        pass

    # (2) log growth vs time
    eps = 1e-12
    axes[1].plot(time_points, np.log(np.maximum(np.array(fd_max_list) - rho_o, 0.0) + eps), label="LAX", color='k', linewidth=2)
    axes[1].plot(time_points, np.log(np.maximum(np.array(pinn_max_list) - rho_o, 0.0) + eps), label="PINN", color='c', linewidth=2)
    axes[1].set_xlabel("t")
    axes[1].set_ylabel(r"$\log(\rho_{\max} - \rho_0)$")
    axes[1].set_title("Density Growth (log)")
    axes[1].grid(True)
    axes[1].legend()
    # Mirror annotation on second axis
    try:
        param_str = f"a={a}, power_index={POWER_EXPONENT}"
        axes[1].text(0.02, 0.95, param_str, transform=axes[1].transAxes,
                     fontsize=9, va='top', bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='gray', alpha=0.6))
    except Exception:
        pass

    plt.tight_layout()

    # Save figure
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "density_growth_comparison.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved density growth comparison plot to {save_path}")

    plt.show()

    return fig, axes


def create_1d_cross_sections_sinusoidal(net, initial_params, time_points=None, y_fixed=0.6, N_fd=1000, nu_fd=0.5,
                                        fd_cache=None):
    """
    Create 1D cross-section plots at fixed y for sinusoidal perturbations, comparing
    PINN vs Linear Theory vs 1D LAX (sinusoidal).

    Args:
        net: trained network
        initial_params: (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_points: list of times to plot
        y_fixed: y value for the 1D slice through the 2D domain
        N_fd: grid size for 1D LAX solver (only used if fd_cache is None)
        nu_fd: Courant number for 1D LAX solver (only used if fd_cache is None)
        fd_cache: Optional dictionary mapping time -> FD data (if provided, solver is not called)
    """
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, _output_folder, _tmax = initial_params
    
    # Handle both single network and list of networks
    if isinstance(net, list):
        nets = net
        use_xpinn = len(nets) > 1
    else:
        nets = [net]
        use_xpinn = False
    num_of_waves = (xmax - xmin) / lam

    if time_points is None:
        time_points = TIMES_1D if isinstance(TIMES_1D, (list, tuple)) and len(TIMES_1D) > 0 else [0.5, 1.0, 1.5]

    # Use baseline density 1.0 for Linear Theory reference
    rho_base = 1.0
    jeans = np.sqrt(4*np.pi**2*cs**2/(const*G*rho_base))
    k = np.sqrt(KX**2 + KY**2 + KZ**2)
    v1_lt = (rho_1 / rho_base) * (alpha / k) if k > 1e-12 else 0.0

    # Build x grid for PINN slice
    X = np.linspace(xmin, xmax, 1000).reshape(1000, 1)
    Y = y_fixed * np.ones_like(X)
    z_fixed = SLICE_Z if DIMENSION >= 3 else None
    if DIMENSION >= 3:
        Z = z_fixed * np.ones_like(X)
    else:
        Z = None

    # Create 2 rows x T columns panel layout matching target style
    T = len(time_points)
    fig = plt.figure(figsize=(6*T, 8), constrained_layout=False)
    grid = plt.GridSpec(4, T, figure=fig, hspace=0.12, wspace=0.18)

    for row_idx, t in enumerate(time_points):
        # PINN predictions at fixed y (and z for 3D)
        t_arr = t * np.ones_like(X)
        pt_x = Variable(torch.from_numpy(X).float(), requires_grad=True).to(device)
        pt_y = Variable(torch.from_numpy(Y).float(), requires_grad=True).to(device)
        pt_z = Variable(torch.from_numpy(Z).float(), requires_grad=True).to(device) if Z is not None else None
        pt_t = Variable(torch.from_numpy(t_arr).float(), requires_grad=True).to(device)
        if use_xpinn:
            if DIMENSION >= 3:
                raise NotImplementedError("XPINN visualizations currently support up to 2D.")
            out = predict_xpinn(nets, pt_x, pt_y, pt_t, xmin, xmax, ymin, ymax)
        else:
            # Use _build_input_list helper to ensure correct coordinate ordering
            inputs = _build_input_list(pt_x, pt_t, pt_y, pt_z)
            out = nets[0](inputs)
        rho_pinn = out[:, 0:1].data.cpu().numpy().reshape(-1)
        vx_pinn = out[:, 1:2].data.cpu().numpy().reshape(-1)
        # potential not used in cross-section plots

        show_lt = np.isclose(KY, 0.0) and np.isclose(KZ, 0.0) and (a < 0.1)
        if show_lt:
            phase = KX * X[:, 0] + KY * y_fixed + (KZ * z_fixed if z_fixed is not None else 0.0)
            if lam >= jeans:
                rho_lt = rho_base + rho_1*np.exp(alpha * t)*np.cos(phase)
                if k > 0:
                    vx_lt = -v1_lt*np.exp(alpha * t)*np.sin(phase) * (KX / k)
                else:
                    vx_lt = -v1_lt*np.exp(alpha * t)*np.sin(phase)
            else:
                omega = np.sqrt(cs**2 * (KX**2 + KY**2 + KZ**2) - const*G*rho_base)
                rho_lt = rho_base + rho_1*np.cos(omega * t - phase)
                if k > 0:
                    vx_lt = v1_lt*np.cos(omega * t - phase) * (KX / k)
                else:
                    vx_lt = v1_lt*np.cos(omega * t - phase)

        from scipy.interpolate import interp1d
        
        # Use cached data if available
        if fd_cache is not None and t in fd_cache:
            cache_data = fd_cache[t]
            if DIMENSION == 3:
                # For 3D, use the volume data from cache
                if 'rho_vol' in cache_data:
                    rho_fd_3d = cache_data['rho_vol']
                    vx_fd_3d = cache_data['vx_vol']
                    phi_fd_3d = cache_data['phi_vol']
                    y_fd = cache_data['y']
                    z_fd = cache_data['z']
                    z_idx = cache_data.get('z_idx', np.argmin(np.abs(z_fd - SLICE_Z)))
                else:
                    # Fallback to 2D slice if volume not cached
                    rho_fd_3d = cache_data['rho'][:, :, np.newaxis]
                    vx_fd_3d = cache_data['vx'][:, :, np.newaxis]
                    phi_fd_3d = cache_data['phi'][:, :, np.newaxis] if cache_data['phi'] is not None else None
                    y_fd = cache_data['y']
                    z_fd = np.array([SLICE_Z])
                    z_idx = 0
                y_idx = np.argmin(np.abs(y_fd - y_fixed))
                rho_fd = rho_fd_3d[:, y_idx, z_idx]
                v_fd = vx_fd_3d[:, y_idx, z_idx]
                phi_fd_slice = phi_fd_3d[:, y_idx, z_idx] if phi_fd_3d is not None else None
                x_fd_line = cache_data['x']
            else:
                x_fd_2d = cache_data['x']
                y_fd_2d = cache_data['y']
                rho_fd_2d = cache_data['rho']
                vx_fd_2d = cache_data['vx']
                _phi_fd_2d = cache_data['phi'] if cache_data['phi'] is not None else np.zeros_like(rho_fd_2d)
                y_idx = np.argmin(np.abs(y_fd_2d - y_fixed))
                rho_fd = rho_fd_2d[:, y_idx]
                v_fd = vx_fd_2d[:, y_idx]
                phi_fd_slice = _phi_fd_2d[:, y_idx]
                x_fd_line = x_fd_2d
        else:
            # Compute FD data if not cached
            if DIMENSION == 3:
                # Use unified solver with proper IC type support
                use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
                result = _call_unified_3d_solver(
                    time=t, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1, nu=nu_fd,
                    use_velocity_ps=use_velocity_ps, ps_index=POWER_EXPONENT,
                    vel_rms=a*cs, random_seed=RANDOM_SEED
                )
                # Extract results from unified solver
                x_fd = result.coordinates['x']
                y_fd = result.coordinates['y']
                z_fd = result.coordinates['z']
                rho_fd_3d = result.density
                vx_fd_3d, vy_fd_3d, vz_fd_3d = result.velocity_components
                phi_fd_3d = result.potential
                y_idx = np.argmin(np.abs(y_fd - y_fixed))
                z_idx = np.argmin(np.abs(z_fd - SLICE_Z))
                rho_fd = rho_fd_3d[:, y_idx, z_idx]
                v_fd = vx_fd_3d[:, y_idx, z_idx]
                phi_fd_slice = phi_fd_3d[:, y_idx, z_idx]
                x_fd_line = x_fd
            else:
                if torch.cuda.is_available():
                    _clear_cuda_cache()
                    if row_idx == 0:
                        print(f"Using GPU solver for 1D cross section plot (CUDA available)")
                    use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
                    x_fd_2d, rho_fd_2d, vx_fd_2d, vy_fd_2d, phi_fd_2d_torch, _n, _rho_max = _timed_call(
                        "LAX 2D slice (torch)",
                        lax_solution_torch,
                        time_val=t, N=FD_N_2D, nu=nu_fd, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1,
                        gravity=True, use_velocity_ps=use_velocity_ps, ps_index=POWER_EXPONENT,
                        vel_rms=a*cs, random_seed=RANDOM_SEED
                    )
                    # Handle case where torch version returns None for phi (compute dummy phi if needed)
                    if phi_fd_2d_torch is None:
                        _phi_fd_2d = np.zeros_like(rho_fd_2d)
                    else:
                        _phi_fd_2d = phi_fd_2d_torch
                else:
                    x_fd_2d, rho_fd_2d, vx_fd_2d, vy_fd_2d, _phi_fd_2d, _n, _rho_max = _timed_call(
                        "LAX 2D slice (cpu)",
                        lax_solution,
                        t, FD_N_2D, nu_fd, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=False, animation=True
                    )
                y_fd_2d = np.linspace(0, lam * num_of_waves, rho_fd_2d.shape[1], endpoint=False)
                y_idx = np.argmin(np.abs(y_fd_2d - y_fixed))
                rho_fd = rho_fd_2d[:, y_idx]
                v_fd = vx_fd_2d[:, y_idx]
                phi_fd_slice = _phi_fd_2d[:, y_idx]
                x_fd_line = x_fd_2d

        rho_fd_interp = interp1d(x_fd_line, rho_fd, kind='linear', bounds_error=False, fill_value='extrapolate')(X[:, 0])
        v_fd_interp = interp1d(x_fd_line, v_fd, kind='linear', bounds_error=False, fill_value='extrapolate')(X[:, 0])
        phi_fd_interp = interp1d(x_fd_line, phi_fd_slice, kind='linear', bounds_error=False, fill_value='extrapolate')(X[:, 0])

        # Column index
        c = row_idx
        # Top row: density
        ax_rho = fig.add_subplot(grid[0, c])
        ax_rho.plot(X[:, 0], rho_pinn, label="GRINN", color='c', linewidth=2)
        # Only plot Linear Theory when enabled in config, KY == 0, and amplitude is small
        if SHOW_LINEAR_THEORY and show_lt:
            ax_rho.plot(X[:, 0], rho_lt, label="LT", linestyle='--', color='firebrick', linewidth=1.5)
        ax_rho.plot(X[:, 0], rho_fd_interp, label="FD", color='k', linewidth=1)
        ax_rho.set_title(f"t={t:.1f}")
        ax_rho.set_ylabel(r"$\rho$")
        ax_rho.grid(True)
        # Dynamic y-axis limits based on actual data with padding (similar to t=3.0 plot)
        # Collect all density values that are plotted
        rho_all = [rho_pinn, rho_fd_interp]
        if SHOW_LINEAR_THEORY and show_lt:
            rho_all.append(rho_lt)
        rho_min = min(np.min(rho) for rho in rho_all)
        rho_max = max(np.max(rho) for rho in rho_all)
        # Add padding: ~10% of the data range on each side (similar to t=3.0 example)
        rho_range = rho_max - rho_min
        padding = max(0.1 * rho_range, 0.05)  # At least 0.05 units of padding
        liml = rho_min - padding
        limu = rho_max + padding
        # Commented out hardcoded limits:
        # if a < 0.1:
        #     limu = 1.2*rho_o
        #     liml = .8*rho_o
        # else:
        #     limu = 3.0*rho_o
        #     liml = -1.0*rho_o
        ax_rho.set_ylim(liml, limu)
        if c == 0:
            ax_rho.legend(loc='upper right', fontsize=8)

        # Second row: epsilon for density using symmetric percent with absolute numerator
        # ε = 200 * |G - R| / (G + R) with more robust denominator
        eps_rho = 200.0 * np.abs(rho_pinn - rho_fd_interp) / (rho_pinn + rho_fd_interp + 1e-6)
        ax_eps_rho = fig.add_subplot(grid[1, c])
        ax_eps_rho.plot(X[:, 0], eps_rho, color='k', linewidth=1, label='FD')
        # Only plot Linear Theory epsilon when enabled in config, KY == 0, and amplitude is small
        if SHOW_LINEAR_THEORY and show_lt:
            eps_rho_lt = 200.0 * np.abs(rho_pinn - rho_lt) / (rho_pinn + rho_lt + 1e-6)
            ax_eps_rho.plot(X[:, 0], eps_rho_lt, color='firebrick', linestyle='--', linewidth=1, label='LT')
        ax_eps_rho.set_ylabel(r"$\varepsilon$")
        ax_eps_rho.grid(True)
        if c == 0:
            ax_eps_rho.legend(loc='upper right', fontsize=8)

        # Third row: velocity
        ax_v = fig.add_subplot(grid[2, c])
        ax_v.plot(X[:, 0], vx_pinn, label="GRINN", color='c', linewidth=2)
        # Only plot Linear Theory when enabled in config, KY == 0, and amplitude is small
        if SHOW_LINEAR_THEORY and show_lt:
            ax_v.plot(X[:, 0], vx_lt, label="LT", linestyle='--', color='firebrick', linewidth=1.5)
        ax_v.plot(X[:, 0], v_fd_interp, label="FD", color='k', linewidth=1)
        ax_v.set_ylabel(r"$v$")
        ax_v.grid(True)
        # Dynamic y-axis limits based on actual data with padding (similar to t=3.0 plot)
        # Collect all velocity values that are plotted
        v_all = [vx_pinn, v_fd_interp]
        if SHOW_LINEAR_THEORY and show_lt:
            v_all.append(vx_lt)
        v_min = min(np.min(v) for v in v_all)
        v_max = max(np.max(v) for v in v_all)
        # Add padding: ~10% of the data range on each side (similar to t=3.0 example)
        v_range = v_max - v_min
        padding = max(0.1 * v_range, 0.005)  # At least 0.005 units of padding
        liml = v_min - padding
        limu = v_max + padding
        # Commented out hardcoded limits:
        # if a < 0.1:
        #     limu = 0.055
        #     liml = -0.055
        # else:
        #     limu = 0.6
        #     liml = -0.6
        ax_v.set_ylim(liml, limu)
        if c == 0:
            ax_v.legend(loc='upper right', fontsize=8)

        # Fourth row: epsilon for velocity using notebook-style +1 offset (symmetric percent with shift)
        # ε = 200 * |(v_pred+1) - (v_ref+1)| / ((v_pred+1) + (v_ref+1)) = 200 * |v_pred - v_ref| / (v_pred + v_ref + 2)
        v_ref = v_fd_interp
        v_pred = vx_pinn
        eps_v = 200.0 * np.abs(v_pred - v_ref) / (v_pred + v_ref + 2.0)
        ax_eps_v = fig.add_subplot(grid[3, c])
        ax_eps_v.plot(X[:, 0], eps_v, color='k', linewidth=1, label='FD')
        # Only plot Linear Theory epsilon when enabled in config, KY == 0, and amplitude is small
        if SHOW_LINEAR_THEORY and show_lt:
            eps_v_lt = 200.0 * np.abs(v_pred - vx_lt) / (v_pred + vx_lt + 2.0)
            ax_eps_v.plot(X[:, 0], eps_v_lt, color='firebrick', linestyle='--', linewidth=1, label='LT')
        ax_eps_v.set_xlabel("x")
        ax_eps_v.set_ylabel(r"$\varepsilon$")
        ax_eps_v.grid(True)
        if c == 0:
            ax_eps_v.legend(loc='upper right', fontsize=8)

        # No potential subplot per request

    # Reduce outer margins similar to notebook style
    fig.subplots_adjust(left=0.06, right=0.99, top=0.92, bottom=0.10, wspace=0.18, hspace=0.12)
    
    # Save the figure to output folder
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "1d_cross_sections_sinusoidal.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved 1D cross sections plot to {save_path}")
    
    plt.show()

    return fig


def Two_D_surface_plots_FD(time, initial_params, N=200, nu=0.5, ax=None, which="density",
                           use_velocity_ps=None, ps_index=None, vel_rms=None, random_seed=None,
                           fd_cache=None):
    """
    Create 2D surface plots with velocity vectors using the Finite Difference (LAX) solver

    Args:
        time: Time to plot
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        N: Grid resolution for LAX solver (Nx = Ny = N) - only used if fd_cache is None
        nu: Courant number for LAX solver - only used if fd_cache is None
        ax: Optional matplotlib axis to plot on
        which: "density" or "velocity"
        use_velocity_ps: Whether to use velocity power spectrum (defaults to config) - only used if fd_cache is None
        ps_index: Power spectrum index (defaults to POWER_EXPONENT) - only used if fd_cache is None
        vel_rms: Velocity RMS amplitude (defaults to a*cs) - only used if fd_cache is None
        random_seed: Random seed (defaults to 1234) - only used if fd_cache is None
        fd_cache: Optional dictionary mapping time -> FD data (if provided, solver is not called)

    Returns:
        The QuadMesh object from pcolormesh
    """
    xmin, xmax, ymin, ymax, rho_1, _alpha, lam, _output_folder, _tmax = initial_params

    # Use cached data if available
    if fd_cache is not None and time in fd_cache:
        cache_data = fd_cache[time]
        x_fd = cache_data['x']
        y_fd = cache_data['y']
        rho_fd = cache_data['rho']
        vx_fd = cache_data['vx']
        vy_fd = cache_data['vy']
        X, Y = np.meshgrid(x_fd, y_fd, indexing='ij')
        Nx = x_fd.shape[0]
        Ny = y_fd.shape[0] if len(y_fd.shape) > 0 else rho_fd.shape[1]
    else:
        # Use config defaults if not specified to ensure consistency with PINN training
        if use_velocity_ps is None:
            use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
        if ps_index is None:
            ps_index = POWER_EXPONENT
        if vel_rms is None:
            vel_rms = a * cs
        if random_seed is None:
            random_seed = RANDOM_SEED

        # Domain properties for LAX (Lx = Ly and Nx = Ny in solver)
        num_of_waves = (xmax - xmin) / lam

        # Decide grid resolution policy: use N_GRID for power spectrum; FD_N_2D for sinusoidal when N is None
        if str(PERTURBATION_TYPE).lower() == "power_spectrum":
            N_use = N_GRID
        else:
            N_use = FD_N_2D if N is None else N

        # Run LAX solver (finite difference) with self-gravity enabled to obtain 2D fields
        # Prefer using the exact shared velocity fields (if available) to ensure identical realization
        # across PINN ICs and all FD visualizations.
        # Returns (gravity=True, comparison=False, animation=True):
        #   x (Nx,), rho (Nx,Ny), vx (Nx,Ny), vy (Nx,Ny), phi (Nx,Ny), n, rho_max
        if DIMENSION == 3:
            # Use unified solver with proper IC type support
            result = _call_unified_3d_solver(
                time=time, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1, nu=nu,
                use_velocity_ps=use_velocity_ps, ps_index=ps_index,
                vel_rms=vel_rms, random_seed=random_seed
            )
            # Extract results from unified solver
            x_fd = result.coordinates['x']
            y_fd = result.coordinates['y']
            z_fd = result.coordinates['z']
            rho_vol = result.density
            vx_vol, vy_vol, vz_vol = result.velocity_components
            z_idx = np.argmin(np.abs(z_fd - SLICE_Z))
            rho_fd = rho_vol[:, :, z_idx]
            vx_fd = vx_vol[:, :, z_idx]
            vy_fd = vy_vol[:, :, z_idx]
            X, Y = np.meshgrid(x_fd, y_fd, indexing='ij')
        else:
            if (str(PERTURBATION_TYPE).lower() == "power_spectrum" \
                and _shared_vx_np is not None and _shared_vy_np is not None):
                # Use native resolution of shared fields to avoid resampling artifacts
                n_fd_use = int(_shared_vx_np.shape[0])
                x_fd, rho_fd, vx_fd, vy_fd, _phi_fd, _n, _rho_max = _timed_call(
                    "LAX 2D (shared-field cpu)",
                    lax_solution,
                    time, n_fd_use, nu, lam, num_of_waves, rho_1,
                    gravity=True, isplot=False, comparison=False, animation=True,
                    vx0_shared=_shared_vx_np, vy0_shared=_shared_vy_np
                )
            else:
                if torch.cuda.is_available():
                    _clear_cuda_cache()
                    x_fd, rho_fd, vx_fd, vy_fd, phi_fd_torch, _n, _rho_max = _timed_call(
                        "LAX 2D (torch)",
                        lax_solution_torch,
                        time_val=time, N=N_use, nu=nu, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1,
                        gravity=True, use_velocity_ps=use_velocity_ps, ps_index=ps_index,
                        vel_rms=vel_rms, random_seed=random_seed
                    )
                    _phi_fd = phi_fd_torch if phi_fd_torch is not None else np.zeros_like(rho_fd)
                else:
                    x_fd, rho_fd, vx_fd, vy_fd, _phi_fd, _n, _rho_max = _timed_call(
                        "LAX 2D (cpu)",
                        lax_solution,
                        time, N_use, nu, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=False, animation=True,
                        use_velocity_ps=use_velocity_ps, ps_index=ps_index, vel_rms=vel_rms, random_seed=random_seed
                    )

            Lx = lam * num_of_waves
            Nx = x_fd.shape[0]
            Ny = rho_fd.shape[1]
            y_fd = np.linspace(0.0, Lx, Ny, endpoint=False)
            X, Y = np.meshgrid(x_fd, y_fd, indexing='ij')

    if ax is None:  # for single plot
        plt.figure(figsize=(5, 5))
        ax = plt.gca()

    # Clean FD velocity fields and avoid zero-length arrows
    vx_c = np.nan_to_num(vx_fd, nan=0.0, posinf=0.0, neginf=0.0)
    vy_c = np.nan_to_num(vy_fd, nan=0.0, posinf=0.0, neginf=0.0)
    Vmag_fd = np.sqrt(vx_c**2 + vy_c**2)
    mask = Vmag_fd > 1e-12

    if which == "density":
        pc = ax.pcolormesh(X, Y, rho_fd, shading='auto', cmap='YlOrBr', vmin=np.min(rho_fd), vmax=np.max(rho_fd))
        skip = (slice(None, None, max(1, Nx // 20)), slice(None, None, max(1, Ny // 20)))
        ax.quiver(
            X[skip][mask[skip]], Y[skip][mask[skip]],
            vx_c[skip][mask[skip]], vy_c[skip][mask[skip]],
            color='k', headwidth=3.0, width=0.003,
            scale_units='xy', angles='xy', scale=1.0, minlength=0.0, pivot='mid'
        )
        ax.set_title("FD Density, t={}".format(round(time, 2)))
        cbar = plt.colorbar(pc, shrink=0.6, location='right')
        cbar.formatter.set_powerlimits((0, 0))
        cbar.ax.set_title(r"$\rho$", fontsize=14)
    else:
        pc = ax.pcolormesh(X, Y, Vmag_fd, shading='auto', cmap='viridis', vmin=np.min(Vmag_fd), vmax=np.max(Vmag_fd))
        skip = (slice(None, None, max(1, Nx // 20)), slice(None, None, max(1, Ny // 20)))
        ax.quiver(
            X[skip][mask[skip]], Y[skip][mask[skip]],
            vx_c[skip][mask[skip]], vy_c[skip][mask[skip]],
            color='k', headwidth=3.0, width=0.003,
            scale_units='xy', angles='xy', scale=1.0, minlength=0.0, pivot='mid'
        )
        ax.set_title("FD Velocity, t={}".format(round(time, 2)))
        cbar = plt.colorbar(pc, shrink=0.6, location='right')
        cbar.ax.set_title(r" $|v|$", fontsize=14)

    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)

    return pc


def create_2d_surface_plots_FD(initial_params, time_points=None, which="density", N=200, nu=0.5,
                               use_velocity_ps=None, ps_index=None, vel_rms=None, random_seed=None,
                               fd_cache=None):
    """
    Create 2D surface plots at multiple time points using the LAX FD solver

    Args:
        initial_params: (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        time_points: list of times
        which: "density" or "velocity"
        N: grid size (only used if fd_cache is None)
        nu: Courant number (only used if fd_cache is None)
        use_velocity_ps: Whether to use velocity power spectrum (defaults to config) - only used if fd_cache is None
        ps_index: Power spectrum index (defaults to POWER_EXPONENT) - only used if fd_cache is None
        vel_rms: Velocity RMS amplitude (defaults to a*cs) - only used if fd_cache is None
        random_seed: Random seed (defaults to 1234) - only used if fd_cache is None
        fd_cache: Optional dictionary mapping time -> FD data (if provided, solver is not called)
    """
    if time_points is None:
        time_points = [0.0, 0.5, 1.0, 1.5, 2.0]

    # Use config defaults if not specified to ensure consistency with PINN training
    if use_velocity_ps is None:
        use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
    if ps_index is None:
        ps_index = POWER_EXPONENT
    if vel_rms is None:
        vel_rms = a * cs
    if random_seed is None:
        random_seed = RANDOM_SEED

    print("Creating 2D FD surface plots...")

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    for i, t in enumerate(time_points):
        if i < len(axes):
            print(f"FD plotting at t = {t}")
            # Enforce grid policy: use N_GRID for power spectrum; else pass through N
            if str(PERTURBATION_TYPE).lower() == "power_spectrum":
                N_call = N_GRID
            else:
                N_call = N
            Two_D_surface_plots_FD(t, initial_params, N=N_call, nu=nu, ax=axes[i], which=which,
                                   use_velocity_ps=use_velocity_ps, ps_index=ps_index, vel_rms=vel_rms, random_seed=random_seed,
                                   fd_cache=fd_cache)

    if len(time_points) < len(axes):
        fig.delaxes(axes[-1])

    plt.tight_layout()
    
    # Save the figure to output folder
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, f"2d_surface_plots_FD_{which}.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved 2D FD surface plots ({which}) to {save_path}")
    
    return fig, axes


def create_5x3_comparison_table(net, initial_params, which="density", N=200, nu=0.5,
                                use_velocity_ps=None, ps_index=None, vel_rms=None, random_seed=None,
                                fd_cache=None):
    """
    Create 5x3 comparison table showing PINN, FD, and epsilon metric at 5 time snapshots
    
    Args:
        net: Trained neural network
        initial_params: Tuple containing (xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax)
        which: "density" or "velocity"
        N: Grid resolution for LAX solver (only used if fd_cache is None)
        nu: Courant number for LAX solver (only used if fd_cache is None)
        use_velocity_ps: Whether to use velocity power spectrum for FD (defaults to config) - only used if fd_cache is None
        ps_index: Power spectrum index for FD (defaults to POWER_EXPONENT) - only used if fd_cache is None
        vel_rms: Velocity RMS for FD (defaults to a*cs) - only used if fd_cache is None
        random_seed: Random seed for FD (defaults to 1234) - only used if fd_cache is None
        fd_cache: Optional dictionary mapping time -> FD data (if provided, solver is not called)
    """
    xmin, xmax, ymin, ymax, rho_1, alpha, lam, output_folder, tmax = initial_params
    
    # Use config defaults if not specified to ensure consistency with PINN training
    if use_velocity_ps is None:
        use_velocity_ps = (str(PERTURBATION_TYPE).lower() == "power_spectrum")
    if ps_index is None:
        ps_index = POWER_EXPONENT
    if vel_rms is None:
        vel_rms = a * cs
    if random_seed is None:
        random_seed = RANDOM_SEED
    
    # Handle both single network and list of networks
    if isinstance(net, list):
        nets = net
        use_xpinn = len(nets) > 1
    else:
        nets = [net]
        use_xpinn = False
    
    # Generate 5 time points uniformly distributed over [0, tmax]
    time_points = np.linspace(0.0, float(tmax), 5)
    
    print(f"Creating 5x3 comparison table for {which}...")
    
    # Create 5x3 subplot grid
    fig, axes = plt.subplots(5, 3, figsize=(15, 20))
    
    # Store data for consistent color limits
    pinn_data = []
    fd_data = []
    pinn_velocity_data = []  # Store velocity components separately
    fd_velocity_data = []    # Store velocity components separately
    
    # First pass: collect data to determine consistent color limits
    for i, t in enumerate(time_points):
        # print(f"Collecting data for t = {t:.2f}")  # Commented out to reduce output noise
        
        # Get PINN data - use N_GRID for consistency with FD solver
        # Exclude right boundary for periodic domains to avoid double-counting
        Q = N_GRID
        xs = np.linspace(xmin, xmax, Q, endpoint=False)
        ys = np.linspace(ymin, ymax, Q, endpoint=False)
        tau, phi = np.meshgrid(xs, ys) 
        Xgrid = np.vstack([tau.flatten(), phi.flatten()]).T
        t_00 = t * np.ones(Q**2).reshape(Q**2, 1)
        
        pt_x_collocation = Variable(torch.from_numpy(Xgrid[:, 0:1]).float(), requires_grad=True).to(device)
        pt_y_collocation = Variable(torch.from_numpy(Xgrid[:, 1:2]).float(), requires_grad=True).to(device) if DIMENSION >= 2 else None
        pt_z_collocation = Variable(torch.from_numpy(np.full((Q**2, 1), SLICE_Z)).float(), requires_grad=True).to(device) if DIMENSION >= 3 else None
        pt_t_collocation = Variable(torch.from_numpy(t_00).float(), requires_grad=True).to(device)
        
        if use_xpinn:
            output_00 = predict_xpinn(nets, pt_x_collocation, pt_y_collocation, pt_t_collocation, xmin, xmax, ymin, ymax)
        else:
            # Ensure inputs are on the same device as the model
            net_device = next(nets[0].parameters()).device
            pt_x_collocation = pt_x_collocation.to(net_device)
            pt_t_collocation = pt_t_collocation.to(net_device)
            if pt_y_collocation is not None:
                pt_y_collocation = pt_y_collocation.to(net_device)
            if pt_z_collocation is not None:
                pt_z_collocation = pt_z_collocation.to(net_device)
            output_00 = nets[0](_build_input_list(pt_x_collocation, pt_t_collocation, pt_y_collocation, pt_z_collocation))
        
        rho_tensor, vx_tensor, vy_tensor, _, _ = _split_outputs(output_00)
        if which == "density":
            pinn_field = rho_tensor.detach().cpu().numpy().reshape(Q, Q)
        else:
            vx_grid = vx_tensor.detach().cpu().numpy().reshape(Q, Q)
            vy_grid = vy_tensor.detach().cpu().numpy().reshape(Q, Q) if vy_tensor is not None else np.zeros_like(vx_grid)
            pinn_field = np.sqrt(vx_grid**2 + vy_grid**2)
        pinn_vx = vx_tensor.detach().cpu().numpy().reshape(Q, Q)
        pinn_vy = vy_tensor.detach().cpu().numpy().reshape(Q, Q) if vy_tensor is not None else np.zeros_like(pinn_vx)
        
        # Debug output (commented out to reduce noise)
        # print(f"  PINN {which} range: [{np.min(pinn_field):.6f}, {np.max(pinn_field):.6f}], std: {np.std(pinn_field):.6f}")
        
        # Get FD data - use cached data if available, otherwise compute
        num_of_waves = (xmax - xmin) / lam
        
        if fd_cache is not None and t in fd_cache:
            # Use cached data
            cache_data = fd_cache[t]
            if DIMENSION == 3:
                # For 3D, use volume data from cache if available, otherwise use 2D slice
                if 'rho_vol' in cache_data:
                    rho_vol = cache_data['rho_vol']
                    vx_vol = cache_data['vx_vol']
                    vy_vol = cache_data['vy_vol']
                    phi_vol = cache_data.get('phi_vol')
                    x_fd = cache_data['x']
                    y_fd = cache_data['y']
                    z_fd = cache_data['z']
                    z_idx = cache_data.get('z_idx', np.argmin(np.abs(cache_data['z'] - SLICE_Z)))
                else:
                    # Fallback to 2D slice
                    rho_vol = cache_data['rho'][:, :, np.newaxis]
                    vx_vol = cache_data['vx'][:, :, np.newaxis]
                    vy_vol = cache_data['vy'][:, :, np.newaxis]
                    phi_vol = cache_data.get('phi')
                    if phi_vol is not None:
                        phi_vol = phi_vol[:, :, np.newaxis]
                    x_fd = cache_data['x']
                    y_fd = cache_data['y']
                    z_fd = np.array([SLICE_Z])
                    z_idx = 0
                rho_fd = rho_vol[:, :, z_idx]
                vx_fd = vx_vol[:, :, z_idx]
                vy_fd = vy_vol[:, :, z_idx]
                phi_fd = phi_vol[:, :, z_idx] if phi_vol is not None else np.zeros_like(rho_fd)
                X_fd, Y_fd = np.meshgrid(x_fd, y_fd, indexing='ij')
            else:
                x_fd = cache_data['x']
                y_fd = cache_data['y']
                rho_fd = cache_data['rho']
                vx_fd = cache_data['vx']
                vy_fd = cache_data['vy']
                phi_fd = cache_data.get('phi')
                if phi_fd is None:
                    phi_fd = np.zeros_like(rho_fd)
                X_fd, Y_fd = np.meshgrid(x_fd, y_fd, indexing='ij')
        else:
            # Compute FD data if not cached
            if DIMENSION == 3:
                # Use unified solver with proper IC type support
                result = _call_unified_3d_solver(
                    time=t, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1, nu=nu,
                    use_velocity_ps=use_velocity_ps, ps_index=ps_index,
                    vel_rms=vel_rms, random_seed=random_seed
                )
                # Extract results from unified solver
                x_fd = result.coordinates['x']
                y_fd = result.coordinates['y']
                z_fd = result.coordinates['z']
                rho_vol = result.density
                vx_vol, vy_vol, vz_vol = result.velocity_components
                phi_vol = result.potential
                z_idx = np.argmin(np.abs(z_fd - SLICE_Z))
                rho_fd = rho_vol[:, :, z_idx]
                vx_fd = vx_vol[:, :, z_idx]
                vy_fd = vy_vol[:, :, z_idx]
                phi_fd = phi_vol[:, :, z_idx] if phi_vol is not None else np.zeros_like(rho_fd)
                X_fd, Y_fd = np.meshgrid(x_fd, y_fd, indexing='ij')
            else:
                if str(PERTURBATION_TYPE).lower() == "power_spectrum":
                    if _shared_vx_np is not None and _shared_vy_np is not None:
                        x_fd, rho_fd, vx_fd, vy_fd, phi_fd, _n, _rho_max = _timed_call(
                            "LAX 2D (shared-field cpu)",
                            lax_solution,
                            t, N, nu, lam, num_of_waves, rho_1,
                            gravity=True, isplot=False, comparison=False, animation=True,
                            vx0_shared=_shared_vx_np, vy0_shared=_shared_vy_np
                        )
                    else:
                        if torch.cuda.is_available():
                            _clear_cuda_cache()
                            x_fd, rho_fd, vx_fd, vy_fd, phi_fd_torch, _n, _rho_max = _timed_call(
                                "LAX 2D (torch)",
                                lax_solution_torch,
                                time_val=t, N=N, nu=nu, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1,
                                gravity=True, use_velocity_ps=True, ps_index=POWER_EXPONENT, vel_rms=a*cs, random_seed=RANDOM_SEED
                            )
                            phi_fd = phi_fd_torch if phi_fd_torch is not None else np.zeros_like(rho_fd)
                        else:
                            x_fd, rho_fd, vx_fd, vy_fd, phi_fd, _n, _rho_max = _timed_call(
                                "LAX 2D (power cpu)",
                                lax_solution,
                                t, N, nu, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=False, animation=True,
                                use_velocity_ps=True, ps_index=POWER_EXPONENT, vel_rms=a*cs, random_seed=RANDOM_SEED
                            )
                else:
                    if torch.cuda.is_available():
                        _clear_cuda_cache()
                        x_fd, rho_fd, vx_fd, vy_fd, phi_fd_torch, _n, _rho_max = _timed_call(
                            "LAX 2D (torch)",
                            lax_solution_torch,
                            time_val=t, N=N, nu=nu, lam=lam, num_of_waves=num_of_waves, rho_1=rho_1,
                            gravity=True, use_velocity_ps=False, ps_index=ps_index, vel_rms=vel_rms, random_seed=random_seed
                        )
                        phi_fd = phi_fd_torch if phi_fd_torch is not None else np.zeros_like(rho_fd)
                    else:
                        x_fd, rho_fd, vx_fd, vy_fd, phi_fd, _n, _rho_max = _timed_call(
                            "LAX 2D (sinusoidal cpu)",
                            lax_solution,
                            t, N, nu, lam, num_of_waves, rho_1, gravity=True, isplot=False, comparison=False, animation=True,
                            use_velocity_ps=False, ps_index=ps_index, vel_rms=vel_rms, random_seed=random_seed
                        )
                Lx = lam * num_of_waves
                y_fd = np.linspace(0.0, Lx, rho_fd.shape[1], endpoint=False)
                X_fd, Y_fd = np.meshgrid(x_fd, y_fd, indexing='ij')
        
        if which == "density":
            fd_field = rho_fd
        else:  # velocity magnitude
            fd_field = np.sqrt(vx_fd**2 + vy_fd**2)
        
        # Interpolate FD data to PINN grid for comparison using single robust method
        from scipy.interpolate import RegularGridInterpolator
        points_fd = np.column_stack([X_fd.ravel(), Y_fd.ravel()])
        points_pinn = np.column_stack([tau.ravel(), phi.ravel()])
        
        # Use RegularGridInterpolator for robust interpolation with proper boundary handling
        # This avoids the interpolation cascade issues and provides consistent results
        interpolator = RegularGridInterpolator(
            (x_fd, y_fd), fd_field, 
            method='linear', 
            bounds_error=False, 
            fill_value=None  # extrapolate
        )
        fd_field_interp = interpolator(points_pinn)
        
        fd_field_interp = fd_field_interp.reshape(Q, Q)
        
        # Interpolate FD velocity components for vector plots using single robust method
        vx_interpolator = RegularGridInterpolator(
            (x_fd, y_fd), vx_fd, 
            method='linear', 
            bounds_error=False, 
            fill_value=None  # extrapolate
        )
        fd_vx_interp = vx_interpolator(points_pinn)
        
        vy_interpolator = RegularGridInterpolator(
            (x_fd, y_fd), vy_fd, 
            method='linear', 
            bounds_error=False, 
            fill_value=None  # extrapolate
        )
        fd_vy_interp = vy_interpolator(points_pinn)
        
        fd_vx_interp = fd_vx_interp.reshape(Q, Q)
        fd_vy_interp = fd_vy_interp.reshape(Q, Q)
        
        pinn_data.append(pinn_field)
        fd_data.append(fd_field_interp)
        
        # Store velocity components for vector plots (both density and velocity plots)
        pinn_velocity_data.append((pinn_vx, pinn_vy))
        fd_velocity_data.append((fd_vx_interp, fd_vy_interp))
    
    # Use individual color limits for each plot (like animation) to show dynamic range
    # This allows collapse features to be visible, rather than using global limits
    
    # Second pass: create plots
    for i, t in enumerate(time_points):
        pinn_field = pinn_data[i]
        fd_field = fd_data[i]
        
        # Extract velocity components for vector plots
        pinn_vx, pinn_vy = pinn_velocity_data[i]
        fd_vx, fd_vy = fd_velocity_data[i]
        
        # Calculate epsilon metric: ε = 2 * |PINN - FD| / (PINN + FD) * 100
        # Use a more robust denominator to reduce sensitivity to small values
        eps = 1e-6  # Increased from 1e-12 to reduce sensitivity
        epsilon_metric = 200.0 * np.abs(pinn_field - fd_field) / (pinn_field + fd_field + eps)
        
        # Column 1: PINN - use individual color limits like animation
        ax_pinn = axes[i, 0]
        if which == "density":
            pc_pinn = ax_pinn.pcolormesh(tau, phi, pinn_field, shading='auto', cmap='YlOrBr', 
                                       vmin=np.min(pinn_field), vmax=np.max(pinn_field))
        else:
            pc_pinn = ax_pinn.pcolormesh(tau, phi, pinn_field, shading='auto', cmap='viridis', 
                                       vmin=np.min(pinn_field), vmax=np.max(pinn_field))
        
        # Add velocity vectors for both density and velocity plots
        if pinn_vx is not None and pinn_vy is not None:
            # Subsample vectors for clarity (similar to analyze_lax.py)
            skip_x = max(1, Q // 20)
            skip_y = max(1, Q // 20)
            skip = (slice(None, None, skip_x), slice(None, None, skip_y))
            ax_pinn.quiver(tau[skip], phi[skip], pinn_vx[skip], pinn_vy[skip], 
                          color='k', headwidth=3.0, width=0.003, alpha=0.7)
        
        ax_pinn.set_title(f"PINN {which.title()}, t={t:.2f}")
        ax_pinn.set_xlim(xmin, xmax)
        ax_pinn.set_ylim(ymin, ymax)
        
        # Add interface lines for XPINN
        if use_xpinn:
            add_interface_lines(ax_pinn, xmin, xmax, ymin, ymax)
        
        cbar_pinn = plt.colorbar(pc_pinn, ax=ax_pinn, shrink=0.6)
        cbar_pinn.ax.set_title(r"$\rho$" if which == "density" else r"$|v|$", fontsize=14)
        
        # Column 2: FD - use individual color limits
        ax_fd = axes[i, 1]
        if which == "density":
            pc_fd = ax_fd.pcolormesh(tau, phi, fd_field, shading='auto', cmap='YlOrBr', 
                                    vmin=np.min(fd_field), vmax=np.max(fd_field))
        else:
            pc_fd = ax_fd.pcolormesh(tau, phi, fd_field, shading='auto', cmap='viridis', 
                                    vmin=np.min(fd_field), vmax=np.max(fd_field))
        
        # Add velocity vectors for both density and velocity plots
        if fd_vx is not None and fd_vy is not None:
            # Subsample vectors for clarity (similar to analyze_lax.py)
            skip_x = max(1, Q // 20)
            skip_y = max(1, Q // 20)
            skip = (slice(None, None, skip_x), slice(None, None, skip_y))
            ax_fd.quiver(tau[skip], phi[skip], fd_vx[skip], fd_vy[skip], 
                        color='k', headwidth=3.0, width=0.003, alpha=0.7)
        
        ax_fd.set_title(f"FD {which.title()}, t={t:.2f}")
        ax_fd.set_xlim(xmin, xmax)
        ax_fd.set_ylim(ymin, ymax)
        cbar_fd = plt.colorbar(pc_fd, ax=ax_fd, shrink=0.6)
        cbar_fd.ax.set_title(r"$\rho$" if which == "density" else r"$|v|$", fontsize=14)
        
        # Column 3: Epsilon Metric
        ax_diff = axes[i, 2]
        pc_diff = ax_diff.pcolormesh(tau, phi, epsilon_metric, shading='auto', cmap='coolwarm')
        ax_diff.set_title(f"ε (%), t={t:.2f}")
        ax_diff.set_xlim(xmin, xmax)
        ax_diff.set_ylim(ymin, ymax)
        cbar_diff = plt.colorbar(pc_diff, ax=ax_diff, shrink=0.6)
        cbar_diff.ax.set_title("ε (%)", fontsize=14)
        
        # Add x-axis labels only on bottom row
        if i == 4:
            ax_pinn.set_xlabel("x")
            ax_fd.set_xlabel("x")
            ax_diff.set_xlabel("x")
        
        # Add y-axis labels only on leftmost column
        ax_pinn.set_ylabel("y")
    
    plt.tight_layout()
    
    # Save the figure
    output_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, f"{which}_comparison_5x3.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Saved 5x3 comparison table to {save_path}")
    
    plt.show()
    return fig, axes
_register_module('visualization.Plotting', ['Two_D_surface_plots', 'Two_D_surface_plots_FD', '_build_input_list', '_call_unified_3d_solver', '_clear_cuda_cache', '_shared_vx_np', '_shared_vy_np', '_shared_vz_np', '_split_outputs', '_timed_call', 'add_interface_lines', 'analyze_z_variation', 'compute_fd_data_cache', 'create_1d_comparison_plots', 'create_1d_cross_sections_sinusoidal', 'create_2d_animation', 'create_2d_surface_plots', 'create_2d_surface_plots_FD', 'create_5x3_comparison_table', 'create_all_plots', 'create_density_growth_plot', 'create_growth_comparison_plot', 'device', 'find_max_contrast_slice', 'find_max_density_slice', 'get_fd_default_params', 'has_gpu', 'has_mps', 'plot_function', 'predict_xpinn', 'set_shared_velocity_fields'])

# ==== Module: train (train.py) ====
import os
import sys
import shutil
import numpy as np
import time
from typing import Tuple
import torch
import torch.nn as nn

# ==================== PyTorch Performance Optimizations ====================
# Enable cuDNN autotuner - finds optimal convolution algorithms
# This helps when input sizes are consistent (which they are in PINN training)
torch.backends.cudnn.benchmark = True

# Enable TF32 on Ampere GPUs (A100, RTX 3090, RTX 4090, etc.) for 20-30% speedup
# TF32 provides faster matrix multiplication with minimal accuracy loss
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    # Prefer explicit device/dtype settings rather than deprecated tensor-type override
    torch.set_default_dtype(torch.float32)
    if hasattr(torch, "set_default_device"):
        torch.set_default_device("cuda")
    print("PyTorch performance optimizations enabled (cuDNN benchmark, TF32)")

from core.data_generator import input_taker, req_consts_calc
from training.trainer import train
from core.initial_conditions import initialize_shared_velocity_fields
from config import BATCH_SIZE, NUM_BATCHES, N_0, N_r, DIMENSION
from config import a, wave, cs, xmin, ymin, zmin, tmin, tmax as TMAX_CFG, iteration_adam_2D, iteration_lbgfs_2D, harmonics, PERTURBATION_TYPE, rho_o, SLICE_Y
from config import num_neurons, num_layers, num_of_waves
from config import RANDOM_SEED, STARTUP_DT
from config import ENABLE_TRAINING_DIAGNOSTICS
from core.losses import ASTPN
from core.model_architecture import PINN
from visualization.Plotting import create_2d_animation
from visualization.Plotting import create_1d_cross_sections_sinusoidal
from visualization.Plotting import create_density_growth_plot
from config import PLOT_DENSITY_GROWTH, GROWTH_PLOT_TMAX, GROWTH_PLOT_DT
from config import FD_N_2D


def _save_trained_models(net):
    """Persist trained model immediately after training completes."""
    try:
        from config import SNAPSHOT_DIR
        model_dir = os.path.join(SNAPSHOT_DIR, "GRINN")
        os.makedirs(model_dir, exist_ok=True)
        model_path = os.path.join(model_dir, "model.pth")
        torch.save(net.state_dict(), model_path)
        print(f"Saved model to {model_path}")
    except Exception as e:
        print(f"Warning: failed to save model: {e}")


def _comprehensive_gpu_cleanup(nets, optimizers=None, collocation_data=None, 
                                cached_ic_values=None, keep_models_for_vis=True, 
                                target_device=None):
    """
    Comprehensive GPU memory cleanup after PINN training.
    
    This function properly frees GPU memory by:
    1. Moving models to CPU (if keep_models_for_vis=True) or deleting them
    2. Deleting optimizer state (Adam momentum buffers, LBFGS history)
    3. Deleting collocation points and related tensors
    4. Deleting cached IC values
    5. Clearing PyTorch's cache allocator
    
    Args:
        nets: List of neural networks (or single network in a list)
        optimizers: List of optimizers to clean up (optional)
        collocation_data: Dict or list of collocation tensors to delete (optional)
        cached_ic_values: List of cached IC value dicts to delete (optional)
        keep_models_for_vis: If True, move models to CPU instead of deleting (default: True)
        target_device: Target device for models if keep_models_for_vis=True (default: 'cpu')
    
    Returns:
        None (modifies nets in-place)
    """
    if not torch.cuda.is_available():
        return
    
    if target_device is None:
        target_device = 'cpu'
    
    print("Starting comprehensive GPU memory cleanup...")
    
    # 1. Set models to eval mode (reduces memory usage) and optionally move to CPU
    if nets is not None:
        for i, net in enumerate(nets):
            if net is not None:
                # Set to eval mode to disable gradient tracking and reduce memory
                net.eval()
                # Clear any cached gradients
                for param in net.parameters():
                    if param.grad is not None:
                        param.grad = None
                
                if keep_models_for_vis:
                    # Move to target device (usually CPU) for visualization
                    # Models can be moved back to GPU later if needed
                    net = net.to(target_device)
                    nets[i] = net
                    print(f"  Moved network {i} to {target_device} (eval mode)")
                else:
                    # Delete model entirely
                    del net
                    nets[i] = None
                    print(f"  Deleted network {i}")
    
    # 2. Delete optimizer state (can be large, especially LBFGS history)
    if optimizers is not None:
        if isinstance(optimizers, (list, tuple)):
            for i, opt in enumerate(optimizers):
                if opt is not None:
                    # Clear optimizer state (this frees momentum buffers, LBFGS history, etc.)
                    opt.state.clear()
                    opt.param_groups.clear()
                    del opt
                    print(f"  Deleted optimizer {i} state")
        else:
            if optimizers is not None:
                optimizers.state.clear()
                optimizers.param_groups.clear()
                del optimizers
                print("  Deleted optimizer state")
    
    # 3. Delete collocation points (these can be very large)
    if collocation_data is not None:
        if isinstance(collocation_data, dict):
            for key, value in collocation_data.items():
                if value is not None:
                    if isinstance(value, (list, tuple)):
                        for item in value:
                            if isinstance(item, torch.Tensor) and item.is_cuda:
                                del item
                    elif isinstance(value, torch.Tensor) and value.is_cuda:
                        del value
            collocation_data.clear()
            print("  Deleted collocation data dict")
        elif isinstance(collocation_data, (list, tuple)):
            for item in collocation_data:
                if item is not None:
                    if isinstance(item, (list, tuple)):
                        for subitem in item:
                            if isinstance(subitem, torch.Tensor) and subitem.is_cuda:
                                del subitem
                    elif isinstance(item, torch.Tensor) and item.is_cuda:
                        del item
            print("  Deleted collocation data list")
        elif isinstance(collocation_data, torch.Tensor) and collocation_data.is_cuda:
            del collocation_data
            print("  Deleted collocation tensor")
    
    # 4. Delete cached IC values
    if cached_ic_values is not None:
        if isinstance(cached_ic_values, list):
            for i, cached_dict in enumerate(cached_ic_values):
                if cached_dict is not None and isinstance(cached_dict, dict):
                    for key, value in cached_dict.items():
                        if isinstance(value, torch.Tensor) and value.is_cuda:
                            del value
                    cached_dict.clear()
            cached_ic_values.clear()
            print("  Deleted cached IC values")
    
    # 5. Force garbage collection
    import gc
    gc.collect()
    
    # 6. Clear PyTorch's cache allocator and synchronize
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    
    print("GPU memory cleanup completed")
    
    # Note: If visualization needs models on GPU, they can be moved back with:
    # for net in nets:
    #     if net is not None:
    #         net = net.to(device)


def _move_models_to_device(nets, target_device):
    """
    Move models to target device (e.g., back to GPU for visualization).
    
    Args:
        nets: List of neural networks
        target_device: Target device string (e.g., 'cuda:0' or 'cpu')
    
    Returns:
        None (modifies nets in-place)
    """
    if nets is not None:
        for i, net in enumerate(nets):
            if net is not None:
                net = net.to(target_device)
                nets[i] = net


def clean_pycache(root_dir: str) -> Tuple[int, int]:
    """Remove all __pycache__ directories and .pyc files under ``root_dir``."""
    removed_dirs = 0
    removed_files = 0

    for dirpath, dirnames, filenames in os.walk(root_dir):
        if "__pycache__" in dirnames:
            pycache_path = os.path.join(dirpath, "__pycache__")
            try:
                shutil.rmtree(pycache_path)
                print(f"Removed: {pycache_path}")
                removed_dirs += 1
            except Exception as exc:  # pragma: no cover - cleanup best effort
                print(f"Error removing {pycache_path}: {exc}")

        for filename in filenames:
            if filename.endswith(".pyc"):
                pyc_path = os.path.join(dirpath, filename)
                try:
                    os.remove(pyc_path)
                    print(f"Removed: {pyc_path}")
                    removed_files += 1
                except Exception as exc:  # pragma: no cover - cleanup best effort
                    print(f"Error removing {pyc_path}: {exc}")

    print("=" * 60)
    print("Cleanup complete!")
    print(f"  Removed {removed_dirs} __pycache__ directories")
    print(f"  Removed {removed_files} .pyc files")
    print("=" * 60)

    return removed_dirs, removed_files


if "--clean-pycache" in sys.argv or "--clean-cache" in sys.argv:
    flag = "--clean-pycache" if "--clean-pycache" in sys.argv else "--clean-cache"
    script_root = os.path.dirname(os.path.abspath(__file__))
    print(f"Cleaning Python cache files from: {script_root}")
    clean_pycache(script_root)
    # Prevent the rest of the training script from running in cleanup-only mode
    sys.exit(0)

has_gpu = torch.cuda.is_available()
has_mps = torch.backends.mps.is_built()
device = "mps" if torch.backends.mps.is_built() else "cuda:0" if torch.cuda.is_available() else "cpu"

# Clear GPU memory if using CUDA
if device.startswith('cuda'):
    torch.cuda.empty_cache()


lam, rho_1, num_of_waves, tmax, _, _, _ = input_taker(wave, a, num_of_waves, TMAX_CFG, N_0, 0, N_r)

jeans, alpha = req_consts_calc(lam, rho_1)
# Set initial velocity amplitude per perturbation type
if str(PERTURBATION_TYPE).lower() == "sinusoidal":
    k = 2*np.pi/lam
    v_1 = (rho_1 / (rho_o if rho_o != 0 else 1.0)) * (alpha / k)
else:
    v_1 = a * cs

xmax = xmin + lam * num_of_waves
ymax = ymin + lam * num_of_waves
zmax = zmin + lam * num_of_waves
#zmax = 1.0

# Initialize shared velocity fields for consistent PINN/FD initial conditions
vx_np, vy_np, vz_np = None, None, None  # Default values for sinusoidal case
if str(PERTURBATION_TYPE).lower() == "power_spectrum":
    if DIMENSION == 3:
        vx_np, vy_np, vz_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=3)
    else:
        vx_np, vy_np = initialize_shared_velocity_fields(lam, num_of_waves, v_1, seed=RANDOM_SEED, dimension=2)
    
    # Set shared velocity fields for plotting (only for 2D visualization)
    if DIMENSION == 2:
        from visualization.Plotting import set_shared_velocity_fields
        set_shared_velocity_fields(vx_np, vy_np)

# ==================== STANDARD PINN MODE ====================
print("Running in standard PINN mode (single network)...")

net = PINN(n_harmonics=harmonics)
net = net.to(device)
mse_cost_function = torch.nn.MSELoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
optimizerL = torch.optim.LBFGS(net.parameters(), line_search_fn='strong_wolfe')

if DIMENSION == 1:
    astpn_rmin = [xmin, tmin]
    astpn_rmax = [xmax, tmax]
elif DIMENSION == 2:
    astpn_rmin = [xmin, ymin, tmin]
    astpn_rmax = [xmax, ymax, tmax]
elif DIMENSION == 3:
    astpn_rmin = [xmin, ymin, zmin, tmin]
    astpn_rmax = [xmax, ymax, zmax, tmax]
else:
    raise ValueError(f"Unsupported DIMENSION={DIMENSION}")
collocation_model = ASTPN(rmin=astpn_rmin, rmax=astpn_rmax, N_0=N_0, N_b=0, N_r=N_r, dimension=DIMENSION)

# Set domain on the network so periodic embeddings enforce hard BCs
spatial_rmin = [xmin]
spatial_rmax = [xmax]
if DIMENSION >= 2:
    spatial_rmin.append(ymin)
    spatial_rmax.append(ymax)
if DIMENSION >= 3:
    spatial_rmin.append(zmin)
    spatial_rmax.append(zmax)
net.set_domain(rmin=spatial_rmin, rmax=spatial_rmax, dimension=DIMENSION)

# IC collocation stays at t=0 throughout
collocation_IC = collocation_model.geo_time_coord(option="IC")

# Generate extra collocation points at t=0 for Poisson enforcement (Option 3)
from core.data_generator import generate_poisson_ic_points
from config import N_POISSON_IC
collocation_poisson_ic = generate_poisson_ic_points(
    rmin=collocation_model.rmin,
    rmax=collocation_model.rmax,
    n_points=N_POISSON_IC,
    dimension=DIMENSION,
    device=device
)
print(f"Generated {N_POISSON_IC} extra collocation points at t=0 for Poisson enforcement")

start_time = time.time()

# Standard training
print("Using standard training...")
collocation_domain = collocation_model.geo_time_coord(option="Domain")

training_diagnostics = train(
    net=net,
    model=collocation_model,
    collocation_domain=collocation_domain,
    collocation_IC=collocation_IC,
    collocation_poisson_ic=collocation_poisson_ic,
    optimizer=optimizer,
    optimizerL=optimizerL,
    closure=None,
    mse_cost_function=mse_cost_function,
    iteration_adam=iteration_adam_2D,
    iterationL=iteration_lbgfs_2D,
    rho_1=rho_1,
    lam=lam,
    jeans=jeans,
    v_1=v_1,
    device=device
)

end_time = time.time()
elapsed_time = end_time - start_time
print(f"[Timing] PINN training completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)")

# Save model immediately after training completes
_save_trained_models(net)

# ==================== POST-TRAINING COMPREHENSIVE DIAGNOSTICS ====================
# Run comprehensive diagnostics for high-tmax failure analysis
# This generates 4 additional critical plots beyond the training diagnostics
if ENABLE_TRAINING_DIAGNOSTICS and training_diagnostics is not None:
    print("\n" + "="*70)
    print("Running post-training comprehensive diagnostics...")
    print("="*70)
    
    try:
        # Run comprehensive diagnostics - generates 4 additional plots:
        # 1. PDE Residual Heatmaps - WHERE/WHEN physics breaks
        # 2. Conservation Laws - Mass/momentum conservation
        # 3. Spectral Evolution - Frequency content analysis
        # 4. Temporal Statistics - Error accumulation tracking
        training_diagnostics.run_comprehensive_diagnostics(
            model=net,
            dimension=DIMENSION,
            tmax=tmax
        )
        
        print("\n" + "="*70)
        print("All diagnostic plots generated successfully!")
        print("Check ./diagnostics/ folder for:")
        print("  1. training_diagnostics.png - Training convergence")
        print("  2. residual_heatmaps.png - Spatiotemporal PDE violations")
        print("  3. conservation_laws.png - Physical consistency")
        print("  4. spectral_evolution.png - Frequency content")
        print("  5. temporal_statistics.png - Field evolution")
        print("="*70 + "\n")
    except Exception as e:
        print(f"[WARNING] Post-training diagnostics failed: {e}")
        print("Training completed successfully, but diagnostic plots may be incomplete.")
        import traceback
        traceback.print_exc()

# Comprehensive GPU memory cleanup after training
# This properly frees model parameters, optimizer state, collocation points, etc.
if device.startswith('cuda'):
    # Collect all training-related data for cleanup
    cleanup_optimizers = [optimizer, optimizerL] if 'optimizer' in locals() and 'optimizerL' in locals() else None
    
    # Try to get collocation data references
    try:
        cleanup_collocation = {
            'domain': collocation_domain if 'collocation_domain' in locals() else None,
            'IC': collocation_IC if 'collocation_IC' in locals() else None,
            'poisson_IC': collocation_poisson_ic if 'collocation_poisson_ic' in locals() else None
        }
    except:
        cleanup_collocation = None
    
    # Perform comprehensive cleanup
    # This frees GPU memory by deleting optimizer state and collocation data
    # Models are temporarily moved to CPU during cleanup, then moved back to GPU for visualization
    _comprehensive_gpu_cleanup(
        nets=[net],
        optimizers=cleanup_optimizers,
        collocation_data=cleanup_collocation,
        cached_ic_values=None,
        keep_models_for_vis=True,  # Keep models for visualization
        target_device='cpu'  # Temporarily move models to CPU during cleanup
    )
    
    # Move model back to GPU for visualization (visualization code expects model on GPU)
    # The FD solver will clear cache before it runs, so this is safe
    if device.startswith('cuda'):
        _move_models_to_device([net], device)
        print("Model moved back to GPU for visualization")

# ==================== VISUALIZATION ====================
initial_params = (xmin, xmax, ymin, ymax, rho_1, alpha, lam, "temp", tmax)

anim_density = create_2d_animation(net, initial_params, which="density", fps=10, verbose=False)
anim_velocity = create_2d_animation(net, initial_params, which="velocity", fps=10, verbose=False)

if str(PERTURBATION_TYPE).lower() == "sinusoidal":
    create_1d_cross_sections_sinusoidal(net, initial_params, time_points=None, y_fixed=SLICE_Y, N_fd=600, nu_fd=0.5)

if PLOT_DENSITY_GROWTH:
    try:
        tmax_growth = float(GROWTH_PLOT_TMAX)
    except Exception:
        tmax_growth = float(TMAX_CFG)
    dt_growth = float(GROWTH_PLOT_DT)
    create_density_growth_plot(net, initial_params, tmax=tmax_growth, dt=dt_growth)

# ==================== FINAL CACHE CLEANUP ====================
script_root = os.path.dirname(os.path.abspath(__file__))
print("Performing final Python cache cleanup...")
clean_pycache(script_root)
_register_module('train', ['_comprehensive_gpu_cleanup', '_move_models_to_device', '_save_trained_models', 'anim_density', 'anim_velocity', 'clean_pycache', 'collocation_IC', 'collocation_domain', 'collocation_model', 'collocation_poisson_ic', 'device', 'elapsed_time', 'end_time', 'has_gpu', 'has_mps', 'initial_params', 'mse_cost_function', 'net', 'optimizer', 'optimizerL', 'script_root', 'spatial_rmax', 'spatial_rmin', 'start_time', 'training_diagnostics', 'xmax', 'ymax', 'zmax'])

